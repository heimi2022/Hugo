<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>大模型 on WJJ</title>
        <link>https://heimi2022.github.io/Hugo/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
        <description>Recent content in 大模型 on WJJ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Wjj</copyright>
        <lastBuildDate>Mon, 07 Jul 2025 16:00:00 +0800</lastBuildDate><atom:link href="https://heimi2022.github.io/Hugo/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Big_Model_System</title>
        <link>https://heimi2022.github.io/Hugo/p/big_model_system/</link>
        <pubDate>Mon, 07 Jul 2025 16:00:00 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/big_model_system/</guid>
        <description>&lt;h2 id=&#34;自然语言处理nlp基础&#34;&gt;自然语言处理（NLP）基础
&lt;/h2&gt;&lt;h3 id=&#34;基础与应用&#34;&gt;基础与应用
&lt;/h3&gt;&lt;p&gt;自然语言处理是让计算机来理解人类所说的语言，然后像人一样去交互，对话，生成自然语言。&lt;/p&gt;
&lt;p&gt;自然语言处理的基础任务：词性标注、命名实体的识别、共指消解（代词与实体之间的连接）、依赖关系（语法等）；中文的自动分词（将中文的每个词区分出来，像英文一样）；机器翻译；情感分类；意见挖掘&lt;/p&gt;
&lt;h3 id=&#34;词表示&#34;&gt;词表示
&lt;/h3&gt;&lt;p&gt;词表示：把词转换为机器所能理解的意思&lt;/p&gt;
&lt;p&gt;词表示的目标：词相似度的计算；发现词与词之间的语义的关系&lt;/p&gt;
&lt;p&gt;word Embedding：&lt;strong&gt;分布式表示&lt;/strong&gt;，建立一个低维的一个稠密的向量空间，然后把每一个词都学到这个空间里，用空间里某个位置所对应的向量来表示词。代表方法：word2vec。&lt;/p&gt;
&lt;h3 id=&#34;语言模型&#34;&gt;语言模型
&lt;/h3&gt;&lt;p&gt;语言模型：根据前文预测下一个词是什么。&lt;/p&gt;
&lt;h4 id=&#34;语言模型的工作&#34;&gt;语言模型的工作
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;完成计算一个序列的词成为一句话的概率（joint probability）&lt;/li&gt;
&lt;/ol&gt;
$$
P(W)=P(w_1,w_2,\cdots,w_n)
$$&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;根据前文预测下一个词（Conditional Probability）&lt;/li&gt;
&lt;/ol&gt;
$$
P(w_n|w_1,w_2,\cdots,w_{n-1})
$$&lt;h4 id=&#34;语言模型的假设&#34;&gt;语言模型的假设
&lt;/h4&gt;&lt;p&gt;即将出现的词只受它之前出现词的影响。因此语言模型可表示为：
&lt;/p&gt;
$$
P(w_1,w_2,\cdots,w_n)=\prod_ip(w_i|w_1,w_2,\cdots,w_{i-1})
$$&lt;h4 id=&#34;n-gram-model&#34;&gt;N-gram Model
&lt;/h4&gt;&lt;p&gt;模型的搭建：统计前面出现了几个词之后后面出现的那个词的频度是什么样的。&lt;/p&gt;
&lt;p&gt;例如4-gram：
&lt;/p&gt;
$$
p(w_j|too \, late \, to)=\frac{count(too \, late \, to \, w_j)}{count(too \, late \, to)}
$$&lt;p&gt;
其满足Markov assumption：
&lt;/p&gt;
$$
P(w_1,w_2,\cdots,w_n) \approx \prod_i p(w_i|w_1,w_2,\cdots,w_{i-1})
$$&lt;p&gt;
并且有：
&lt;/p&gt;
$$
p(w_i|w_1,w_2,\cdots,w_{i-1}) \approx p(w_i|w_{i-k},\cdots,w_{i-1})
$$&lt;p&gt;
问题：不能发现句子间的相似度。类似用独热码来表示词。&lt;/p&gt;
&lt;h4 id=&#34;neural-language-model&#34;&gt;Neural Language Model
&lt;/h4&gt;&lt;p&gt;用分布式表示词，将词表示为一个低维向量，在把低维向量拼在一起，形成一个更高维的上下文向量，再进行非线性转换，就可以用其预测下一个词。类似神经网络的过程，其可基于神经网络可调参数来学习上下文间的向量的关系。&lt;/p&gt;
&lt;h2 id=&#34;神经网络&#34;&gt;神经网络
&lt;/h2&gt;&lt;h3 id=&#34;神经网络基础&#34;&gt;神经网络基础
&lt;/h3&gt;&lt;p&gt;为什么要有激活函数？若不存在激活函数，则多层神经网络都为线性运算，其最终可以被转化为一个单层的神经网络。也就是说，在每一激活函数的情况下，多层神经网络和单层神经网络表达能力是一致的。举例如下：
&lt;/p&gt;
$$
h_1=W_1 x + b_1,h_2=W_2 h_1 + b_2  \quad -&gt; \quad h_2=W_2 W_1 x + W_2 b_1 + b_2
$$&lt;h3 id=&#34;word2vec&#34;&gt;Word2vec
&lt;/h3&gt;&lt;p&gt;word2vec有两类模型：Continuous bag-of-words(CBOW)以及continuous skip-gram。&lt;/p&gt;
&lt;p&gt;word2vec用滑动窗口的方式构造训练数据，一个滑动窗口是一个文本中连续出现的几个单词，在窗口中间的词叫做target，其它叫做context。&lt;/p&gt;
&lt;h4 id=&#34;cbow&#34;&gt;CBOW
&lt;/h4&gt;&lt;p&gt;CBOW：根据context推测target。其不考虑context词的次序。&lt;/p&gt;
&lt;p&gt;下图是CBOW的模型，其是一个多分类问题。类别数为词表大小。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/Big_Model_CBOW&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708175947075&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;continuous-skip-gram&#34;&gt;continuous skip-gram
&lt;/h4&gt;&lt;p&gt;continuous skip-gram：根据target推出context。其一次预测多个context时，先将问题进行分解，即一个一个预测context。&lt;/p&gt;
&lt;p&gt;下图是continuous skip-gram的模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/Big_Model_continuous_skip_gram&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708180328724&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上述模型中，若词表非常大，则训练效率很慢。采用负采样来提高训练效率。&lt;/p&gt;
&lt;p&gt;负采样：不把整个词表作为负例，只选几个词表中不是需要predict的词来作为负例来提高计算效率。&lt;/p&gt;
&lt;h4 id=&#34;sub-sampling&#34;&gt;Sub-Sampling
&lt;/h4&gt;&lt;p&gt;Sub-Sampling：平衡常见词和罕见词出现的概率&lt;/p&gt;
&lt;p&gt;常见词，可能包含的语义比较少，如&amp;quot;的&amp;quot;等，需在训练时去掉，下面是词被去掉的概率
&lt;/p&gt;
$$
p=1- \sqrt{\frac{t}{f(w)}}
$$&lt;p&gt;
f(w)为一个词出现的频度，t是可自己定义的值。&lt;/p&gt;
&lt;h3 id=&#34;循环神经网络rnn&#34;&gt;循环神经网络RNN
&lt;/h3&gt;&lt;p&gt;RNN:Recurrent Neural Network.&lt;/p&gt;
&lt;p&gt;其处理序列数据时，会进行顺序记忆。&lt;/p&gt;
&lt;p&gt;下面是一个常见的模型结构&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250708183236474.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708183236474&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;rnn单元&#34;&gt;RNN单元
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250708183317584.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708183317584&#34;
	
	
&gt;
&lt;/p&gt;
$$
h_i = tanh(W_x x_i + W_h h_{i-1} + b) \\
y_i =F(h_i)
$$&lt;p&gt;
RNN模型是每一个RNN单元的复制，其参数是一样的，有利于实现参数共享，使得模型能够泛化到不同长度的样本，节省参数量&lt;/p&gt;
&lt;h4 id=&#34;rnn的应用与问题&#34;&gt;RNN的应用与问题
&lt;/h4&gt;&lt;p&gt;应用场景：序列标注，序列预测，图片描述，文本分类。&lt;/p&gt;
&lt;p&gt;问题：容易产生梯度消失、梯度爆炸。&lt;/p&gt;
&lt;p&gt;进行反向传播时，有：
&lt;/p&gt;
$$
h_i = \tanh(W_x x_i + W_h h_{i-1} + b) \\
\Delta w_1 = \frac{\partial Loss}{\partial w_2} = \frac{\partial Loss}{\partial h_n} \cdot \frac{\partial h_n}{\partial h_{n-1}} \cdot \frac{\partial h_{n-1}}{\partial h_{n-2}} \cdot \; \cdots \; \cdot \frac{\partial h_3}{\partial h_2} \cdot \frac{\partial h_2}{\partial w_2} (反向传播时，链式法则展开)
$$&lt;p&gt;
根据$\frac{\partial h_n}{\partial h_{n-1}}$ 讨论。当 $\frac{\partial h_n}{\partial h_{n-1}} &amp;gt; 1$  ，随着网络层数增多，梯度会像滚雪球一样 &lt;strong&gt;指数级增大&lt;/strong&gt; 。极端情况下，梯度过大可能让参数更新变得异常剧烈，模型参数值飙升，训练过程难以稳定，甚至无法收敛，这就是&lt;strong&gt;梯度爆炸&lt;/strong&gt;现象。   当 $\frac{\partial h_n}{\partial h_{n-1}} &amp;lt; 1$ ，随着网络层数不断加深，梯度会 &lt;strong&gt;指数级衰减&lt;/strong&gt; 。传到前面层（比如靠近输入的层 ）时，梯度会变得极其微小，几乎接近 0 。这会导致这些层的参数更新停滞，模型很难学到深层有意义的特征，训练效果大打折扣，就是&lt;strong&gt;梯度消失问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;RNN单元的变体：GRU、LSTM。核心：计算时，保存周围的记忆，来捕捉远距离的依赖性。&lt;/p&gt;
&lt;h4 id=&#34;门控循环单元gru&#34;&gt;门控循环单元（GRU）
&lt;/h4&gt;&lt;p&gt;GRU:Gated Recurrent Unit&lt;/p&gt;
&lt;p&gt;GRU是RNN的一个变体，其包括更新门以及重置门。&lt;/p&gt;
&lt;p&gt;重置门：考虑到上一层的隐藏状态对当前激活，可通过计算获得一个新的临时的激活。当重置门$r_i \approx 0$时，新的激活值和上一状态几乎没有关系。
&lt;/p&gt;
$$
r_i=\sigma(W_x^{(r)}x_i + W_h^{(r)}h_{i-1} + b^{(r)} )
$$&lt;p&gt;
更新门：权衡目前新得到的激活$h_i$和过去状态$h_{i-1}$的影响。
&lt;/p&gt;
$$
z_i = \sigma(W_x^{(z)} x_i + W_h^{(z)} h_{i-1} + b^{(z)} )
$$&lt;p&gt;
则新的激活值
&lt;/p&gt;
$$
\widetilde{h_i}=tanh(W_xx_i + r_i * W_h h_{i-1} + b )
$$&lt;p&gt;
最终隐藏层的状态：
&lt;/p&gt;
$$
h_i = z_i * h_{i-1} + (1 - z_i)* \widetilde{h_i}
$$&lt;h4 id=&#34;长短期记忆网络lstm&#34;&gt;长短期记忆网络（LSTM）
&lt;/h4&gt;&lt;p&gt;LSTM：Long Short-Term Memory Network&lt;/p&gt;
&lt;p&gt;也是RNN的变体，可以学习长期的数据一来关系，关键：&lt;strong&gt;cell&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;遗忘门$f_t = \sigma(W \cdot [h_{t-1},x_t] + b_f )$，若$f_t =0$则直接丢弃上一个状态。&lt;/p&gt;
&lt;p&gt;输入门，决定有哪些信息可以存入到cell状态中去&lt;/p&gt;
&lt;p&gt;输入门$i_t$以及新的待选的cell状态如下：
&lt;/p&gt;
$$
i_t= \sigma(W_i \cdot [h_{t-1},x_t] + b_i ) \\
\widetilde{C_t} = tanh(W_C \cdot [h_{t-1},x_t] + b_C )
$$&lt;p&gt;
则cell状态：$C_t = f_t * C_{t-1} + i_t * \widetilde{C_t}$&lt;/p&gt;
&lt;p&gt;输出门，决定哪些信息可以进行输出
&lt;/p&gt;
$$
o_t =\sigma(W_o \cdot [h_{t-1},x_t] + b_o ) \\
h_t = o_t * tamh(C_t)
$$&lt;h4 id=&#34;双向rnn&#34;&gt;双向RNN
&lt;/h4&gt;&lt;p&gt;RNN的变体&lt;/p&gt;
&lt;p&gt;让当前的predit不仅取决于过去的状态，而且取决于未来的状态&lt;/p&gt;
&lt;h2 id=&#34;transformer&#34;&gt;Transformer
&lt;/h2&gt;&lt;h3 id=&#34;注意力机制&#34;&gt;注意力机制
&lt;/h3&gt;&lt;p&gt;解决信息瓶颈。核心思想：在decoder的每一步都把encoder端所有的向量提供给decoder模型。&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先利用RNN得到一个向量$s_t$&lt;/li&gt;
&lt;li&gt;再利用向量$s_1$与encoder所有向量做点积，得到注意力分数$e^t=[s_t^Th_1,\cdots,s_t^Th_N]$&lt;/li&gt;
&lt;li&gt;利用softmax将注意力分数变为一个概率分布$\alpha^t=softmax(e^t)$，decoder更关注概率越大的位置的encoder向量。&lt;/li&gt;
&lt;li&gt;利用概率分布对encoder向量进行加权平均$o_t=\sum_{i=1}^N \alpha_i^th_i$&lt;/li&gt;
&lt;li&gt;拼接$[o_t;s_t]$得到最终用于生成predict的向量&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;注意力机制的各种变式&#34;&gt;注意力机制的各种变式
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;若decoder端向量s&lt;strong&gt;维度&lt;/strong&gt;与encoder端向量$h_i$不一样，则计算注意力分数时，需添加一个权重矩阵&lt;/li&gt;
&lt;/ol&gt;
$$
e_i=s^T W h_i
$$&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Additive attention：使用一层前馈神经网络&lt;/li&gt;
&lt;/ol&gt;
$$
e_i=v^T tanh(W_1 h_i + W_2 s)
$$&lt;h4 id=&#34;注意力机制的特点&#34;&gt;注意力机制的特点
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;decoder端每次生成的时候，可以关注到encoder端所有信息，解决信息瓶颈问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缓解RNN中的梯度消失问题。通过在encoder和decoder之间提供一种直接连接的方式，防止梯度在RNN中传播过长，进而导致梯度消失。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;给神经网络模型，提供了可解释性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transformer机制&#34;&gt;Transformer机制
&lt;/h3&gt;&lt;h4 id=&#34;模型结构&#34;&gt;模型结构
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;输入层，将一个文本序列切成一个小的单元token，然后通过embedding可以化为一个向量表示。
&lt;ol&gt;
&lt;li&gt;transformer采用Byte Pair Encoding的方式来对文本进行切分（BPE方法）&lt;/li&gt;
&lt;li&gt;在每个位置加上一个token的位置向量，叫positional encoding，用来表示它在文本序列中的位置&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;主要图层部分，主要由encoder或者decoder的Transformer block堆叠而成。&lt;/li&gt;
&lt;li&gt;输出层，线性层的变换和softmax，输出一个在词表上的概率分布&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;输入编码-bpepe&#34;&gt;输入编码 BPE，PE
&lt;/h4&gt;&lt;p&gt;BPE过程：首先将语料库中出现的所有单词切分为一个个字母，然后通过统计在语料库中每一个byte gram出现的数量，一个一个把频度最高的Byte gram抽象成一个词加入词表中。&lt;/p&gt;
&lt;p&gt;byte gram：连续两个相邻位置字母拼到一起的组合&lt;/p&gt;
&lt;p&gt;PE（Positional Encoding）：通过在原有的embedding上加上一个位置向量，让不同位置的单词具有不同的表示，进而让Transformer block可以进行区分。&lt;/p&gt;
&lt;p&gt;首先假设经过BPE和embedding之后的向量维度d，则位置编码也是一个维度为的向量，Transformer采用基于三角函数的方法来得到位置向量。具体公式如下：
&lt;/p&gt;
$$
PE_{(pos,2i)}=sin(pos/1000^{2i/d}) \\
PE_{pos,2i+1}=cos(pos/1000^{2i/d})
$$&lt;p&gt;
其中pos表示当前token在句子中的位置，是从0到这个序列长度的一个数。i为从0到d/2的一个数，表示当前这个位置在embedding中的index。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input = BPE + PE&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;encoder-block&#34;&gt;Encoder Block
&lt;/h4&gt;&lt;p&gt;整体由两大块组成，分别为Muti-Head Attention网络，Feed-Forward Network前馈神经网络（本质上是一个带激活函数的MLP全连接）&lt;/p&gt;
&lt;p&gt;两个技巧：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;残差连接：将输入和输出直接相加，缓解模型过深后带来的梯度消失问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;正则化：将输入变为一个均值为0，方差为一的分布，解决梯度消失与梯度爆炸。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;注意力机制-1&#34;&gt;&lt;strong&gt;注意力机制&lt;/strong&gt;
&lt;/h5&gt;&lt;p&gt;&lt;strong&gt;输入&lt;/strong&gt;为&lt;strong&gt;query q&lt;/strong&gt;和key-value(k，v)的集合。query和key向量的维度为$d_k$，Value向量的维度为$d_v$。&lt;/p&gt;
&lt;p&gt;**输出：**先对value向量做加权平均
&lt;/p&gt;
$$
A(q,K,V)=\sum_i\frac{e^{q\cdot k_i}}{\sum_j e^{q\cdot k_j}}v_i
$$&lt;p&gt;
当有多个q时，其可以组成矩阵Q，则：
&lt;/p&gt;
$$
A(Q,K,V)=softmax(QK^T)V
$$&lt;p&gt;
&lt;strong&gt;Scaled Dot-Product Attention:&lt;/strong&gt;
若没有Scale，当$d_k$增大时，$q^T k$的方差也随之增大，则softmax后概率分布可能会更尖锐，导致梯度越来越小，使得参数不利于更新。&lt;/p&gt;
&lt;p&gt;加入Scale，如下：
&lt;/p&gt;
$$
A(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$&lt;p&gt;
使得注意力分数方差依然为1.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意力机制的流程如下：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250711172044512.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250711172044512&#34;
	
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;多头注意力机制&#34;&gt;多头注意力机制
&lt;/h5&gt;&lt;p&gt;采用多个结构相同，但参数不同的注意力模块，组成多头注意力机制。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250711172243703.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250711172243703&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;每一个注意力头的计算方式为：
&lt;/p&gt;
$$
head_i=A(QW_i^Q,KW_i^K,VW_i^V)
$$&lt;p&gt;
当每一个注意力头得到输出后，将输出在维度上进行拼接，然后通过一个线性层进行整合：
&lt;/p&gt;
$$
MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O
$$&lt;p&gt;
一个Encodr Block的结构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250711173107761.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250711173107761&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;decoder-block&#34;&gt;Decoder Block
&lt;/h4&gt;&lt;p&gt;一个Decoder Block的结构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250711173227452.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250711173227452&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;Masked Attention Score：将注意力分数修改为$-\infty$，使得在softmax时，使之变0。用于decoder在文本生成时，不受前面生成文本的影响。&lt;/p&gt;
&lt;h3 id=&#34;优点和缺点&#34;&gt;优点和缺点
&lt;/h3&gt;&lt;h4 id=&#34;优点&#34;&gt;优点
&lt;/h4&gt;&lt;p&gt;表示能力强，本身适合并行计算，成为预训练语言模型的一个主要框架&lt;/p&gt;
&lt;h4 id=&#34;缺点&#34;&gt;缺点
&lt;/h4&gt;&lt;p&gt;对于参数敏感，优化困难。处理文本复杂度与文本长度$n$为平方关系$O(n^2)$，导致对长度特别长的文本束手无策。&lt;/p&gt;
&lt;h2 id=&#34;预训练语言模型&#34;&gt;预训练语言模型
&lt;/h2&gt;&lt;p&gt;对语言模型的预训练的一个过程&lt;/p&gt;
&lt;h3 id=&#34;plmspre-trained-lanuage-models&#34;&gt;PLMs（Pre-trained Lanuage Models）
&lt;/h3&gt;&lt;p&gt;好处：在语言模型的预训练之后，学习到的知识可以非常容易地去迁移到各种下游任务。&lt;/p&gt;
&lt;p&gt;预训练语言模型整体可以分为两种范式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature-based：作为feature的提取器，在大规模的预料上预训练好模型参数后，把它编码的表示作为一个固定的feature，来交给下游做具体任务的模型，作为下游任务模型的输入。&lt;/li&gt;
&lt;li&gt;Fine-turning（主流）：&lt;strong&gt;通过特定领域数据对预训练模型进行针对性优化，以提升其在特定任务上的性能。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;masked-lm&#34;&gt;Masked LM
&lt;/h3&gt;&lt;p&gt;BERT为了解决LM是单向的过程，提出Masked LM。 其是Bert预训练中最核心的任务。&lt;/p&gt;
&lt;p&gt;基本思想：利用双向信息来预测target token。&lt;strong&gt;掩盖掉$k %$（15% in Bert）的input word，然后去预测这些被掩盖的word。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;mask太少，预训练花的时间很长。&lt;/p&gt;
&lt;p&gt;mask太多，文章可利用的信息太少。&lt;/p&gt;
&lt;h3 id=&#34;few-shotin-context-learning&#34;&gt;few-shot/in-context learning
&lt;/h3&gt;&lt;p&gt;zero-shot：不给任何数据，只给任务的描述，就能完成任务&lt;/p&gt;
&lt;p&gt;few-shot：给一点样例，使之完成任务。&lt;/p&gt;
&lt;p&gt;in-context learning：没有针对下游任务做任何参数的更新，只是在上下文中给了这个任务的描述以及样例，通过语言模型自回归地生成去完成任务&lt;/p&gt;
&lt;p&gt;Mixture of Experts：把模型参数分成一块一块的模块，每次模型的输入会调用其中的部分子模块来参与计算。每一个子模块相当于experts。&lt;/p&gt;
&lt;h2 id=&#34;transformer-使用&#34;&gt;Transformer 使用
&lt;/h2&gt;&lt;h3 id=&#34;pipeline&#34;&gt;Pipeline
&lt;/h3&gt;&lt;p&gt;自动帮忙选择适配任务的模型&lt;/p&gt;
&lt;h3 id=&#34;tokenization&#34;&gt;Tokenization
&lt;/h3&gt;&lt;p&gt;不同的模型有不同的tokenization方式&lt;/p&gt;
&lt;h2 id=&#34;prompt-learning&#34;&gt;Prompt-learning
&lt;/h2&gt;&lt;p&gt;Promopt-learning给模型额外加一些上下文，去trigger一些想要的token，再用这些token进行后续的处理，进行分类等操作。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;prompt-learning使用预训练语言模型作为基础的encoders&lt;/li&gt;
&lt;li&gt;其加入了一些额外的包含mask的内容，称为template&lt;/li&gt;
&lt;li&gt;将标签映射为标签词的过程，称为verbalizer&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;verbalizer&#34;&gt;Verbalizer
&lt;/h3&gt;&lt;p&gt;构成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;人经验的选取一个初始词&lt;/li&gt;
&lt;li&gt;利用初始词，进行同义词改写扩大词表&lt;/li&gt;
&lt;li&gt;利用初始词，用额外的知识库扩大词表&lt;/li&gt;
&lt;li&gt;把一个长label分解为多个label&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bmtrain&#34;&gt;BMtrain
&lt;/h2&gt;&lt;h3 id=&#34;数据并行&#34;&gt;数据并行
&lt;/h3&gt;&lt;p&gt;把数据分成3份，送至3张显卡上，且每张显卡复制所有的参数。每张显卡对数据进行前向传播和反向传播，得到各自的梯度。将所有梯度进行聚合，利用聚合好的参数调整模型。&lt;/p&gt;
&lt;h4 id=&#34;分布式数据并行&#34;&gt;分布式数据并行
&lt;/h4&gt;&lt;p&gt;其没有参数服务器。&lt;/p&gt;
&lt;p&gt;每张显卡各自完成参数的更新，让所有显卡参数更新保持一致。&lt;/p&gt;
&lt;h3 id=&#34;模型并行&#34;&gt;模型并行
&lt;/h3&gt;&lt;p&gt;以线性层为例，把模型分成很多个小的部分，即将参数分成n份，n为显卡的个数，分别发送至每张显卡上，每张显卡上的参数与完整的输入数据进行乘法。最后进行聚合。 由于每张显卡都需要处理完整的输入数据，导致其中间计算结果并没有减少，仍然可能造成显存的溢出。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>

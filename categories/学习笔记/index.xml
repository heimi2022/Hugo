<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>学习笔记 on WJJ</title>
        <link>https://heimi2022.github.io/Hugo/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
        <description>Recent content in 学习笔记 on WJJ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Wjj</copyright>
        <lastBuildDate>Thu, 22 May 2025 16:36:28 +0800</lastBuildDate><atom:link href="https://heimi2022.github.io/Hugo/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Machine-Learning</title>
        <link>https://heimi2022.github.io/Hugo/p/machine-learning/</link>
        <pubDate>Thu, 22 May 2025 16:36:28 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/machine-learning/</guid>
        <description>&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;h3 id=&#34;什么是机器学习&#34;&gt;什么是机器学习
&lt;/h3&gt;&lt;p&gt;定义如下，一个程序被认为能从 &lt;strong&gt;经验 E&lt;/strong&gt; 中学习，解决 &lt;strong&gt;任务 T&lt;/strong&gt;，达到 &lt;strong&gt;性能度量值 P&lt;/strong&gt;，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。&lt;/p&gt;
&lt;p&gt;例如下棋游戏:&lt;br&gt;
E 就是程序上万次的自我练习的经验。&lt;br&gt;
T 就是下棋。&lt;br&gt;
P 就是它在与一些新的对手比赛时，赢得比赛的概率。&lt;/p&gt;
&lt;h3 id=&#34;监督学习&#34;&gt;监督学习
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;监督学习&lt;/strong&gt; 指的就是我们给学习算法一个 &lt;strong&gt;数据集&lt;/strong&gt;。这个数据集由“正确答案”组成。学习算法再根据这个数据集作出预测，算出更多的正确答案。&lt;br&gt;
监督学习的两个例子, &lt;strong&gt;回归问题 与 分类问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;回归问题 : 推测出一个 &lt;strong&gt;连续&lt;/strong&gt; 值的输出结果。&lt;br&gt;
例如 : 卖水果，数据集 3 斤卖 10 元左右，预测 4 斤能卖多少。&lt;/p&gt;
&lt;p&gt;分类问题 : 推测出一组 &lt;strong&gt;离散&lt;/strong&gt; 的结果。&lt;br&gt;
例如 : 识别红绿灯。&lt;/p&gt;
&lt;h3 id=&#34;无监督学习&#34;&gt;无监督学习
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;无监督学习&lt;/strong&gt; 指的是一种学习策略，它交给算法大量的数据，并让算法为我们从数据中找出某种结构。&lt;br&gt;
无监督学习中 &lt;strong&gt;已知数据没有任何的标签&lt;/strong&gt; 是指数据 &lt;strong&gt;有相同的标签或者就是没标签&lt;/strong&gt;。&lt;br&gt;
无监督学习的例子，&lt;strong&gt;聚类算法、鸡尾酒算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;聚类算法 : 将数据分为几类不同的 &lt;strong&gt;簇&lt;/strong&gt;。&lt;br&gt;
例子 : 谷歌新闻，同一主题新闻归为一类; 市场分割。&lt;/p&gt;
&lt;p&gt;鸡尾酒算法 : 分离麦克风接收到的不同的人的声音与环境声音。&lt;/p&gt;
&lt;h2 id=&#34;单变量线性回归linear-regression-with-one-variable&#34;&gt;单变量线性回归(Linear Regression with One Variable)
&lt;/h2&gt;&lt;h3 id=&#34;模型表示&#34;&gt;模型表示
&lt;/h3&gt;&lt;p&gt;样本数目 : 小写 &lt;strong&gt;m&lt;/strong&gt;&lt;br&gt;
训练集 : &lt;strong&gt;Training Set&lt;/strong&gt;&lt;br&gt;
特征/输入变量 : &lt;strong&gt;$x$&lt;/strong&gt;&lt;br&gt;
目标/输出变量 : &lt;strong&gt;$y$&lt;/strong&gt;&lt;br&gt;
训练集中的实例 : &lt;strong&gt;($x$, $y$)&lt;/strong&gt;&lt;br&gt;
第 $i$ 个观察实例 : &lt;strong&gt;($x^{(i)}$, $y^{(i)}$)&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;学习算法的解决方案或函数也称为假设(hypothesis) : h&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;模型表示就是找到将训练集给学习算法找到一个合适的函数 $h$ , $y = h(x)$&lt;/p&gt;
&lt;p&gt;函数 $h_\theta(x) = \theta_0 + \theta_1x$&lt;br&gt;
由于只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。&lt;/p&gt;
&lt;h3 id=&#34;代价函数&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;建模误差: 模型所预测的值与训练集中实际值之间的差距 $h_\theta(x^{(i)}) - y^{(i)}$&lt;/p&gt;
&lt;p&gt;平方误差函数 $J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$&lt;br&gt;
平方误差函数是代价函数之一，对于大多数问题，特别是回归问题，其都是个合理的选择。&lt;br&gt;
为什么是 $\frac{1}{2m}$:&lt;br&gt;
除以 m 是为了消除样本数量 m 对 J 的影响&lt;br&gt;
除以 2 是为了抵消对 J 关于 θ 求偏导数时式子多出的一个 2，使计算更方便&lt;/p&gt;
&lt;p&gt;我们的目标就是找到一个合适的参数 $\theta_0,\theta_1$ 使得代价函数最小。 即 Goal: $\underset{\theta_0,\theta_1}{minimize}J(\theta_0,\theta_1)$&lt;/p&gt;
&lt;h3 id=&#34;梯度下降&#34;&gt;梯度下降
&lt;/h3&gt;&lt;p&gt;梯度下降是一个用来 &lt;strong&gt;求函数最小值&lt;/strong&gt; 的算法，可以用其来求出使代价函数 $J$ 最小的参数 $\theta_0$ 和 $\theta_1$ 的值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;梯度下降的思想&lt;/strong&gt; : 开始时，随机选择一组参数 $(\theta_0,\theta_1)$, 计算代价函数, 然后寻找下一个能让代价函数下降最多的参数组合。持续这种操作，直至找到一个局部最小值。&lt;br&gt;
局部最小值不等同于全局最小值。不同的初始参数会可能会得到完全不同的局部最小值。&lt;/p&gt;
&lt;h4 id=&#34;批量梯度下降算法batch-gradient-descent&#34;&gt;批量梯度下降算法(batch gradient descent)
&lt;/h4&gt;&lt;p&gt;批量 batch 指的是，在梯度下降的每一步中，都用到来所有的训练样本。&lt;/p&gt;
&lt;p&gt;公式为:
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat until convergence（重复直至收敛）} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad (\text{for } j = 0 \text{ and } j = 1) \\
&amp;\}
\end{aligned}
$$&lt;p&gt;其中 $\alpha$ 是学习率，其决定了我们沿代价函数梯度方向向下迈出的幅度有多大。&lt;br&gt;
$\alpha$ 太小，需要很多步才能够达到局部最低点。&lt;br&gt;
$\alpha$ 太大，梯度下降法可能会越过最低点，甚至可能无法收敛，导致发散。&lt;/p&gt;
&lt;p&gt;在梯度下降法中，当参数取值接近局部最低点时，梯度下降法会自动采取更小的幅度，因为代价函数导数会趋向于 0。&lt;/p&gt;
&lt;h2 id=&#34;多变量线性回归&#34;&gt;多变量线性回归
&lt;/h2&gt;&lt;h3 id=&#34;多维特征&#34;&gt;多维特征
&lt;/h3&gt;&lt;p&gt;新的注释：&lt;/p&gt;
&lt;p&gt;$n$ 代表特征的数量&lt;/p&gt;
&lt;p&gt;$x^{(i)}$ 代表第 $i$ 个训练实例，是特征数据的第 $i$ 行，是一个 **向量 **&lt;/p&gt;
&lt;p&gt;$x_j^{(i)}$ 代表特征矩阵中第 $i$ 行第 $j$ 个特征&lt;/p&gt;
&lt;p&gt;多变量的假设 $h$ 表示为：&lt;br&gt;
&lt;/p&gt;
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
$$&lt;p&gt;
此时模型中参数为一个 $n+1$ 维向量&lt;/p&gt;
&lt;p&gt;任何一个训练实例也都是一个 $n+1$ 维向量&lt;/p&gt;
&lt;p&gt;特征矩阵 X 的维度是 $m*(n+1)$&lt;/p&gt;
&lt;p&gt;因此公式可以简化为&lt;/p&gt;
$$
h_\theta(x)=\theta^TX
$$&lt;h3 id=&#34;多变量梯度下降&#34;&gt;多变量梯度下降
&lt;/h3&gt;&lt;p&gt;代价函数为所有建模误差的平方和，即:&lt;br&gt;
&lt;/p&gt;
$$
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2
$$&lt;p&gt;多变量线性回归的批量梯度下降算法为：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1,...\theta_n) \\ 
&amp;\}
\end{aligned}
$$&lt;p&gt;即:&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2 \\
&amp;\}
\end{aligned}
$$&lt;p&gt;
求导后得到：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha  \frac{1}{m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)}) \cdot x_j^{(i)} \\
&amp;\}
\end{aligned}
$$&lt;h3 id=&#34;梯度下降法&#34;&gt;梯度下降法
&lt;/h3&gt;&lt;h4 id=&#34;特征缩放&#34;&gt;特征缩放
&lt;/h4&gt;&lt;p&gt;使特征具有相近的尺度有助于梯度下降宣发更快的收敛。&lt;/p&gt;
&lt;p&gt;方法：将所有特征的尺度都尽量缩放到-1 到 1 &lt;strong&gt;这种小范围&lt;/strong&gt; 内，也不能太小。&lt;/p&gt;
&lt;p&gt;最简单的，令 $x_n=\frac{x_n-\mu_n}{s_n}$, 其中 $\mu_n$ 是平均值，$s_n$ 是标准差。&lt;/p&gt;
&lt;h4 id=&#34;学习率&#34;&gt;学习率
&lt;/h4&gt;&lt;p&gt;梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制 &lt;strong&gt;迭代次数-代价函数&lt;/strong&gt; 的图表来观测算法在何时收敛。&lt;/p&gt;
&lt;p&gt;梯度下降算法的每次迭代受学习率 $\alpha $ 影响：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha$ 太小，则到达收敛所需的迭代次数很大，但理论上终会收敛。&lt;/li&gt;
&lt;li&gt;$\alpha $ 太大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;调整学习率时可以采用以下 &lt;strong&gt;策略&lt;/strong&gt;：从 0.001 开始每次上在原基础上乘 3 倍，&lt;strong&gt;最终选择一个较大的可以使得代价函数收敛的 $\alpha $&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如：0.001，0.003，0.01，0.03，0.1&lt;/p&gt;
&lt;h3 id=&#34;特征选择与多项式回归&#34;&gt;特征选择与多项式回归
&lt;/h3&gt;&lt;h4 id=&#34;特征选择&#34;&gt;特征选择
&lt;/h4&gt;&lt;p&gt;建立模型时，可以 &lt;strong&gt;根据需要选择适合模型的特征&lt;/strong&gt; 而不局限于数据集中的特征。比如：数据集给出房子的长和宽，预测房价时，可以创造一个特征为 *&lt;em&gt;面积 = 长 &lt;em&gt;宽&lt;/em&gt;&lt;/em&gt; 作为模型的新特征。&lt;/p&gt;
&lt;h4 id=&#34;多项式回归&#34;&gt;多项式回归
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;线性回归并不适合所有数据&lt;/strong&gt;，有时需要曲线来适应数据，比如：&lt;/p&gt;
&lt;p&gt;三次方模型 $h_\theta(x)=\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$&lt;/p&gt;
&lt;p&gt;使用多项式回归模型，特征缩放很有必要。&lt;/p&gt;
&lt;h3 id=&#34;正规方程&#34;&gt;正规方程
&lt;/h3&gt;&lt;p&gt;当特征数量小于 10000 时（对于目前的计算机），通常采用正规方程的方法找到一组最优参数 $\theta$。&lt;/p&gt;
&lt;p&gt;正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial \theta_j} J(\theta_j)=0$，&lt;/p&gt;
&lt;p&gt;其 &lt;strong&gt;解&lt;/strong&gt; 为 $\theta=(X^TX)^{-1}X^Ty$。$X$ 为训练集特征矩阵，$y$ 为训练集结果。&lt;/p&gt;
&lt;h4 id=&#34;梯度下降于正规方程的比较&#34;&gt;梯度下降于正规方程的比较
&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;梯度下降&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;正规方程&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;需要选择学习率 $\alpha $&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;不需要&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;需要多次迭代&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一次运算得出&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;特征数量 n 大时也可用&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;矩阵逆的计算时间复杂度为 $O(n^3)$。因此 n 小于 10000 时，可以接受&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;使用于各种类型&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;只适用于线性模型。不适合逻辑回归等其它模型。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;以下情况时，正规方程不能用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有两个特征线性相关。  解决：使用正规方程前，剔除相关特征。&lt;/li&gt;
&lt;li&gt;含有大量特征，进而出现 $m \leqslant n$ 时。  解决：剔除特征，用较少特征反应尽可能多的内容。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;逻辑回归&#34;&gt;逻辑回归
&lt;/h2&gt;&lt;p&gt;逻辑回归算法是一种分类算法。&lt;/p&gt;
&lt;p&gt;分类问题预测的变量 $y$ 为 &lt;strong&gt;离散值&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;假说表示&#34;&gt;假说表示
&lt;/h3&gt;&lt;p&gt;引入一个 &lt;strong&gt;逻辑函数 Sigmoid function&lt;/strong&gt;，S 形函数，公式为 $g(z)=\frac{1}{1+e^{-z}}$。其图像如下所示：&lt;/p&gt;
&lt;center&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_sigmoid_function.png&#34;
         width=&#34;40%&#34;&gt;
    &lt;br&gt;
    sigmoid function
&lt;/center&gt;
&lt;p&gt;令逻辑回归模型的假设是：$h_\theta(x)=g(\theta^T X)$, $X$ 代表特征向量，$g$ 代表逻辑函数。&lt;/p&gt;
&lt;p&gt;此时模型的输出变量范围始终在 0 和 1 之间。&lt;/p&gt;
&lt;p&gt;$h_\theta(x)$ 表示，对于给定的输入变量，根据选择的参数，计算输出变量为 1 的可能性，即：
&lt;/p&gt;
$$
h_\theta(x)= P(y = 1|x;\theta)
$$&lt;h3 id=&#34;决策边界&#34;&gt;决策边界
&lt;/h3&gt;&lt;p&gt;在逻辑回归中，假设：&lt;br&gt;
&lt;/p&gt;
$$
y = \begin{cases}
1, &amp; \text{if } h_\theta(x) \geq 0.5 \\
0, &amp; \text{if } h_\theta(x) &lt; 0.5
\end{cases}
$$&lt;p&gt;因此当 $\theta^T x \geqslant 0$ 时，预测 $y=1$，当 $\theta^T x &amp;lt; 0$ 时，预测 $y=0$。&lt;/p&gt;
&lt;p&gt;因此对于一组参数 $\theta$，有 $\theta^T x=0$，这条 &lt;strong&gt;曲线&lt;/strong&gt; 即为模型的决策边界。&lt;/p&gt;
&lt;h3 id=&#34;代价函数-1&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;为什么逻辑回归的代价函数和线性回归的不同？由于 sigmoid 函数的非线性，导致逻辑回归的假设函数带入代价函数时，会导致代价函数有很多局部最小值，其是一个非凸函数。因此需对代价函数进行修正。&lt;/p&gt;
&lt;p&gt;定义逻辑回归的代价函数 $J(\theta)=\frac{1}{m} \sum^m_{i=1} Cost(h_\theta(x^{(i)}),y^{(i)})$，其中
&lt;/p&gt;
$$
Cost(h_\theta(x^{(i)}), y^{(i)}) = \begin{cases}
   -log( h_\theta(x)) \quad &amp;  if\quad y = 1, &amp;  \\
   -log( 1- h_\theta(x)) &amp;  if\quad y = 0, &amp;  \\
\end{cases}
\overset{简化}{=} -ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))
$$&lt;h4 id=&#34;逻辑回归中-cost-函数特点&#34;&gt;逻辑回归中 Cost 函数特点
&lt;/h4&gt;&lt;p&gt;当实际的 $y=1$ 且 $h_\theta(x)=1$ 时，误差为 0，当 $y=1$ 但 $h_\theta(x)$ 不为 1 时，误差随着 $h_\theta(x)$ 的变小而变大。当 $h_\theta(x)$ 减小为 0，表示预测与实际完全相反，其误差为无穷大，这时代价函数将会受到很大的惩罚；当实际的 $y=0$ 且 $h_\theta(x)=0$ 时，误差为 0，当 $y=0$ 但 $h_\theta(x)$ 不为 0 时，误差随着 $h_\theta(x)$ 的变大而变大。&lt;/p&gt;
&lt;h4 id=&#34;简化后的代价函数&#34;&gt;简化后的代价函数
&lt;/h4&gt;$$
\begin{aligned}
J(\theta)&amp; = \frac{1}{m} \sum^m_{i = 1} Cost(h_\theta(x^{(i)}), y^{(i)}) &amp; \\
&amp; = -\frac{1}{m} [\sum^m_{i = 1}-ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))] &amp;\\
\end{aligned}
$$&lt;p&gt;最小化代价函数的方法 &lt;strong&gt;是梯度下降法&lt;/strong&gt;：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{Repeat} \{ \\
&amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp;\}
\end{aligned}
$$&lt;p&gt;虽然其形式上与线性回归的梯度下降法相同，但由于回归的模型假设发生了变化，两者是完全不同的函数。&lt;/p&gt;
&lt;h3 id=&#34;高级优化&#34;&gt;高级优化
&lt;/h3&gt;&lt;p&gt;优化算法：共轭梯度法 BFGS（变尺度法）和 L-BFGS（限制遍尺度法）。&lt;/p&gt;
&lt;p&gt;优点：不需要手动选择学习率。&lt;/p&gt;
&lt;h3 id=&#34;多类别分类&#34;&gt;多类别分类
&lt;/h3&gt;&lt;p&gt;假设有 n 个类别，则将其分为 &lt;strong&gt;n 个&lt;/strong&gt; 1 对（n-1）的 &lt;strong&gt;二元分类问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;记每一个分类问题的模型为：$h_\theta ^{(i)} (x)= p(y=i|x;\theta)$，其中 $i=(1,2,3,&amp;hellip;,k)$。&lt;/p&gt;
&lt;p&gt;最终输出结果为让 $h_\theta ^{(i)} (x)$ 最大的 i，即 $\mathop{max}\limits_{i} ; h_\theta ^{(i)} (x)$。&lt;/p&gt;
&lt;h2 id=&#34;正则化&#34;&gt;正则化
&lt;/h2&gt;&lt;h3 id=&#34;过拟合&#34;&gt;过拟合
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;过拟合&lt;/strong&gt; 指通过学习得到的假设可能能够非常好的适应训练集（代价函数可能几乎为 0），但是可能 &lt;strong&gt;不会&lt;/strong&gt; 推广到预测新的数据。&lt;/p&gt;
&lt;p&gt;当有 &lt;strong&gt;过多的变量但只有很少的训练数据&lt;/strong&gt; 时很容易出现过拟合问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欠拟合&lt;/strong&gt; 指模型不能适应训练集。&lt;/p&gt;
&lt;h4 id=&#34;解决过拟合&#34;&gt;解决过拟合
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;减少特征变量的数目 &amp;ndash; 人工选择去除一些变量；模型算法选择（如 PCA）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;正则化&lt;/strong&gt; &amp;ndash; 保留所有的特征变量，但减小参数 $\theta$ 的大小。 其适用于有很多特征变量，且每一个特征变量看起来对预测结果都有一点用的情况。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;代价函数-2&#34;&gt;代价函数
&lt;/h3&gt;&lt;h4 id=&#34;正则化的直观理解&#34;&gt;正则化的直观理解
&lt;/h4&gt;&lt;p&gt;假设模型为 $h_\theta(x)=\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 + \theta_4 x^4$，且高次项导致了过拟合的产生，则 &lt;strong&gt;假如能让高次项的系数趋向于 0，就能够缓解过拟合的问题&lt;/strong&gt; 了。 因此修改代价函数 ，对 $\theta_3 \text{和}\theta_4$ 设置一些惩罚，如下所示：
&lt;/p&gt;
$$
J(\theta)=\frac{1}{2m}[\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2+1000 \theta_3^2 + 1000 \theta_4^2]
$$&lt;p&gt;
则通过这样的代价函数选择出来的 $\theta_3 \text{和}\theta_4$ 对预测结果的影响就很小了。&lt;/p&gt;
&lt;h4 id=&#34;带正则化参数的代价函数&#34;&gt;带正则化参数的代价函数
&lt;/h4&gt;&lt;p&gt;修改后的代价函数如下：
&lt;/p&gt;
$$
J(\theta)=\frac{1}{2m}[\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum^n_{j = 1}\theta_j^2]
$$&lt;p&gt;
其中，$\lambda$ 成为正则化参数。&lt;/p&gt;
&lt;p&gt;假如 $\lambda$ 太大，则所有 $\theta$ 都会趋于 0，除了 $\theta_0$，这样最终模型为一条平行于 x 轴的直线，导致欠拟合。&lt;/p&gt;
&lt;h3 id=&#34;正则化线性回归&#34;&gt;正则化线性回归
&lt;/h3&gt;&lt;h4 id=&#34;梯度下降-1&#34;&gt;梯度下降
&lt;/h4&gt;$$
\begin{aligned}
&amp; \text{Repeat until convergence} \{ \\
&amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
&amp; \theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp; \}
\end{aligned}
$$&lt;p&gt;正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新柜子的基础上令 $\theta$ 值减少了一个额外的值。&lt;/p&gt;
&lt;h4 id=&#34;正规化&#34;&gt;正规化
&lt;/h4&gt;$$
\theta = (X^T X + \lambda \begin{bmatrix}
0 &amp; &amp; &amp; &amp; &amp; \\
&amp; 1 &amp; &amp; &amp; &amp; \\
&amp; &amp; 1 &amp; &amp; &amp; \\
&amp; &amp; &amp; \cdot &amp; &amp; \\
&amp; &amp; &amp; &amp; \cdot &amp; \\
&amp; &amp; &amp; &amp; &amp; 1 \\
\end{bmatrix}^{-1})^{-1} X^Ty
$$&lt;p&gt;图中矩阵尺寸为 $(n+1)*(n+1)$&lt;/p&gt;
&lt;h3 id=&#34;正则化逻辑回归&#34;&gt;正则化逻辑回归
&lt;/h3&gt;&lt;p&gt;正则化逻辑回归的代价函数：
&lt;/p&gt;
$$
J(\theta) = \frac{1}{m} [\sum^m_{i = 1}-ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))] + \frac{\lambda}{2m} \sum^n_{j = 1}\theta^2_j 
$$&lt;p&gt;
则梯度下降算法为：
&lt;/p&gt;
$$
\begin{aligned}
&amp; \text{Repeat until convergence} \{ \\
&amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
&amp; \theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp; \}
\end{aligned}
$$&lt;h2 id=&#34;神经网络&#34;&gt;神经网络
&lt;/h2&gt;&lt;h3 id=&#34;模型表示-1&#34;&gt;模型表示
&lt;/h3&gt;&lt;p&gt;神经网络模型建立在很多神经元之上，每一个神经元又是一个学习模型。这些神经元也叫激活单元（activation unit）。&lt;/p&gt;
&lt;p&gt;神经网络模型是许多逻辑单元按照不同的层级组织起来的网络，每一层的输出变量都是下一层的输入变量。&lt;/p&gt;
&lt;p&gt;引入以下标记法来描述模型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha _i ^{(j)}$代表第$j$层的第$i$个激活单元。&lt;/li&gt;
&lt;li&gt;$\theta^{(j)}$代表从第$j$层映射到第$j+1$层时的权重矩阵。其&lt;strong&gt;尺寸&lt;/strong&gt;为以第$j+1$层的激活单元数量作为行数，以第$j$层的激活单元数量&lt;strong&gt;加一&lt;/strong&gt;作为列数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;举例&#34;&gt;举例
&lt;/h4&gt;&lt;center&gt;
    &lt;img src=&#34;https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_Neural_Network_layer3_example&#34;
         width=&#34;40%&#34;&gt;
    &lt;br&gt;
    sigmoid function
&lt;/center&gt;
&lt;p&gt;对于上图所示模型，激活单元和输出分别表达为：
&lt;/p&gt;
$$
\begin{aligned}
&amp; \alpha_1^{(2)}=g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1  + \Theta_{12}^{(1)}x_2  + \Theta_{13}^{(1)}x_3 ) \\
&amp; \alpha_2^{(2)}=g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1  + \Theta_{22}^{(1)}x_2  + \Theta_{23}^{(1)}x_3 ) \\
&amp; \alpha_3^{(2)}=g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1  + \Theta_{32}^{(1)}x_2  + \Theta_{33}^{(1)}x_3 ) \\
&amp; h_\theta(x)=g(\Theta_{10}^{(2)}\alpha_0^{(2)} + \Theta_{11}^{(2)}\alpha_1^{(2)}  + \Theta_{12}^{(2)}\alpha_2^{(2)}  + \Theta_{13}^{(2)}\alpha_3^{(2)} ) \\
\end{aligned}
$$&lt;p&gt;
将例中这种从左往右的算法称为&lt;strong&gt;向前传播算法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;若只看输出部分，其输出方式类似逻辑回归模型，可以将$\alpha $看成是更高级的特征值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多类分类&lt;/strong&gt;问题：输出层用n个神经元，表示n类。&lt;/p&gt;
&lt;h2 id=&#34;神经网络的学习&#34;&gt;神经网络的学习
&lt;/h2&gt;&lt;h3 id=&#34;代价函数-3&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;新的标记方法：&lt;/p&gt;
&lt;p&gt;假设神经网络的训练样本由$m$个，每个包含一组输入$x$和一组输出信号$y$&lt;/p&gt;
&lt;p&gt;$L$表示神经网络的层数&lt;/p&gt;
&lt;p&gt;$S_l$表示$l$层中神经元的个数，$S_L$表示最后一层中神经元的个数&lt;/p&gt;
&lt;p&gt;神经网络的分类定义为两种情况：二类分类和多类分类，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. 二类分类：$S_L=1$,$y=0 \text{ or } 1$
1. $K$类分类：$S_L=k$,$y_i=1$表示分到第$i$类；$(K&amp;gt;2)$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在神经网络中，$h_\theta(x)$是一个维度为$K$的向量，则：$h_\theta(x) \in R^K,(h_\theta(x))_i=i^{th} output$&lt;/p&gt;
&lt;p&gt;代价函数如下：
&lt;/p&gt;
$$
J(\Theta)=-\frac{1}{m}[\sum_{i=1}^m \sum_{k=1}^k y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})log(1 - (h_\Theta(x^{(i)}))_k) ] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{ji}^{(l)})^2
$$&lt;ol&gt;
&lt;li&gt;对于每一行特征：由于有$K$个特征，可以利用循环，对每一行特征都预测$K$个不同结果，然后利用循环将$K$个预测偏差累加。&lt;/li&gt;
&lt;li&gt;对于正则化这一项，遍历所有层的所有参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;反向传播算法&#34;&gt;反向传播算法
&lt;/h3&gt;&lt;p&gt;为了&lt;strong&gt;计算代价函数的偏导数&lt;/strong&gt;$\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)$，需要采用一种反向传播算法。&lt;/p&gt;
&lt;p&gt;反向传播算法：首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。&lt;/p&gt;
&lt;h4 id=&#34;反向传播算法中误差的计算&#34;&gt;反向传播算法中误差的计算
&lt;/h4&gt;&lt;p&gt;用$\delta$来表示误差。对于一个4层的网络，反向传播误差分3步计算：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\delta^{(4)}=a^{(4)}-y$&lt;/li&gt;
&lt;li&gt;$\delta^{(3)}=(\Theta^{(3)})^T \delta^{(4)}*g&amp;rsquo;(z^{(3)})$，$g&amp;rsquo;(z^{(3)})$是sigmoid函数的导数，且$g&amp;rsquo;(z^{(3)})= a^{(3)} * (1 - a^{(3)})$，$(\Theta^{(3)})^T \delta^{(4)}$是权重导致的误差的和。&lt;/li&gt;
&lt;li&gt;$\delta^{(2)}=(\Theta^{(2)})^T \delta^{(4)}*g&amp;rsquo;(z^{(2)})$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于4层的网络，设隐藏层都只有两个神经元，则$\delta_2^{(2)}=\Theta_{12}^{(2)} \delta_1^{(3)}+\Theta_{22}^{(2)} \delta_2^{(3)}$。&lt;/p&gt;
&lt;h4 id=&#34;忽略代价函数中的正则化项&#34;&gt;忽略代价函数中的正则化项
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;忽略代价函数中正则化项，则$\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)} \delta_i^{l+1}$。其中上下标的含义：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$l$代表目前所计算的是第几层&lt;/p&gt;
&lt;p&gt;$j$代表目前计算层中的激活单元的下标，也就是下一层第$j$个输入变量的下标&lt;/p&gt;
&lt;p&gt;$i$代表下一层中误差单元的下标&lt;/p&gt;
&lt;h4 id=&#34;考虑正则化处理&#34;&gt;考虑正则化处理
&lt;/h4&gt;&lt;p&gt;则用$\Delta_{ij}^{(l)}$来表示误差矩阵，则算法表示为：
&lt;/p&gt;
$$
\begin{aligned}
\text{for } i = 1 : m \quad \{ \\
\quad &amp; a^{(1)} := x^{(i)} \\
\quad &amp; \text{向前传播计算 } a^{(l)} \text{ for } l = 1, 2, 3, \dots, L \\
\quad &amp; \text{using } \delta^{(L)} := a^{(L)} - y^{(i)} \\
\quad &amp; \text{执行向后传播算法计算直至第二层的所有误差} \\
\quad &amp; \Delta^{(l)}_{ij} := \Delta^{(l)}_{ij} + a^{(l)}_j \delta^{(l+1)}_i \\
\}
\end{aligned}
$$&lt;p&gt;
在求出了$\Delta_{ij}^{(l)}$之后，则可求出代价函数的偏导数：
&lt;/p&gt;
$$
\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}:= \begin{cases}
\frac{1}{m}\Delta_{ij}^{(l)}+\lambda \Theta_{ij}^{(l)}&amp; ,\quad if \quad j \ne 0 \\
\frac{1}{m}\Delta_{ij}^{(l)}&amp;, \quad if \quad j = 0
\end{cases}
$$&lt;h3 id=&#34;梯度检验&#34;&gt;梯度检验
&lt;/h3&gt;&lt;p&gt;对一个复杂模型使用梯度下降算法时，可能会存在一些不易察觉的错误，意味着虽然代价看上去不断减小，但是最终的结果可能并不是最优解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方法：梯度检验&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思想&lt;/strong&gt;：通过估计梯度值来检验我门计算的导数值是否真的是预期的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法&lt;/strong&gt;：在代价函数上，对于某个特定的$\theta$，我们计算出在$\theta - \epsilon $处和$\theta + \epsilon $的代价值，然后求两个代价的平均，即为估计值。&lt;/p&gt;
&lt;p&gt;针对$\theta_1$进行检验的示例：
&lt;/p&gt;
$$
\frac{\partial}{\partial_{\theta_1}}=\frac{J(\theta_1 + \epsilon,\theta_2,\cdots,\theta_n)-J(\theta_1 - \epsilon,\theta_2,\cdots,\theta_n)}{2 \epsilon}
$$&lt;p&gt;
对于反向传播算法，计算出的偏导数存储在矩阵$D_{ij}^{(l)}$中，检验时，将该矩阵展开成为向量；与此同时，将$\theta$矩阵也展开为向量，针对每一个$\theta$都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，同$D_{ij}^{(l)}$比较。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>

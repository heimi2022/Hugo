[{"content":"公式 上标与下标 用^来输出上标，使用_来输出下标，使用{}包含作用范围。\n空格表示 两个quad空格\u0026mdash;\u0026mdash;\u0026mdash;a \\qquad b\u0026mdash;\u0026mdash;\u0026mdash;两个m的宽度\nquad空格\u0026mdash;\u0026mdash;\u0026mdash;a \\quad b\u0026mdash;\u0026mdash;\u0026mdash;一个m的宽度\n大空格\u0026mdash;\u0026mdash;\u0026mdash;a\\ b\u0026mdash;\u0026mdash;\u0026mdash;1/3m宽度\n中等空格\u0026mdash;\u0026mdash;\u0026mdash;a;b\u0026mdash;\u0026mdash;\u0026mdash;2/7m宽度\n小空格\ta,b\u0026mdash;\u0026mdash;\u0026mdash;a,b\u0026mdash;\u0026mdash;\u0026mdash;1/6m宽度\n没有空格\u0026mdash;\u0026mdash;\u0026mdash;ab\n紧贴\u0026mdash;\u0026mdash;\u0026mdash;a!b\u0026mdash;\u0026mdash;\u0026mdash;缩进1/6m宽度\n数学结构 分式 \\frac{a}{b} : $\\frac{a}{b}$ 根式 \\sqrt[n]{a} : $\\sqrt[n]{a}$ 求导 f\u0026rsquo; : $f'$ 求和$\\sum$ ：\\sum 极限 $\\lim\\limits_{x\\to\\infty}$：\\lim\\limits_{x\\to\\infty} 积分号$\\int$：\\int 其它 上划线:\\overline{a},下划线\\underline{a},上右箭头\\overrightarrow{a}:$\\overrightarrow{a}$ 数学函数 1.三角函数 $\\sin$：\\sin ; $\\cos$：\\cos;\n数学符号 偏导数 $\\partial$ ：\\partial 数学运算 点乘 $a \\cdot b$：\\cdot\n叉乘 $a \\times b$：\\times\n除以 $a \\div b$：\\div\n希腊字母 $\\alpha \\beta \\gamma \\delta \\epsilon \\zeta \\eta \\theta$：\\alpha \\beta \\gamma \\delta \\epsilon \\zeta \\eta \\theta $\\iota \\kappa \\lambda \\mu \\nu \\omicron \\xi \\pi$ ：\\iota \\kappa \\lambda \\mu \\nu \\omicron \\xi \\pi $\\rho \\sigma \\tau \\upsilon \\phi \\chi \\psi \\omega$：\\rho \\sigma \\tau \\upsilon \\phi \\chi \\psi \\omega $\\Alpha \\Beta \\Gamma \\Delta \\Epsilon \\Zeta \\Eta \\Theta$：\\Alpha \\Beta \\Gamma \\Delta \\Epsilon \\Zeta \\Eta \\Theta $\\Iota \\Kappa \\Lambda \\Mu \\Nu \\Xi \\Omicron \\Pi$：\\Iota \\Kappa \\Lambda \\Mu \\Nu \\Xi \\Omicron \\Pi $\\Rho \\Sigma \\Tau \\Upsilon \\Phi \\Chi \\Psi \\Omega$：\\Rho \\Sigma \\Tau \\Upsilon \\Phi \\Chi \\Psi \\Omega 关系符号 $\\leqslant ， \\geqslant ，\\le ，\\ge$：\\leqslant ， \\geqslant ，\\le ，\\ge \\in ：$\\in$ \\ne：$\\ne$ 括号 大括号 \\{ 其它 \\underset{123}{abc} : $\\underset{123}{abc}$ 使用 \\limits 将下标放在某个文字或者符号的正下方。 用**\\mathop**{文本}命令将文本转化成数学符号 省略符号： \\cdots $\\cdots$ , \\ddots $\\ddots$ , \\vdots $\\vdots$ 矩阵、条件表达式、方程组 语法：\n1 2 3 \\begin{类型} 公式内容 \\end{类型} 无框矩阵 matrix 1 2 3 4 \\begin{matrix} x \u0026amp; y \\\\ z \u0026amp; v \\end{matrix} $$ \\begin{matrix} x \u0026 y \\\\ z \u0026 v \\end{matrix} $$有框矩阵 pmatrix、bmatrix、vmatrix vmatrix 1 2 3 4 \\begin{vmatrix} x \u0026amp; y \\\\ z \u0026amp; v \\end{vmatrix} $$ \\begin{vmatrix} x \u0026 y \\\\ z \u0026 v \\end{vmatrix} $$ Vmatrix $$ \\begin{Vmatrix} x \u0026 y \\\\ z \u0026 v \\end{Vmatrix} $$ bmatrix $$ \\begin{bmatrix} 0 \u0026 \\cdots \u0026 0 \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 0 \u0026 \\cdots \u0026 0 \\end{bmatrix} $$ Bmatrix $$ \\begin{Bmatrix} a \u0026 b \\\\ c \u0026 d \\end{Bmatrix} $$ pmatrix $$ \\begin{pmatrix} x \u0026 y \\\\ z \u0026 v \\end{pmatrix} $$条件表达式、方程组：cases 1 2 3 4 5 f(n) = \\begin{cases} 1, \u0026amp; \\text{if n is 1} \\\\ 2, \u0026amp; \\text{if n is 2} \\end{cases} $$ f(n) = \\begin{cases} 1, \u0026 \\text{if n is 1} \\\\ 2, \u0026 \\text{if n is 2} \\end{cases} $$多行等式 aligned 1 2 3 4 \\begin{aligned} f(x) \u0026amp; = 2 \\\\ \u0026amp; = 1+1 \\\\ \\end{aligned} $$ \\begin{aligned} f(x) \u0026 = 2 \\\\ \u0026 = 1+1 \\\\ \\end{aligned} $$","date":"2025-05-23T20:12:04+08:00","permalink":"https://heimi2022.github.io/Hugo/p/latex%E8%AF%AD%E6%B3%95/","title":"LaTex语法"},{"content":"插入公式 行内公式 : 输入 $ + esc ,则可实现 $ 公式 $ 行间公式 : 输入 $$ + 回车 ,则可实现 $$ 公式 $$ 图片排版 适用html语言\n1 2 3 4 5 6 7 8 \u0026lt;center\u0026gt; 创建中间对齐图片 \u0026lt;img src = \u0026#34;地址\u0026#34; width =\u0026#34;40\\%\u0026#34;\u0026gt; 缩放比率，还有height可选 \u0026lt;br\u0026gt; 插入回车，回车之前重复图片操作可并排放图 图注 \u0026lt;/center\u0026gt; ","date":"2025-05-23T20:12:04+08:00","permalink":"https://heimi2022.github.io/Hugo/p/markdown%E8%AF%AD%E6%B3%95/","title":"Markdown语法"},{"content":"问题解决 git push fatal: unable to access ‘https://github.com/\u0026hellip;/.git‘: Could not resolve host: github.com 1 2 git config --global --unset http.proxy git config --global --unset https.proxy 解决方案：cmd下命令执行 ipconfig/flushdns 清理dns缓存\n","date":"2025-05-22T17:20:11+08:00","permalink":"https://heimi2022.github.io/Hugo/p/git%E4%BD%BF%E7%94%A8/","title":"Git使用"},{"content":"引言 什么是机器学习 定义如下，一个程序被认为能从 经验 E 中学习，解决 任务 T，达到 性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n例如下棋游戏:\nE 就是程序上万次的自我练习的经验。\nT 就是下棋。\nP 就是它在与一些新的对手比赛时，赢得比赛的概率。\n监督学习 监督学习 指的就是我们给学习算法一个 数据集。这个数据集由“正确答案”组成。学习算法再根据这个数据集作出预测，算出更多的正确答案。\n监督学习的两个例子, 回归问题 与 分类问题。\n回归问题 : 推测出一个 连续 值的输出结果。\n例如 : 卖水果，数据集 3 斤卖 10 元左右，预测 4 斤能卖多少。\n分类问题 : 推测出一组 离散 的结果。\n例如 : 识别红绿灯。\n无监督学习 无监督学习 指的是一种学习策略，它交给算法大量的数据，并让算法为我们从数据中找出某种结构。\n无监督学习中 已知数据没有任何的标签 是指数据 有相同的标签或者就是没标签。\n无监督学习的例子，聚类算法、鸡尾酒算法\n聚类算法 : 将数据分为几类不同的 簇。\n例子 : 谷歌新闻，同一主题新闻归为一类; 市场分割。\n鸡尾酒算法 : 分离麦克风接收到的不同的人的声音与环境声音。\n单变量线性回归(Linear Regression with One Variable) 模型表示 样本数目 : 小写 m\n训练集 : Training Set\n特征/输入变量 : $x$\n目标/输出变量 : $y$\n训练集中的实例 : ($x$, $y$)\n第 $i$ 个观察实例 : ($x^{(i)}$, $y^{(i)}$)\n学习算法的解决方案或函数也称为假设(hypothesis) : h\n模型表示就是找到将训练集给学习算法找到一个合适的函数 $h$ , $y = h(x)$\n函数 $h_\\theta(x) = \\theta_0 + \\theta_1x$\n由于只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。\n代价函数 建模误差: 模型所预测的值与训练集中实际值之间的差距 $h_\\theta(x^{(i)}) - y^{(i)}$\n平方误差函数 $J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$\n平方误差函数是代价函数之一，对于大多数问题，特别是回归问题，其都是个合理的选择。\n为什么是 $\\frac{1}{2m}$:\n除以 m 是为了消除样本数量 m 对 J 的影响\n除以 2 是为了抵消对 J 关于 θ 求偏导数时式子多出的一个 2，使计算更方便\n我们的目标就是找到一个合适的参数 $\\theta_0,\\theta_1$ 使得代价函数最小。 即 Goal: $\\underset{\\theta_0,\\theta_1}{minimize}J(\\theta_0,\\theta_1)$\n梯度下降 梯度下降是一个用来 求函数最小值 的算法，可以用其来求出使代价函数 $J$ 最小的参数 $\\theta_0$ 和 $\\theta_1$ 的值。\n梯度下降的思想 : 开始时，随机选择一组参数 $(\\theta_0,\\theta_1)$, 计算代价函数, 然后寻找下一个能让代价函数下降最多的参数组合。持续这种操作，直至找到一个局部最小值。\n局部最小值不等同于全局最小值。不同的初始参数会可能会得到完全不同的局部最小值。\n批量梯度下降算法(batch gradient descent) 批量 batch 指的是，在梯度下降的每一步中，都用到来所有的训练样本。\n公式为: $$ \\begin{aligned} \u0026\\text{repeat until convergence（重复直至收敛）} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) \\quad (\\text{for } j = 0 \\text{ and } j = 1) \\\\ \u0026\\} \\end{aligned} $$其中 $\\alpha$ 是学习率，其决定了我们沿代价函数梯度方向向下迈出的幅度有多大。\n$\\alpha$ 太小，需要很多步才能够达到局部最低点。\n$\\alpha$ 太大，梯度下降法可能会越过最低点，甚至可能无法收敛，导致发散。\n在梯度下降法中，当参数取值接近局部最低点时，梯度下降法会自动采取更小的幅度，因为代价函数导数会趋向于 0。\n多变量线性回归 多维特征 新的注释：\n$n$ 代表特征的数量\n$x^{(i)}$ 代表第 $i$ 个训练实例，是特征数据的第 $i$ 行，是一个 **向量 **\n$x_j^{(i)}$ 代表特征矩阵中第 $i$ 行第 $j$ 个特征\n多变量的假设 $h$ 表示为：\n$$ h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n $$ 此时模型中参数为一个 $n+1$ 维向量\n任何一个训练实例也都是一个 $n+1$ 维向量\n特征矩阵 X 的维度是 $m*(n+1)$\n因此公式可以简化为\n$$ h_\\theta(x)=\\theta^TX $$多变量梯度下降 代价函数为所有建模误差的平方和，即:\n$$ J(\\theta_0,\\theta_1,...,\\theta_n)=\\frac{1}{2m}\\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2 $$多变量线性回归的批量梯度下降算法为： $$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1,...\\theta_n) \\\\ \u0026\\} \\end{aligned} $$即:\n$$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} \\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2 \\\\ \u0026\\} \\end{aligned} $$ 求导后得到： $$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} \\\\ \u0026\\} \\end{aligned} $$梯度下降法 特征缩放 使特征具有相近的尺度有助于梯度下降宣发更快的收敛。\n方法：将所有特征的尺度都尽量缩放到-1 到 1 这种小范围 内，也不能太小。\n最简单的，令 $x_n=\\frac{x_n-\\mu_n}{s_n}$, 其中 $\\mu_n$ 是平均值，$s_n$ 是标准差。\n学习率 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制 迭代次数-代价函数 的图表来观测算法在何时收敛。\n梯度下降算法的每次迭代受学习率 $\\alpha $ 影响：\n$\\alpha$ 太小，则到达收敛所需的迭代次数很大，但理论上终会收敛。 $\\alpha $ 太大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。 调整学习率时可以采用以下 策略：从 0.001 开始每次上在原基础上乘 3 倍，最终选择一个较大的可以使得代价函数收敛的 $\\alpha $。\n如：0.001，0.003，0.01，0.03，0.1\n特征选择与多项式回归 特征选择 建立模型时，可以 根据需要选择适合模型的特征 而不局限于数据集中的特征。比如：数据集给出房子的长和宽，预测房价时，可以创造一个特征为 *面积 = 长 宽 作为模型的新特征。\n多项式回归 线性回归并不适合所有数据，有时需要曲线来适应数据，比如：\n三次方模型 $h_\\theta(x)=\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$\n使用多项式回归模型，特征缩放很有必要。\n正规方程 当特征数量小于 10000 时（对于目前的计算机），通常采用正规方程的方法找到一组最优参数 $\\theta$。\n正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\\frac{\\partial}{\\partial \\theta_j} J(\\theta_j)=0$，\n其 解 为 $\\theta=(X^TX)^{-1}X^Ty$。$X$ 为训练集特征矩阵，$y$ 为训练集结果。\n梯度下降于正规方程的比较 梯度下降 正规方程 需要选择学习率 $\\alpha $ 不需要 需要多次迭代 一次运算得出 特征数量 n 大时也可用 矩阵逆的计算时间复杂度为 $O(n^3)$。因此 n 小于 10000 时，可以接受 使用于各种类型 只适用于线性模型。不适合逻辑回归等其它模型。 以下情况时，正规方程不能用：\n有两个特征线性相关。 解决：使用正规方程前，剔除相关特征。 含有大量特征，进而出现 $m \\leqslant n$ 时。 解决：剔除特征，用较少特征反应尽可能多的内容。 逻辑回归 逻辑回归算法是一种分类算法。\n分类问题预测的变量 $y$ 为 离散值。\n假说表示 引入一个 逻辑函数 Sigmoid function，S 形函数，公式为 $g(z)=\\frac{1}{1+e^{-z}}$。其图像如下所示：\nsigmoid function 令逻辑回归模型的假设是：$h_\\theta(x)=g(\\theta^T X)$, $X$ 代表特征向量，$g$ 代表逻辑函数。\n此时模型的输出变量范围始终在 0 和 1 之间。\n$h_\\theta(x)$ 表示，对于给定的输入变量，根据选择的参数，计算输出变量为 1 的可能性，即： $$ h_\\theta(x)= P(y = 1|x;\\theta) $$决策边界 在逻辑回归中，假设：\n$$ y = \\begin{cases} 1, \u0026 \\text{if } h_\\theta(x) \\geq 0.5 \\\\ 0, \u0026 \\text{if } h_\\theta(x) \u003c 0.5 \\end{cases} $$因此当 $\\theta^T x \\geqslant 0$ 时，预测 $y=1$，当 $\\theta^T x \u0026lt; 0$ 时，预测 $y=0$。\n因此对于一组参数 $\\theta$，有 $\\theta^T x=0$，这条 曲线 即为模型的决策边界。\n代价函数 为什么逻辑回归的代价函数和线性回归的不同？由于 sigmoid 函数的非线性，导致逻辑回归的假设函数带入代价函数时，会导致代价函数有很多局部最小值，其是一个非凸函数。因此需对代价函数进行修正。\n定义逻辑回归的代价函数 $J(\\theta)=\\frac{1}{m} \\sum^m_{i=1} Cost(h_\\theta(x^{(i)}),y^{(i)})$，其中 $$ Cost(h_\\theta(x^{(i)}), y^{(i)}) = \\begin{cases} -log( h_\\theta(x)) \\quad \u0026 if\\quad y = 1, \u0026 \\\\ -log( 1- h_\\theta(x)) \u0026 if\\quad y = 0, \u0026 \\\\ \\end{cases} \\overset{简化}{=} -ylog( h_\\theta(x))-(1-y)log( 1- h_\\theta(x)) $$逻辑回归中 Cost 函数特点 当实际的 $y=1$ 且 $h_\\theta(x)=1$ 时，误差为 0，当 $y=1$ 但 $h_\\theta(x)$ 不为 1 时，误差随着 $h_\\theta(x)$ 的变小而变大。当 $h_\\theta(x)$ 减小为 0，表示预测与实际完全相反，其误差为无穷大，这时代价函数将会受到很大的惩罚；当实际的 $y=0$ 且 $h_\\theta(x)=0$ 时，误差为 0，当 $y=0$ 但 $h_\\theta(x)$ 不为 0 时，误差随着 $h_\\theta(x)$ 的变大而变大。\n简化后的代价函数 $$ \\begin{aligned} J(\\theta)\u0026 = \\frac{1}{m} \\sum^m_{i = 1} Cost(h_\\theta(x^{(i)}), y^{(i)}) \u0026 \\\\ \u0026 = -\\frac{1}{m} [\\sum^m_{i = 1}-ylog( h_\\theta(x))-(1-y)log( 1- h_\\theta(x))] \u0026\\\\ \\end{aligned} $$最小化代价函数的方法 是梯度下降法： $$ \\begin{aligned} \u0026\\text{Repeat} \\{ \\\\ \u0026 \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\\\ \u0026\\} \\end{aligned} $$虽然其形式上与线性回归的梯度下降法相同，但由于回归的模型假设发生了变化，两者是完全不同的函数。\n高级优化 优化算法：共轭梯度法 BFGS（变尺度法）和 L-BFGS（限制遍尺度法）。\n优点：不需要手动选择学习率。\n多类别分类 假设有 n 个类别，则将其分为 n 个 1 对（n-1）的 二元分类问题。\n记每一个分类问题的模型为：$h_\\theta ^{(i)} (x)= p(y=i|x;\\theta)$，其中 $i=(1,2,3,\u0026hellip;,k)$。\n最终输出结果为让 $h_\\theta ^{(i)} (x)$ 最大的 i，即 $\\mathop{max}\\limits_{i} ; h_\\theta ^{(i)} (x)$。\n正则化 过拟合 过拟合 指通过学习得到的假设可能能够非常好的适应训练集（代价函数可能几乎为 0），但是可能 不会 推广到预测新的数据。\n当有 过多的变量但只有很少的训练数据 时很容易出现过拟合问题。\n欠拟合 指模型不能适应训练集。\n解决过拟合 减少特征变量的数目 \u0026ndash; 人工选择去除一些变量；模型算法选择（如 PCA）。 正则化 \u0026ndash; 保留所有的特征变量，但减小参数 $\\theta$ 的大小。 其适用于有很多特征变量，且每一个特征变量看起来对预测结果都有一点用的情况。 代价函数 正则化的直观理解 假设模型为 $h_\\theta(x)=\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3 + \\theta_4 x^4$，且高次项导致了过拟合的产生，则 假如能让高次项的系数趋向于 0，就能够缓解过拟合的问题 了。 因此修改代价函数 ，对 $\\theta_3 \\text{和}\\theta_4$ 设置一些惩罚，如下所示： $$ J(\\theta)=\\frac{1}{2m}[\\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2+1000 \\theta_3^2 + 1000 \\theta_4^2] $$ 则通过这样的代价函数选择出来的 $\\theta_3 \\text{和}\\theta_4$ 对预测结果的影响就很小了。\n带正则化参数的代价函数 修改后的代价函数如下： $$ J(\\theta)=\\frac{1}{2m}[\\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda \\sum^n_{j = 1}\\theta_j^2] $$ 其中，$\\lambda$ 成为正则化参数。\n假如 $\\lambda$ 太大，则所有 $\\theta$ 都会趋于 0，除了 $\\theta_0$，这样最终模型为一条平行于 x 轴的直线，导致欠拟合。\n正则化线性回归 梯度下降 $$ \\begin{aligned} \u0026 \\text{Repeat until convergence} \\{ \\\\ \u0026 \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\\\ \u0026 \\theta_j := \\theta_j(1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\\\ \u0026 \\} \\end{aligned} $$正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新柜子的基础上令 $\\theta$ 值减少了一个额外的值。\n正规化 $$ \\theta = (X^T X + \\lambda \\begin{bmatrix} 0 \u0026 \u0026 \u0026 \u0026 \u0026 \\\\ \u0026 1 \u0026 \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 1 \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \\cdot \u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \\cdot \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 1 \\\\ \\end{bmatrix}^{-1})^{-1} X^Ty $$图中矩阵尺寸为 $(n+1)*(n+1)$\n正则化逻辑回归 正则化逻辑回归的代价函数： $$ J(\\theta) = \\frac{1}{m} [\\sum^m_{i = 1}-ylog( h_\\theta(x))-(1-y)log( 1- h_\\theta(x))] + \\frac{\\lambda}{2m} \\sum^n_{j = 1}\\theta^2_j $$ 则梯度下降算法为： $$ \\begin{aligned} \u0026 \\text{Repeat until convergence} \\{ \\\\ \u0026 \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\\\ \u0026 \\theta_j := \\theta_j(1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\\\ \u0026 \\} \\end{aligned} $$神经网络 模型表示 神经网络模型建立在很多神经元之上，每一个神经元又是一个学习模型。这些神经元也叫激活单元（activation unit）。\n神经网络模型是许多逻辑单元按照不同的层级组织起来的网络，每一层的输出变量都是下一层的输入变量。\n引入以下标记法来描述模型：\n$\\alpha _i ^{(j)}$代表第$j$层的第$i$个激活单元。 $\\theta^{(j)}$代表从第$j$层映射到第$j+1$层时的权重矩阵。其尺寸为以第$j+1$层的激活单元数量作为行数，以第$j$层的激活单元数量加一作为列数。 举例 3-Layer Neural model 对于上图所示模型，激活单元和输出分别表达为： $$ \\begin{aligned} \u0026 \\alpha_1^{(2)}=g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3 ) \\\\ \u0026 \\alpha_2^{(2)}=g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3 ) \\\\ \u0026 \\alpha_3^{(2)}=g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3 ) \\\\ \u0026 h_\\theta(x)=g(\\Theta_{10}^{(2)}\\alpha_0^{(2)} + \\Theta_{11}^{(2)}\\alpha_1^{(2)} + \\Theta_{12}^{(2)}\\alpha_2^{(2)} + \\Theta_{13}^{(2)}\\alpha_3^{(2)} ) \\\\ \\end{aligned} $$ 将例中这种从左往右的算法称为向前传播算法。\n若只看输出部分，其输出方式类似逻辑回归模型，可以将$\\alpha $看成是更高级的特征值。\n多类分类问题：输出层用n个神经元，表示n类。\n神经网络的学习 代价函数 新的标记方法：\n假设神经网络的训练样本由$m$个，每个包含一组输入$x$和一组输出信号$y$\n$L$表示神经网络的层数\n$S_l$表示$l$层中神经元的个数，$S_L$表示最后一层中神经元的个数\n神经网络的分类定义为两种情况：二类分类和多类分类，\n二类分类：$S_L=1$,$y=0 \\text{ or } 1$ $K$类分类：$S_L=k$,$y_i=1$表示分到第$i$类；$(K\u0026gt;2)$ 在神经网络中，$h_\\theta(x)$是一个维度为$K$的向量，则：$h_\\theta(x) \\in R^K,(h_\\theta(x))_i=i^{th} output$\n代价函数如下： $$ J(\\Theta)=-\\frac{1}{m}[\\sum_{i=1}^m \\sum_{k=1}^k y_k^{(i)}log(h_\\Theta(x^{(i)}))_k + (1 - y_k^{(i)})log(1 - (h_\\Theta(x^{(i)}))_k) ] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\Theta_{ji}^{(l)})^2 $$ 对于每一行特征：由于有$K$个特征，可以利用循环，对每一行特征都预测$K$个不同结果，然后利用循环将$K$个预测偏差累加。 对于正则化这一项，遍历所有层的所有参数。 反向传播算法 为了计算代价函数的偏导数$\\frac{\\partial}{\\partial\\theta_{ij}^{(l)}}J(\\Theta)$，需要采用一种反向传播算法。\n反向传播算法：首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。\n反向传播算法中误差的计算 用$\\delta$来表示误差。对于一个4层的网络，反向传播误差分3步计算：\n$\\delta^{(4)}=a^{(4)}-y$ $\\delta^{(3)}=(\\Theta^{(3)})^T \\delta^{(4)}*g\u0026rsquo;(z^{(3)})$，$g\u0026rsquo;(z^{(3)})$是sigmoid函数的导数，且$g\u0026rsquo;(z^{(3)})= a^{(3)} * (1 - a^{(3)})$，$(\\Theta^{(3)})^T \\delta^{(4)}$是权重导致的误差的和。 $\\delta^{(2)}=(\\Theta^{(2)})^T \\delta^{(4)}*g\u0026rsquo;(z^{(2)})$ 对于4层的网络，设隐藏层都只有两个神经元，则$\\delta_2^{(2)}=\\Theta_{12}^{(2)} \\delta_1^{(3)}+\\Theta_{22}^{(2)} \\delta_2^{(3)}$。\n忽略代价函数中的正则化项 忽略代价函数中正则化项，则$\\frac{\\partial}{\\partial\\theta_{ij}^{(l)}}J(\\Theta)=a_j^{(l)} \\delta_i^{l+1}$。其中上下标的含义：\n$l$代表目前所计算的是第几层\n$j$代表目前计算层中的激活单元的下标，也就是下一层第$j$个输入变量的下标\n$i$代表下一层中误差单元的下标\n考虑正则化处理 则用$\\Delta_{ij}^{(l)}$来表示误差矩阵，则算法表示为： $$ \\begin{aligned} \\text{for } i = 1 : m \\quad \\{ \\\\ \\quad \u0026 a^{(1)} := x^{(i)} \\\\ \\quad \u0026 \\text{向前传播计算 } a^{(l)} \\text{ for } l = 1, 2, 3, \\dots, L \\\\ \\quad \u0026 \\text{using } \\delta^{(L)} := a^{(L)} - y^{(i)} \\\\ \\quad \u0026 \\text{执行向后传播算法计算直至第二层的所有误差} \\\\ \\quad \u0026 \\Delta^{(l)}_{ij} := \\Delta^{(l)}_{ij} + a^{(l)}_j \\delta^{(l+1)}_i \\\\ \\} \\end{aligned} $$ 在求出了$\\Delta_{ij}^{(l)}$之后，则可求出代价函数的偏导数： $$ \\frac{\\partial}{\\partial\\theta_{ij}^{(l)}}J(\\Theta)=D_{ij}^{(l)}:= \\begin{cases} \\frac{1}{m}\\Delta_{ij}^{(l)}+\\lambda \\Theta_{ij}^{(l)}\u0026 ,\\quad if \\quad j \\ne 0 \\\\ \\frac{1}{m}\\Delta_{ij}^{(l)}\u0026, \\quad if \\quad j = 0 \\end{cases} $$梯度检验 对一个复杂模型使用梯度下降算法时，可能会存在一些不易察觉的错误，意味着虽然代价看上去不断减小，但是最终的结果可能并不是最优解。\n解决方法：梯度检验。\n思想：通过估计梯度值来检验我门计算的导数值是否真的是预期的。\n方法：在代价函数上，对于某个特定的$\\theta$，我们计算出在$\\theta - \\epsilon $处和$\\theta + \\epsilon $的代价值，然后求两个代价的平均，即为估计值。\n针对$\\theta_1$进行检验的示例： $$ \\frac{\\partial}{\\partial_{\\theta_1}}=\\frac{J(\\theta_1 + \\epsilon,\\theta_2,\\cdots,\\theta_n)-J(\\theta_1 - \\epsilon,\\theta_2,\\cdots,\\theta_n)}{2 \\epsilon} $$ 对于反向传播算法，计算出的偏导数存储在矩阵$D_{ij}^{(l)}$中，检验时，将该矩阵展开成为向量；与此同时，将$\\theta$矩阵也展开为向量，针对每一个$\\theta$都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，同$D_{ij}^{(l)}$比较。\n随机初始化 任何优化算法都需要一些初始的参数。\n对于神经网络，令所有的初试参数都为一个相同的值是不可行的，这意味着对于一个层的所有激活单元都为相同的值。进而在梯度下降时，对于每一个激活单元的误差都是相同的，这意味着高度的冗余。因此需要对参数进行随机初始化。\n神经网络的训练 网络结构 第一层的单元数：训练集的特征数量\n最后一层的单元数：训练集的结果的类的数量\n隐藏层：该层大小最好为1。如果层数大于1，应确保每个隐藏层的单元个数相同，通常情况下隐藏层单元个数越多越好。\n训练神经网络 参数随机初始化 正向传播算法，计算所有的$h_\\theta(x)$ 选择代价函数 反向传播算法，计算所有偏导数 梯度检验 使用优化算法来最小化代价函数 机器学习诊断法 评估一个假设 如何评估一个假设是否过拟合呢？ 将训练数据分为训练集和测试集。 一般训练集：测试集=7：3。\n用测试集计算误差 对于线性回归模型，可以利用测试集数据计算代价函数$J$。\n对于逻辑回归模型，除了可以用测试集数据计算代价函数外，还可利用误分类的比率对每一个测试集实例计算： $$ err(h_\\theta(x),y)= \\begin{cases} 1,\u0026if \\quad h_\\theta(x) \\ge 0.5 \\quad and \\quad y=0,or \\quad if \\quad h_\\theta(x) \u003c 0.5 \\quad and \\quad y=1 \\\\ 0，\u0026其它 \\end{cases} $$ 然后对计算结果求平均。\n模型选择和交叉验证集 越高次数的多项式模型越能够适应训练数据集，但适应训练数据集的模型不一定能够推广到一般状况。可以使用交叉验证集，简称验证集，来帮助选择模型。\n典型的数据集的比例，训练集：验证集：测试集=6：2：2。\n模型选择方法 使用训练集训练出几个模型 用几个模型分别对验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤3中选出的模型对测试集计算得出推广误差。 模型阶数和偏差/方差 对于训练集，当模型阶数$d$较小时，模型的拟合程度低，误差较大。随着$d$的增长。拟合程度提高，误差减小。\n对于验证集，误差随着$d$先减小后增大，转折点为模型开始过拟合的时候。\n训练集误差和验证集误差都比较大，且接近时： 偏差/欠拟合。\n验证集误差远大于训练集误差时：方差/过拟合。\npolynomial_d 正则化和偏差/方差 选择正则化参数 使用训练集训练几个不同程度正则化的模型。正则化参数一般从小到达，按2的倍数增加。 用这几个模型分别对验证集计算交叉验证误差 选择得出交叉验证误差最小的模型 用步骤3中选出的模型对测试集计算得出推广误差 偏差和方差 当$\\lambda$较小时。表现为方差，训练集误差较小（过拟合），交叉验证集误差较大。\n随着$\\lambda$的增加，训练集误差不断增加（欠拟合），交叉验证集误差先减小后增大。\nBias/variance as function of the regularzation 学习曲线 高偏差，欠拟合 对于高偏差，欠拟合的情况，曲线如下图所示：\nHigh bias 曲线有如下特点：对于验证集和训练集，误差都很大，且二者很接近。无论训练集多大，其误差不会有太大改变\n高方差，过拟合 对于高偏差，欠拟合的情况，曲线如下图所示：\nHigh variance 曲线有以下特点，训练集误差很小，验证集误差很大。但是增加训练集数据可以提高模型效果。\n总结 解决高方差：获取更多的训练数据，尝试减少特征的数量，增加正则化程度\n解决高偏差：增加特征的数量，增加多项式特征，减小正则化程度\n较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合。 较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，但可以通过正则化手段来调整而更加适应数据。\n通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。\n对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数。\n","date":"2025-05-22T16:36:28+08:00","permalink":"https://heimi2022.github.io/Hugo/p/machine-learning/","title":"Machine-Learning"},{"content":"启动 Hugo 服务 hugo server -D\n创建文章 命令创建 命令 : hugo new post/foldername/*.md 其模板来自 archetypes/default.md 手动创建 在post目录下，手动创建一个文件夹，在文件夹里创建md文件，并将需要的图片资源放入其中。 文章推送 git add . git commit -m\u0026quot;update\u0026quot; git push github 图床使用 创建Github仓库并生成Token 创建仓库 在 GitHub 上新建一个仓库（如 my-image-repo），确保仓库是公开的（Public）。 生成 Token 登录 GitHub，进入 Settings -\u0026gt; Developer settings -\u0026gt; Personal access tokens。 点击 Generate new token，勾选 repo 权限。 生成 Token 后，复制并保存（Token 只会显示一次）。 配置typora PicGo 打开 Typora 的配置文件：\n在 Typora 的图像设置中，点击 打开配置文件。\n这会打开一个 picgo.json 文件（如果没有，可以手动创建）。\n编辑 picgo.json 文件，添加 GitHub 图床的配置。以下是一个示例配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \u0026#34;picBed\u0026#34;: { \u0026#34;current\u0026#34;: \u0026#34;github\u0026#34;, \u0026#34;github\u0026#34;: { \u0026#34;repo\u0026#34;: \u0026#34;你的用户名/你的仓库名\u0026#34;, // 例如：your-username/my-image-repo \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, // 或 master \u0026#34;token\u0026#34;: \u0026#34;你的 GitHub Token\u0026#34;, // 生成的 Personal Access Token \u0026#34;path\u0026#34;: \u0026#34;assets/\u0026#34;, // 图片上传到仓库的 assets 目录 \u0026#34;customUrl\u0026#34;: \u0026#34;\u0026#34; // 如果需要自定义域名，可以填写 } }, \u0026#34;picgoPlugins\u0026#34;: { \u0026#34;github\u0026#34;: true // 确保启用 GitHub 插件 } } repo: 你的 GitHub 仓库名，格式为 用户名/仓库名。\nbranch: 仓库的分支，通常是 main 或 master。\ntoken: 你的 GitHub Personal Access Token（生成方法见下文）。\npath: 图片上传的路径，例如 assets/。\ncustomUrl: 如果需要自定义域名（如 CDN），可以填写。\n保存 picgo.json 文件。\n","date":"2025-05-22T15:53:57+08:00","image":"https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/cover_hu_3e3cbd19f9ff226a.png","permalink":"https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/","title":"Hugo使用"},{"content":"我的第一条博客送给杨思敏同学 ","date":"2025-05-20T21:27:55+08:00","permalink":"https://heimi2022.github.io/Hugo/p/to_ysm/","title":"To_YSM"}]
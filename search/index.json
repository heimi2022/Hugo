[{"content":"公式 上标与下标 用^来输出上标，使用_来输出下标，使用{}包含作用范围。\n空格表示 两个quad空格\u0026mdash;\u0026mdash;\u0026mdash;a \\qquad b\u0026mdash;\u0026mdash;\u0026mdash;两个m的宽度\nquad空格\u0026mdash;\u0026mdash;\u0026mdash;a \\quad b\u0026mdash;\u0026mdash;\u0026mdash;一个m的宽度\n大空格\u0026mdash;\u0026mdash;\u0026mdash;a\\ b\u0026mdash;\u0026mdash;\u0026mdash;1/3m宽度\n中等空格\u0026mdash;\u0026mdash;\u0026mdash;a;b\u0026mdash;\u0026mdash;\u0026mdash;2/7m宽度\n小空格\ta,b\u0026mdash;\u0026mdash;\u0026mdash;a,b\u0026mdash;\u0026mdash;\u0026mdash;1/6m宽度\n没有空格\u0026mdash;\u0026mdash;\u0026mdash;ab\n紧贴\u0026mdash;\u0026mdash;\u0026mdash;a!b\u0026mdash;\u0026mdash;\u0026mdash;缩进1/6m宽度\n数学结构 分式 \\frac{a}{b} : $\\frac{a}{b}$ 根式 \\sqrt[n]{a} : $\\sqrt[n]{a}$ 求导 f\u0026rsquo; : $f'$ 求和$\\sum$ ：\\sum 极限 $\\lim\\limits_{x\\to\\infty}$：\\lim\\limits_{x\\to\\infty} 积分号$\\int$：\\int 其它 上划线:\\overline{a},下划线\\underline{a},上右箭头\\overrightarrow{a}:$\\overrightarrow{a}$ 数学函数 1.三角函数 $\\sin$：\\sin ; $\\cos$：\\cos;\n数学符号 偏导数 $\\partial$ ：\\partial 数学运算 点乘 $a \\cdot b$：\\cdot\n叉乘 $a \\times b$：\\times\n除以 $a \\div b$：\\div\n希腊字母 $\\alpha \\beta \\gamma \\delta \\epsilon \\zeta \\eta \\theta$：\\alpha \\beta \\gamma \\delta \\epsilon \\zeta \\eta \\theta $\\iota \\kappa \\lambda \\mu \\nu \\omicron \\xi \\pi$ ：\\iota \\kappa \\lambda \\mu \\nu \\omicron \\xi \\pi $\\rho \\sigma \\tau \\upsilon \\phi \\chi \\psi \\omega$：\\rho \\sigma \\tau \\upsilon \\phi \\chi \\psi \\omega $\\Alpha \\Beta \\Gamma \\Delta \\Epsilon \\Zeta \\Eta \\Theta$：\\Alpha \\Beta \\Gamma \\Delta \\Epsilon \\Zeta \\Eta \\Theta $\\Iota \\Kappa \\Lambda \\Mu \\Nu \\Xi \\Omicron \\Pi$：\\Iota \\Kappa \\Lambda \\Mu \\Nu \\Xi \\Omicron \\Pi $\\Rho \\Sigma \\Tau \\Upsilon \\Phi \\Chi \\Psi \\Omega$：\\Rho \\Sigma \\Tau \\Upsilon \\Phi \\Chi \\Psi \\Omega 关系符号 $\\leqslant ， \\geqslant ，\\le ，\\ge$：\\leqslant ， \\geqslant ，\\le ，\\ge 括号 大括号 \\{ 其它 \\underset{123}{abc} : $\\underset{123}{abc}$ 使用 \\limits 将下标放在某个文字或者符号的正下方。 用**\\mathop**{文本}命令将文本转化成数学符号 省略符号： \\cdots $\\cdots$ , \\ddots $\\ddots$ , \\vdots $\\vdots$ 矩阵、条件表达式、方程组 语法：\n1 2 3 \\begin{类型} 公式内容 \\end{类型} 无框矩阵 matrix 1 2 3 4 \\begin{matrix} x \u0026amp; y \\\\ z \u0026amp; v \\end{matrix} $$ \\begin{matrix} x \u0026 y \\\\ z \u0026 v \\end{matrix} $$有框矩阵 pmatrix、bmatrix、vmatrix vmatrix 1 2 3 4 \\begin{vmatrix} x \u0026amp; y \\\\ z \u0026amp; v \\end{vmatrix} $$ \\begin{vmatrix} x \u0026 y \\\\ z \u0026 v \\end{vmatrix} $$ Vmatrix $$ \\begin{Vmatrix} x \u0026 y \\\\ z \u0026 v \\end{Vmatrix} $$ bmatrix $$ \\begin{bmatrix} 0 \u0026 \\cdots \u0026 0 \\\\ \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ 0 \u0026 \\cdots \u0026 0 \\end{bmatrix} $$ Bmatrix $$ \\begin{Bmatrix} a \u0026 b \\\\ c \u0026 d \\end{Bmatrix} $$ pmatrix $$ \\begin{pmatrix} x \u0026 y \\\\ z \u0026 v \\end{pmatrix} $$条件表达式、方程组：cases 1 2 3 4 5 f(n) = \\begin{cases} 1, \u0026amp; \\text{if n is 1} \\\\ 2, \u0026amp; \\text{if n is 2} \\end{cases} $$ f(n) = \\begin{cases} 1, \u0026 \\text{if n is 1} \\\\ 2, \u0026 \\text{if n is 2} \\end{cases} $$多行等式 aligned 1 2 3 4 \\begin{aligned} f(x) \u0026amp; = 2 \\\\ \u0026amp; = 1+1 \\\\ \\end{aligned} $$ \\begin{aligned} f(x) \u0026 = 2 \\\\ \u0026 = 1+1 \\\\ \\end{aligned} $$","date":"2025-05-23T20:12:04+08:00","permalink":"https://heimi2022.github.io/Hugo/p/latex%E8%AF%AD%E6%B3%95/","title":"LaTex语法"},{"content":"插入公式 行内公式 : 输入 $ + esc ,则可实现 $ 公式 $ 行间公式 : 输入 $$ + 回车 ,则可实现 $$ 公式 $$ 图片排版 适用html语言\n1 2 3 4 5 6 7 8 \u0026lt;center\u0026gt; 创建中间对齐图片 \u0026lt;img src = \u0026#34;地址\u0026#34; width =\u0026#34;40\\%\u0026#34;\u0026gt; 缩放比率，还有height可选 \u0026lt;br\u0026gt; 插入回车，回车之前重复图片操作可并排放图 图注 \u0026lt;/center\u0026gt; ","date":"2025-05-23T20:12:04+08:00","permalink":"https://heimi2022.github.io/Hugo/p/markdown%E8%AF%AD%E6%B3%95/","title":"Markdown语法"},{"content":"问题解决 git push fatal: unable to access ‘https://github.com/\u0026hellip;/.git‘: Could not resolve host: github.com git config \u0026ndash;global \u0026ndash;unset http.proxy git config \u0026ndash;global \u0026ndash;unset https.proxy\n解决方案：cmd下命令执行 ipconfig/flushdns 清理dns缓存\n","date":"2025-05-22T17:20:11+08:00","permalink":"https://heimi2022.github.io/Hugo/p/git%E4%BD%BF%E7%94%A8/","title":"Git使用"},{"content":"引言 什么是机器学习 定义如下，一个程序被认为能从 经验 E 中学习，解决 任务 T，达到 性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n例如下棋游戏:\nE 就是程序上万次的自我练习的经验。\nT 就是下棋。\nP 就是它在与一些新的对手比赛时，赢得比赛的概率。\n监督学习 监督学习 指的就是我们给学习算法一个 数据集。这个数据集由“正确答案”组成。学习算法再根据这个数据集作出预测，算出更多的正确答案。\n监督学习的两个例子, 回归问题 与 分类问题。\n回归问题 : 推测出一个 连续 值的输出结果。\n例如 : 卖水果，数据集 3 斤卖 10 元左右，预测 4 斤能卖多少。\n分类问题 : 推测出一组 离散 的结果。\n例如 : 识别红绿灯。\n无监督学习 无监督学习 指的是一种学习策略，它交给算法大量的数据，并让算法为我们从数据中找出某种结构。\n无监督学习中 已知数据没有任何的标签 是指数据 有相同的标签或者就是没标签。\n无监督学习的例子，聚类算法、鸡尾酒算法\n聚类算法 : 将数据分为几类不同的 簇。\n例子 : 谷歌新闻，同一主题新闻归为一类; 市场分割。\n鸡尾酒算法 : 分离麦克风接收到的不同的人的声音与环境声音。\n单变量线性回归(Linear Regression with One Variable) 模型表示 样本数目 : 小写 m\n训练集 : Training Set\n特征/输入变量 : $x$\n目标/输出变量 : $y$\n训练集中的实例 : ($x$, $y$)\n第 $i$ 个观察实例 : ($x^{(i)}$, $y^{(i)}$)\n学习算法的解决方案或函数也称为假设(hypothesis) : h\n模型表示就是找到将训练集给学习算法找到一个合适的函数 $h$ , $y = h(x)$\n函数 $h_\\theta(x) = \\theta_0 + \\theta_1x$\n由于只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。\n代价函数 建模误差: 模型所预测的值与训练集中实际值之间的差距 $h_\\theta(x^{(i)}) - y^{(i)}$\n平方误差函数 $J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$\n平方误差函数是代价函数之一，对于大多数问题，特别是回归问题，其都是个合理的选择。\n为什么是 $\\frac{1}{2m}$:\n除以 m 是为了消除样本数量 m 对 J 的影响\n除以 2 是为了抵消对 J 关于 θ 求偏导数时式子多出的一个 2，使计算更方便\n我们的目标就是找到一个合适的参数 $\\theta_0,\\theta_1$ 使得代价函数最小。 即 Goal: $\\underset{\\theta_0,\\theta_1}{minimize}J(\\theta_0,\\theta_1)$\n梯度下降 梯度下降是一个用来 求函数最小值 的算法，可以用其来求出使代价函数 $J$ 最小的参数 $\\theta_0$ 和 $\\theta_1$ 的值。\n梯度下降的思想 : 开始时，随机选择一组参数 $(\\theta_0,\\theta_1)$, 计算代价函数, 然后寻找下一个能让代价函数下降最多的参数组合。持续这种操作，直至找到一个局部最小值。\n局部最小值不等同于全局最小值。不同的初始参数会可能会得到完全不同的局部最小值。\n批量梯度下降算法(batch gradient descent) 批量 batch 指的是，在梯度下降的每一步中，都用到来所有的训练样本。\n公式为: $$ \\begin{aligned} \u0026\\text{repeat until convergence（重复直至收敛）} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) \\quad (\\text{for } j = 0 \\text{ and } j = 1) \\\\ \u0026\\} \\end{aligned} $$其中 $\\alpha$ 是学习率，其决定了我们沿代价函数梯度方向向下迈出的幅度有多大。\n$\\alpha$ 太小，需要很多步才能够达到局部最低点。\n$\\alpha$ 太大，梯度下降法可能会越过最低点，甚至可能无法收敛，导致发散。\n在梯度下降法中，当参数取值接近局部最低点时，梯度下降法会自动采取更小的幅度，因为代价函数导数会趋向于 0。\n多变量线性回归 多维特征 新的注释：\n$n$ 代表特征的数量\n$x^{(i)}$ 代表第 $i$ 个训练实例，是特征数据的第 $i$ 行，是一个 **向量 **\n$x_j^{(i)}$ 代表特征矩阵中第 $i$ 行第 $j$ 个特征\n多变量的假设 $h$ 表示为：\n$$ h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n $$ 此时模型中参数为一个 $n+1$ 维向量\n任何一个训练实例也都是一个 $n+1$ 维向量\n特征矩阵 X 的维度是 $m*(n+1)$\n因此公式可以简化为\n$$ h_\\theta(x)=\\theta^TX $$多变量梯度下降 代价函数为所有建模误差的平方和，即:\n$$ J(\\theta_0,\\theta_1,...,\\theta_n)=\\frac{1}{2m}\\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2 $$多变量线性回归的批量梯度下降算法为： $$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1,...\\theta_n) \\\\ \u0026\\} \\end{aligned} $$即:\n$$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} \\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2 \\\\ \u0026\\} \\end{aligned} $$ 求导后得到： $$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} \\\\ \u0026\\} \\end{aligned} $$梯度下降法 特征缩放 使特征具有相近的尺度有助于梯度下降宣发更快的收敛。\n方法：将所有特征的尺度都尽量缩放到-1 到 1 这种小范围 内，也不能太小。\n最简单的，令 $x_n=\\frac{x_n-\\mu_n}{s_n}$, 其中 $\\mu_n$ 是平均值，$s_n$ 是标准差。\n学习率 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制 迭代次数-代价函数 的图表来观测算法在何时收敛。\n梯度下降算法的每次迭代受学习率 $\\alpha $ 影响：\n$\\alpha$ 太小，则到达收敛所需的迭代次数很大，但理论上终会收敛。 $\\alpha $ 太大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。 调整学习率时可以采用以下 策略：从 0.001 开始每次上在原基础上乘 3 倍，最终选择一个较大的可以使得代价函数收敛的 $\\alpha $。\n如：0.001，0.003，0.01，0.03，0.1\n特征选择与多项式回归 特征选择 建立模型时，可以 根据需要选择适合模型的特征 而不局限于数据集中的特征。比如：数据集给出房子的长和宽，预测房价时，可以创造一个特征为 *面积 = 长 宽 作为模型的新特征。\n多项式回归 线性回归并不适合所有数据，有时需要曲线来适应数据，比如：\n三次方模型 $h_\\theta(x)=\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$\n使用多项式回归模型，特征缩放很有必要。\n正规方程 当特征数量小于 10000 时（对于目前的计算机），通常采用正规方程的方法找到一组最优参数 $\\theta$。\n正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\\frac{\\partial}{\\partial \\theta_j} J(\\theta_j)=0$，\n其 解 为 $\\theta=(X^TX)^{-1}X^Ty$。$X$ 为训练集特征矩阵，$y$ 为训练集结果。\n梯度下降于正规方程的比较 梯度下降 正规方程 需要选择学习率 $\\alpha $ 不需要 需要多次迭代 一次运算得出 特征数量 n 大时也可用 矩阵逆的计算时间复杂度为 $O(n^3)$。因此 n 小于 10000 时，可以接受 使用于各种类型 只适用于线性模型。不适合逻辑回归等其它模型。 以下情况时，正规方程不能用：\n有两个特征线性相关。 解决：使用正规方程前，剔除相关特征。 含有大量特征，进而出现 $m \\leqslant n$ 时。 解决：剔除特征，用较少特征反应尽可能多的内容。 逻辑回归 逻辑回归算法是一种分类算法。\n分类问题预测的变量 $y$ 为 离散值。\n假说表示 引入一个 逻辑函数 Sigmoid function，S 形函数，公式为 $g(z)=\\frac{1}{1+e^{-z}}$。其图像如下所示：\nsigmoid function 令逻辑回归模型的假设是：$h_\\theta(x)=g(\\theta^T X)$, $X$ 代表特征向量，$g$ 代表逻辑函数。\n此时模型的输出变量范围始终在 0 和 1 之间。\n$h_\\theta(x)$ 表示，对于给定的输入变量，根据选择的参数，计算输出变量为 1 的可能性，即： $$ h_\\theta(x)= P(y = 1|x;\\theta) $$决策边界 在逻辑回归中，假设：\n$$ y = \\begin{cases} 1, \u0026 \\text{if } h_\\theta(x) \\geq 0.5 \\\\ 0, \u0026 \\text{if } h_\\theta(x) \u003c 0.5 \\end{cases} $$因此当 $\\theta^T x \\geqslant 0$ 时，预测 $y=1$，当 $\\theta^T x \u0026lt; 0$ 时，预测 $y=0$。\n因此对于一组参数 $\\theta$，有 $\\theta^T x=0$，这条 曲线 即为模型的决策边界。\n代价函数 为什么逻辑回归的代价函数和线性回归的不同？由于 sigmoid 函数的非线性，导致逻辑回归的假设函数带入代价函数时，会导致代价函数有很多局部最小值，其是一个非凸函数。因此需对代价函数进行修正。\n定义逻辑回归的代价函数 $J(\\theta)=\\frac{1}{m} \\sum^m_{i=1} Cost(h_\\theta(x^{(i)}),y^{(i)})$，其中 $$ Cost(h_\\theta(x^{(i)}), y^{(i)}) = \\begin{cases} -log( h_\\theta(x)) \\quad \u0026 if\\quad y = 1, \u0026 \\\\ -log( 1- h_\\theta(x)) \u0026 if\\quad y = 0, \u0026 \\\\ \\end{cases} \\overset{简化}{=} -ylog( h_\\theta(x))-(1-y)log( 1- h_\\theta(x)) $$逻辑回归中 Cost 函数特点 当实际的 $y=1$ 且 $h_\\theta(x)=1$ 时，误差为 0，当 $y=1$ 但 $h_\\theta(x)$ 不为 1 时，误差随着 $h_\\theta(x)$ 的变小而变大。当 $h_\\theta(x)$ 减小为 0，表示预测与实际完全相反，其误差为无穷大，这时代价函数将会受到很大的惩罚；当实际的 $y=0$ 且 $h_\\theta(x)=0$ 时，误差为 0，当 $y=0$ 但 $h_\\theta(x)$ 不为 0 时，误差随着 $h_\\theta(x)$ 的变大而变大。\n简化后的代价函数 $$ \\begin{aligned} J(\\theta)\u0026 = \\frac{1}{m} \\sum^m_{i = 1} Cost(h_\\theta(x^{(i)}), y^{(i)}) \u0026 \\\\ \u0026 = -\\frac{1}{m} [\\sum^m_{i = 1}-ylog( h_\\theta(x))-(1-y)log( 1- h_\\theta(x))] \u0026\\\\ \\end{aligned} $$最小化代价函数的方法 是梯度下降法： $$ \\begin{aligned} \u0026\\text{Repeat} \\{ \\\\ \u0026 \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\\\ \u0026\\} \\end{aligned} $$虽然其形式上与线性回归的梯度下降法相同，但由于回归的模型假设发生了变化，两者是完全不同的函数。\n高级优化 优化算法：共轭梯度法 BFGS（变尺度法）和 L-BFGS（限制遍尺度法）。\n优点：不需要手动选择学习率。\n多类别分类 假设有 n 个类别，则将其分为 n 个 1 对（n-1）的 二元分类问题。\n记每一个分类问题的模型为：$h_\\theta ^{(i)} (x)= p(y=i|x;\\theta)$，其中 $i=(1,2,3,\u0026hellip;,k)$。\n最终输出结果为让 $h_\\theta ^{(i)} (x)$ 最大的 i，即 $\\mathop{max}\\limits_{i} ; h_\\theta ^{(i)} (x)$。\n正则化 过拟合 过拟合 指通过学习得到的假设可能能够非常好的适应训练集（代价函数可能几乎为 0），但是可能 不会 推广到预测新的数据。\n当有 过多的变量但只有很少的训练数据 时很容易出现过拟合问题。\n欠拟合 指模型不能适应训练集。\n解决过拟合 减少特征变量的数目 \u0026ndash; 人工选择去除一些变量；模型算法选择（如 PCA）。 正则化 \u0026ndash; 保留所有的特征变量，但减小参数 $\\theta$ 的大小。 其适用于有很多特征变量，且每一个特征变量看起来对预测结果都有一点用的情况。 代价函数 正则化的直观理解 假设模型为 $h_\\theta(x)=\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3 + \\theta_4 x^4$，且高次项导致了过拟合的产生，则 假如能让高次项的系数趋向于 0，就能够缓解过拟合的问题 了。 因此修改代价函数 ，对 $\\theta_3 \\text{和}\\theta_4$ 设置一些惩罚，如下所示： $$ J(\\theta)=\\frac{1}{2m}[\\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2+1000 \\theta_3^2 + 1000 \\theta_4^2] $$ 则通过这样的代价函数选择出来的 $\\theta_3 \\text{和}\\theta_4$ 对预测结果的影响就很小了。\n带正则化参数的代价函数 修改后的代价函数如下： $$ J(\\theta)=\\frac{1}{2m}[\\sum^m_{i = 1}(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda \\sum^n_{j = 1}\\theta_j^2] $$ 其中，$\\lambda$ 成为正则化参数。\n假如 $\\lambda$ 太大，则所有 $\\theta$ 都会趋于 0，除了 $\\theta_0$，这样最终模型为一条平行于 x 轴的直线，导致欠拟合。\n正则化线性回归 梯度下降 $$ \\begin{aligned} \u0026 \\text{Repeat until convergence} \\{ \\\\ \u0026 \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\\\ \u0026 \\theta_j := \\theta_j(1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\\\ \u0026 \\} \\end{aligned} $$正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新柜子的基础上令 $\\theta$ 值减少了一个额外的值。\n正规化 $$ \\theta = (X^T X + \\lambda \\begin{bmatrix} 0 \u0026 \u0026 \u0026 \u0026 \u0026 \\\\ \u0026 1 \u0026 \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 1 \u0026 \u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \\cdot \u0026 \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \\cdot \u0026 \\\\ \u0026 \u0026 \u0026 \u0026 \u0026 1 \\\\ \\end{bmatrix}^{-1})^{-1} X^Ty $$图中矩阵尺寸为 $(n+1)*(n+1)$\n正则化逻辑回归 正则化逻辑回归的代价函数： $$ J(\\theta) = \\frac{1}{m} [\\sum^m_{i = 1}-ylog( h_\\theta(x))-(1-y)log( 1- h_\\theta(x))] + \\frac{\\lambda}{2m} \\sum^n_{j = 1}\\theta^2_j $$ 则梯度下降算法为： $$ \\begin{aligned} \u0026 \\text{Repeat until convergence} \\{ \\\\ \u0026 \\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\\\ \u0026 \\theta_j := \\theta_j(1-\\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum^m_{i = 1} (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\\\ \u0026 \\} \\end{aligned} $$神经网络 模型表示 神经网络模型建立在很多神经元之上，每一个神经元又是一个学习模型。这些神经元也叫激活单元（activation unit）。\n神经网络模型是许多逻辑单元按照不同的层级组织起来的网络，每一层的输出变量都是下一层的输入变量。\n引入以下标记法来描述模型：\n$\\alpha _i ^{(j)}$代表第$j$层的第$i$个激活单元。 $\\theta^{(j)}$代表从第$j$层映射到第$j+1$层时的权重矩阵。其尺寸为以第$j+1$层的激活单元数量作为行数，以第$j$层的激活单元数量加一作为列数。 ","date":"2025-05-22T16:36:28+08:00","permalink":"https://heimi2022.github.io/Hugo/p/machine-learning/","title":"Machine-Learning"},{"content":"启动 Hugo 服务 hugo server -D\n创建文章 命令创建 命令 : hugo new post/foldername/*.md 其模板来自 archetypes/default.md 手动创建 在post目录下，手动创建一个文件夹，在文件夹里创建md文件，并将需要的图片资源放入其中。 文章推送 git add . git commit -m\u0026quot;update\u0026quot; git push ","date":"2025-05-22T15:53:57+08:00","image":"https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/cover_hu_3e3cbd19f9ff226a.png","permalink":"https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/","title":"Hugo使用"},{"content":"我的第一条博客送给杨思敏同学 ","date":"2025-05-20T21:27:55+08:00","permalink":"https://heimi2022.github.io/Hugo/p/to_ysm/","title":"To_YSM"}]
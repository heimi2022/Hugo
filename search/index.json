[{"content":"公式 上标与下标 用^来输出上标，使用_来输出下标，使用{}包含作用范围。\n空格表示 两个quad空格\u0026mdash;\u0026mdash;\u0026mdash;a \\qquad b\u0026mdash;\u0026mdash;\u0026mdash;两个m的宽度\nquad空格\u0026mdash;\u0026mdash;\u0026mdash;a \\quad b\u0026mdash;\u0026mdash;\u0026mdash;一个m的宽度\n大空格\u0026mdash;\u0026mdash;\u0026mdash;a\\ b\u0026mdash;\u0026mdash;\u0026mdash;1/3m宽度\n中等空格\u0026mdash;\u0026mdash;\u0026mdash;a;b\u0026mdash;\u0026mdash;\u0026mdash;2/7m宽度\n小空格\ta,b\u0026mdash;\u0026mdash;\u0026mdash;a,b\u0026mdash;\u0026mdash;\u0026mdash;1/6m宽度\n没有空格\u0026mdash;\u0026mdash;\u0026mdash;ab\n紧贴\u0026mdash;\u0026mdash;\u0026mdash;a!b\u0026mdash;\u0026mdash;\u0026mdash;缩进1/6m宽度\n数学结构 分式 \\frac{a}{b} : $\\frac{a}{b}$ 根式 \\sqrt[n]{a} : $\\sqrt[n]{a}$ 求导 f\u0026rsquo; : $f'$ 求和$\\sum$ ：\\sum 极限 $\\lim\\limits_{x\\to\\infty}$：\\lim\\limits_{x\\to\\infty} 积分号$\\int$：\\int 其它 上划线:\\overline{a},下划线\\underline{a},上右箭头\\overrightarrow{a}:$\\overrightarrow{a}$ 数学函数 1.三角函数 $\\sin$：\\sin ; $\\cos$：\\cos;\n数学符号 偏导数 $\\partial$ ：\\partial 数学运算 点乘 $a \\cdot b$：\\cdot\n叉乘 $a \\times b$：\\times\n除以 $a \\div b$：\\div\n希腊字母 $\\alpha \\beta \\gamma \\delta \\epsilon \\zeta \\eta \\theta$：\\alpha \\beta \\gamma \\delta \\epsilon \\zeta \\eta \\theta\n$\\iota \\kappa \\lambda \\mu \\nu \\omicron \\xi \\pi$ ：\\iota \\kappa \\lambda \\mu \\nu \\omicron \\xi \\pi\n$\\rho \\sigma \\tau \\upsilon \\phi \\chi \\psi \\omega$：\\rho \\sigma \\tau \\upsilon \\phi \\chi \\psi \\omega\n$\\Alpha \\Beta \\Gamma \\Delta \\Epsilon \\Zeta \\Eta \\Theta$：\\Alpha \\Beta \\Gamma \\Delta \\Epsilon \\Zeta \\Eta \\Theta\n$\\Iota \\Kappa \\Lambda \\Mu \\Nu \\Xi \\Omicron \\Pi$：\\Iota \\Kappa \\Lambda \\Mu \\Nu \\Xi \\Omicron \\Pi\n$\\Rho \\Sigma \\Tau \\Upsilon \\Phi \\Chi \\Psi \\Omega$：\\Rho \\Sigma \\Tau \\Upsilon \\Phi \\Chi \\Psi \\Omega\n关系符号 $\\leqslant ， \\geqslant ，\\le ，\\ge$：\\leqslant ， \\geqslant ，\\le ，\\ge 括号 大括号 { 其它 \\underset{123}{abc} : $\\underset{123}{abc}$ 换行 \\ ","date":"2025-05-23T20:12:04+08:00","permalink":"https://heimi2022.github.io/Hugo/p/latex%E8%AF%AD%E6%B3%95/","title":"LaTex语法"},{"content":"插入公式 行内公式 : 输入 $ + esc ,则可实现 $ 公式 $ 行间公式 : 输入 $$ + 回车 ,则可实现 $$ 公式 $$ ","date":"2025-05-23T20:12:04+08:00","permalink":"https://heimi2022.github.io/Hugo/p/markdown%E8%AF%AD%E6%B3%95/","title":"Markdown语法"},{"content":"问题解决 git push fatal: unable to access ‘https://github.com/\u0026hellip;/.git‘: Could not resolve host: github.com git config \u0026ndash;global \u0026ndash;unset http.proxy git config \u0026ndash;global \u0026ndash;unset https.proxy\n解决方案：cmd下命令执行 ipconfig/flushdns 清理dns缓存\n","date":"2025-05-22T17:20:11+08:00","permalink":"https://heimi2022.github.io/Hugo/p/git%E4%BD%BF%E7%94%A8/","title":"Git使用"},{"content":"引言 什么是机器学习 定义如下，一个程序被认为能从 经验 E 中学习，解决 任务 T，达到 性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n例如下棋游戏:\nE 就是程序上万次的自我练习的经验。\nT 就是下棋。\nP 就是它在与一些新的对手比赛时，赢得比赛的概率。\n监督学习 监督学习 指的就是我们给学习算法一个 数据集。这个数据集由“正确答案”组成。学习算法再根据这个数据集作出预测，算出更多的正确答案。\n监督学习的两个例子, 回归问题 与 分类问题。\n回归问题 : 推测出一个 连续 值的输出结果。\n例如 : 卖水果，数据集 3 斤卖 10 元左右，预测 4 斤能卖多少。\n分类问题 : 推测出一组 离散 的结果。\n例如 : 识别红绿灯。\n无监督学习 无监督学习 指的是一种学习策略，它交给算法大量的数据，并让算法为我们从数据中找出某种结构。\n无监督学习中 已知数据没有任何的标签 是指数据 有相同的标签或者就是没标签。\n无监督学习的例子，聚类算法、鸡尾酒算法\n聚类算法 : 将数据分为几类不同的 簇。\n例子 : 谷歌新闻，同一主题新闻归为一类; 市场分割。\n鸡尾酒算法 : 分离麦克风接收到的不同的人的声音与环境声音。\n单变量线性回归(Linear Regression with One Variable) 模型表示 样本数目 : 小写 m\n训练集 : Training Set\n特征/输入变量 : $x$\n目标/输出变量 : $y$\n训练集中的实例 : ($x$, $y$)\n第 $i$ 个观察实例 : ($x^{(i)}$, $y^{(i)}$)\n学习算法的解决方案或函数也称为假设(hypothesis) : h\n模型表示就是找到将训练集给学习算法找到一个合适的函数 $h$ , $y = h(x)$\n函数 $h_\\theta(x) = \\theta_0 + \\theta_1x$\n由于只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。\n代价函数 建模误差: 模型所预测的值与训练集中实际值之间的差距 $h_\\theta(x^{(i)}) - y^{(i)}$\n平方误差函数 $J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$\n平方误差函数是代价函数之一，对于大多数问题，特别是回归问题，其都是个合理的选择。\n为什么是 $\\frac{1}{2m}$:\n除以 m 是为了消除样本数量 m 对 J 的影响\n除以 2 是为了抵消对 J 关于 θ 求偏导数时式子多出的一个 2，使计算更方便\n我们的目标就是找到一个合适的参数 $\\theta_0,\\theta_1$ 使得代价函数最小。 即 Goal: $\\underset{\\theta_0,\\theta_1}{minimize}J(\\theta_0,\\theta_1)$\n梯度下降 梯度下降是一个用来 求函数最小值 的算法，可以用其来求出使代价函数 $J$ 最小的参数 $\\theta_0$ 和 $\\theta_1$ 的值。\n梯度下降的思想 : 开始时，随机选择一组参数 $(\\theta_0,\\theta_1)$, 计算代价函数, 然后寻找下一个能让代价函数下降最多的参数组合。持续这种操作，直至找到一个局部最小值。\n局部最小值不等同于全局最小值。不同的初始参数会可能会得到完全不同的局部最小值。\n批量梯度下降算法(batch gradient descent) 批量 batch 指的是，在梯度下降的每一步中，都用到来所有的训练样本。\n公式为: $$ \\begin{aligned} \u0026\\text{repeat until convergence（重复直至收敛）} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1) \\quad (\\text{for } j = 0 \\text{ and } j = 1) \\\\ \u0026\\} \\end{aligned} $$其中 $\\alpha$ 是学习率，其决定了我们沿代价函数梯度方向向下迈出的幅度有多大。\n$\\alpha$ 太小，需要很多步才能够达到局部最低点。\n$\\alpha$ 太大，梯度下降法可能会越过最低点，甚至可能无法收敛，导致发散。\n在梯度下降法中，当参数取值接近局部最低点时，梯度下降法会自动采取更小的幅度，因为代价函数导数会趋向于 0。\n多变量线性回归 多维特征 新的注释：\n$n$ 代表特征的数量\n$x^{(i)}$ 代表第 $i$ 个训练实例，是特征数据的第 $i$ 行，是一个 **向量 **\n$x_j^{(i)}$ 代表特征矩阵中第 $i$ 行第 $j$ 个特征\n多变量的假设 $h$ 表示为：\n$$ h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n $$ 此时模型中参数为一个$n+1$维向量\n任何一个训练实例也都是一个$n+1$维向量\n特征矩阵X的维度是$m*(n+1)$\n因此公式可以简化为\n$$ h_\\theta(x)=\\theta^TX $$多变量梯度下降 代价函数为所有建模误差的平方和，即:\n$$ J(\\theta_0,\\theta_1,...,\\theta_n)=\\frac{1}{2m}\\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2 $$多变量线性回归的批量梯度下降算法为： $$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1,...\\theta_n) \\\\ \u0026\\} \\end{aligned} $$即:\n$$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2m} \\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2 \\\\ \u0026\\} \\end{aligned} $$ 求导后得到： $$ \\begin{aligned} \u0026\\text{repeat} \\{ \\\\ \u0026\\quad \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)}) \\cdot x_j^{(i)} \\\\ \u0026\\} \\end{aligned} $$梯度下降法 特征缩放 使特征具有相近的尺度有助于梯度下降宣发更快的收敛。\n方法：将所有特征的尺度都尽量缩放到-1到1这种小范围内，也不能太小。\n最简单的，令$x_n=\\frac{x_n-\\mu_n}{s_n}$,其中$\\mu_n$是平均值，$s_n$是标准差。\n学习率 梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制迭代次数-代价函数的图表来观测算法在何时收敛。\n梯度下降算法的每次迭代受学习率$\\alpha $影响：\n$\\alpha$太小，则到达收敛所需的迭代次数很大，但理论上终会收敛。 $\\alpha $太大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。 调整学习率时可以采用以下策略：从0.001开始每次上在原基础上乘3倍，最终选择一个较大的可以使得代价函数收敛的$\\alpha $。\n如：0.001，0.003，0.01，0.03，0.1\n特征选择与多项式回归 特征选择 建立模型时，可以根据需要选择适合模型的特征而不局限于数据集中的特征。比如：数据集给出房子的长和宽，预测房价时，可以创造一个特征为 面积=长*宽 作为模型的新特征。\n多项式回归 线性回归并不适合所有数据，有时需要曲线来适应数据，比如：\n三次方模型$h_\\theta(x)=\\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$\n使用多项式回归模型，特征缩放很有必要。\n正规方程 当特征数量小于10000时（对于目前的计算机），通常采用正规方程的方法找到一组最优参数$\\theta$。\n正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\\frac{\\partial}{\\partial \\theta_j} J(\\theta_j)=0$，\n其解为$\\theta=(X^TX)^{-1}X^Ty$。$X$为训练集特征矩阵，$y$为训练集结果。\n梯度下降于正规方程的比较 梯度下降 正规方程 需要选择学习率$\\alpha $ 不需要 需要多次迭代 一次运算得出 特征数量n大时也可用 矩阵逆的计算时间复杂度为$O(n^3)$。因此n小于10000时，可以接受 使用于各种类型 只适用于线性模型。不适合逻辑回归等其它模型。 以下情况时，正规方程不能用：\n有两个特征线性相关。 解决：使用正规方程前，剔除相关特征。 含有大量特征，进而出现$m \\leqslant n$时。 解决：剔除特征，用较少特征反应尽可能多的内容。 ","date":"2025-05-22T16:36:28+08:00","permalink":"https://heimi2022.github.io/Hugo/p/machine-learning/","title":"Machine-Learning"},{"content":"启动 Hugo 服务 hugo server -D\n创建文章 命令创建 命令 : hugo new post/foldername/*.md 其模板来自 archetypes/default.md 手动创建 在post目录下，手动创建一个文件夹，在文件夹里创建md文件，并将需要的图片资源放入其中。 文章推送 git add . git commit -m\u0026quot;update\u0026quot; git push ","date":"2025-05-22T15:53:57+08:00","image":"https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/cover_hu_3e3cbd19f9ff226a.png","permalink":"https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/","title":"Hugo使用"},{"content":"我的第一条博客送给杨思敏同学 ","date":"2025-05-20T21:27:55+08:00","permalink":"https://heimi2022.github.io/Hugo/p/to_ysm/","title":"To_YSM"}]
<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="自然语言处理（NLP）基础 基础与应用 自然语言处理是让计算机来理解人类所说的语言，然后像人一样去交互，对话，生成自然语言。\n自然语言处理的基础任务：词性标注、命名实体的识别、共指消解（代词与实体之间的连接）、依赖关系（语法等）；中文的自动分词（将中文的每个词区分出来，像英文一样）；机器翻译；情感分类；意见挖掘\n">
<title>Big_Model_System</title>

<link rel='canonical' href='https://heimi2022.github.io/Hugo/p/big_model_system/'>

<link rel="stylesheet" href="/Hugo/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css"><meta property='og:title' content="Big_Model_System">
<meta property='og:description' content="自然语言处理（NLP）基础 基础与应用 自然语言处理是让计算机来理解人类所说的语言，然后像人一样去交互，对话，生成自然语言。\n自然语言处理的基础任务：词性标注、命名实体的识别、共指消解（代词与实体之间的连接）、依赖关系（语法等）；中文的自动分词（将中文的每个词区分出来，像英文一样）；机器翻译；情感分类；意见挖掘\n">
<meta property='og:url' content='https://heimi2022.github.io/Hugo/p/big_model_system/'>
<meta property='og:site_name' content='WJJ'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='大模型' /><meta property='article:published_time' content='2025-07-07T16:00:00&#43;08:00'/><meta property='article:modified_time' content='2025-07-07T16:00:00&#43;08:00'/>
<meta name="twitter:title" content="Big_Model_System">
<meta name="twitter:description" content="自然语言处理（NLP）基础 基础与应用 自然语言处理是让计算机来理解人类所说的语言，然后像人一样去交互，对话，生成自然语言。\n自然语言处理的基础任务：词性标注、命名实体的识别、共指消解（代词与实体之间的连接）、依赖关系（语法等）；中文的自动分词（将中文的每个词区分出来，像英文一样）；机器翻译；情感分类；意见挖掘\n">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/Hugo/">
                
                    
                    
                    
                        
                        <img src="/Hugo/img/avatar_hu_be2515b413f73217.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">📖</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/Hugo">WJJ</a></h1>
            <h2 class="site-description">欢迎来到我的博客</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/heimi2022/'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/Hugo/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>归档</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>友情链接</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#自然语言处理nlp基础">自然语言处理（NLP）基础</a>
      <ol>
        <li><a href="#基础与应用">基础与应用</a></li>
        <li><a href="#词表示">词表示</a></li>
        <li><a href="#语言模型">语言模型</a>
          <ol>
            <li><a href="#语言模型的工作">语言模型的工作</a></li>
            <li><a href="#语言模型的假设">语言模型的假设</a></li>
            <li><a href="#n-gram-model">N-gram Model</a></li>
            <li><a href="#neural-language-model">Neural Language Model</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#神经网络">神经网络</a>
      <ol>
        <li><a href="#神经网络基础">神经网络基础</a></li>
        <li><a href="#word2vec">Word2vec</a>
          <ol>
            <li><a href="#cbow">CBOW</a></li>
            <li><a href="#continuous-skip-gram">continuous skip-gram</a></li>
            <li><a href="#sub-sampling">Sub-Sampling</a></li>
          </ol>
        </li>
        <li><a href="#循环神经网络rnn">循环神经网络RNN</a>
          <ol>
            <li><a href="#rnn单元">RNN单元</a></li>
            <li><a href="#rnn的应用与问题">RNN的应用与问题</a></li>
            <li><a href="#门控循环单元gru">门控循环单元（GRU）</a></li>
            <li><a href="#长短期记忆网络lstm">长短期记忆网络（LSTM）</a></li>
            <li><a href="#双向rnn">双向RNN</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#transformer">Transformer</a>
      <ol>
        <li><a href="#注意力机制">注意力机制</a>
          <ol>
            <li><a href="#注意力机制的各种变式">注意力机制的各种变式</a></li>
            <li><a href="#注意力机制的特点">注意力机制的特点</a></li>
          </ol>
        </li>
        <li><a href="#transformer机制">Transformer机制</a>
          <ol>
            <li><a href="#模型结构">模型结构</a></li>
            <li><a href="#输入编码-bpepe">输入编码 BPE，PE</a></li>
            <li><a href="#encoder-block">Encoder Block</a></li>
            <li><a href="#decoder-block">Decoder Block</a></li>
          </ol>
        </li>
        <li><a href="#优点和缺点">优点和缺点</a>
          <ol>
            <li><a href="#优点">优点</a></li>
            <li><a href="#缺点">缺点</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#预训练语言模型">预训练语言模型</a>
      <ol>
        <li><a href="#plmspre-trained-lanuage-models">PLMs（Pre-trained Lanuage Models）</a></li>
        <li><a href="#masked-lm">Masked LM</a></li>
        <li><a href="#few-shotin-context-learning">few-shot/in-context learning</a></li>
      </ol>
    </li>
    <li><a href="#transformer-使用">Transformer 使用</a>
      <ol>
        <li><a href="#pipeline">Pipeline</a></li>
        <li><a href="#tokenization">Tokenization</a></li>
      </ol>
    </li>
    <li><a href="#prompt-learning">Prompt-learning</a>
      <ol>
        <li><a href="#verbalizer">Verbalizer</a></li>
      </ol>
    </li>
    <li><a href="#bmtrain">BMtrain</a>
      <ol>
        <li><a href="#数据并行">数据并行</a>
          <ol>
            <li><a href="#分布式数据并行">分布式数据并行</a></li>
          </ol>
        </li>
        <li><a href="#模型并行">模型并行</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/Hugo/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="background-color: #2a9d8f; color: #fff;">
                学习笔记
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/Hugo/p/big_model_system/">Big_Model_System</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-07-07</time>
            </div>
        

        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="自然语言处理nlp基础">自然语言处理（NLP）基础
</h2><h3 id="基础与应用">基础与应用
</h3><p>自然语言处理是让计算机来理解人类所说的语言，然后像人一样去交互，对话，生成自然语言。</p>
<p>自然语言处理的基础任务：词性标注、命名实体的识别、共指消解（代词与实体之间的连接）、依赖关系（语法等）；中文的自动分词（将中文的每个词区分出来，像英文一样）；机器翻译；情感分类；意见挖掘</p>
<h3 id="词表示">词表示
</h3><p>词表示：把词转换为机器所能理解的意思</p>
<p>词表示的目标：词相似度的计算；发现词与词之间的语义的关系</p>
<p>word Embedding：<strong>分布式表示</strong>，建立一个低维的一个稠密的向量空间，然后把每一个词都学到这个空间里，用空间里某个位置所对应的向量来表示词。代表方法：word2vec。</p>
<h3 id="语言模型">语言模型
</h3><p>语言模型：根据前文预测下一个词是什么。</p>
<h4 id="语言模型的工作">语言模型的工作
</h4><ol>
<li>完成计算一个序列的词成为一句话的概率（joint probability）</li>
</ol>
$$
P(W)=P(w_1,w_2,\cdots,w_n)
$$<ol start="2">
<li>根据前文预测下一个词（Conditional Probability）</li>
</ol>
$$
P(w_n|w_1,w_2,\cdots,w_{n-1})
$$<h4 id="语言模型的假设">语言模型的假设
</h4><p>即将出现的词只受它之前出现词的影响。因此语言模型可表示为：
</p>
$$
P(w_1,w_2,\cdots,w_n)=\prod_ip(w_i|w_1,w_2,\cdots,w_{i-1})
$$<h4 id="n-gram-model">N-gram Model
</h4><p>模型的搭建：统计前面出现了几个词之后后面出现的那个词的频度是什么样的。</p>
<p>例如4-gram：
</p>
$$
p(w_j|too \, late \, to)=\frac{count(too \, late \, to \, w_j)}{count(too \, late \, to)}
$$<p>
其满足Markov assumption：
</p>
$$
P(w_1,w_2,\cdots,w_n) \approx \prod_i p(w_i|w_1,w_2,\cdots,w_{i-1})
$$<p>
并且有：
</p>
$$
p(w_i|w_1,w_2,\cdots,w_{i-1}) \approx p(w_i|w_{i-k},\cdots,w_{i-1})
$$<p>
问题：不能发现句子间的相似度。类似用独热码来表示词。</p>
<h4 id="neural-language-model">Neural Language Model
</h4><p>用分布式表示词，将词表示为一个低维向量，在把低维向量拼在一起，形成一个更高维的上下文向量，再进行非线性转换，就可以用其预测下一个词。类似神经网络的过程，其可基于神经网络可调参数来学习上下文间的向量的关系。</p>
<h2 id="神经网络">神经网络
</h2><h3 id="神经网络基础">神经网络基础
</h3><p>为什么要有激活函数？若不存在激活函数，则多层神经网络都为线性运算，其最终可以被转化为一个单层的神经网络。也就是说，在每一激活函数的情况下，多层神经网络和单层神经网络表达能力是一致的。举例如下：
</p>
$$
h_1=W_1 x + b_1,h_2=W_2 h_1 + b_2  \quad -> \quad h_2=W_2 W_1 x + W_2 b_1 + b_2
$$<h3 id="word2vec">Word2vec
</h3><p>word2vec有两类模型：Continuous bag-of-words(CBOW)以及continuous skip-gram。</p>
<p>word2vec用滑动窗口的方式构造训练数据，一个滑动窗口是一个文本中连续出现的几个单词，在窗口中间的词叫做target，其它叫做context。</p>
<h4 id="cbow">CBOW
</h4><p>CBOW：根据context推测target。其不考虑context词的次序。</p>
<p>下图是CBOW的模型，其是一个多分类问题。类别数为词表大小。</p>
<p><img src="/Hugo/assets/Big_Model_CBOW"
	
	
	
	loading="lazy"
	
		alt="image-20250708175947075"
	
	
></p>
<h4 id="continuous-skip-gram">continuous skip-gram
</h4><p>continuous skip-gram：根据target推出context。其一次预测多个context时，先将问题进行分解，即一个一个预测context。</p>
<p>下图是continuous skip-gram的模型。</p>
<p><img src="/Hugo/assets/Big_Model_continuous_skip_gram"
	
	
	
	loading="lazy"
	
		alt="image-20250708180328724"
	
	
></p>
<p>上述模型中，若词表非常大，则训练效率很慢。采用负采样来提高训练效率。</p>
<p>负采样：不把整个词表作为负例，只选几个词表中不是需要predict的词来作为负例来提高计算效率。</p>
<h4 id="sub-sampling">Sub-Sampling
</h4><p>Sub-Sampling：平衡常见词和罕见词出现的概率</p>
<p>常见词，可能包含的语义比较少，如&quot;的&quot;等，需在训练时去掉，下面是词被去掉的概率
</p>
$$
p=1- \sqrt{\frac{t}{f(w)}}
$$<p>
f(w)为一个词出现的频度，t是可自己定义的值。</p>
<h3 id="循环神经网络rnn">循环神经网络RNN
</h3><p>RNN:Recurrent Neural Network.</p>
<p>其处理序列数据时，会进行顺序记忆。</p>
<p>下面是一个常见的模型结构</p>
<p><img src="/Hugo/assets/image-20250708183236474.png"
	
	
	
	loading="lazy"
	
		alt="image-20250708183236474"
	
	
></p>
<h4 id="rnn单元">RNN单元
</h4><p><img src="/Hugo/assets/image-20250708183317584.png"
	
	
	
	loading="lazy"
	
		alt="image-20250708183317584"
	
	
>
</p>
$$
h_i = tanh(W_x x_i + W_h h_{i-1} + b) \\
y_i =F(h_i)
$$<p>
RNN模型是每一个RNN单元的复制，其参数是一样的，有利于实现参数共享，使得模型能够泛化到不同长度的样本，节省参数量</p>
<h4 id="rnn的应用与问题">RNN的应用与问题
</h4><p>应用场景：序列标注，序列预测，图片描述，文本分类。</p>
<p>问题：容易产生梯度消失、梯度爆炸。</p>
<p>进行反向传播时，有：
</p>
$$
h_i = \tanh(W_x x_i + W_h h_{i-1} + b) \\
\Delta w_1 = \frac{\partial Loss}{\partial w_2} = \frac{\partial Loss}{\partial h_n} \cdot \frac{\partial h_n}{\partial h_{n-1}} \cdot \frac{\partial h_{n-1}}{\partial h_{n-2}} \cdot \; \cdots \; \cdot \frac{\partial h_3}{\partial h_2} \cdot \frac{\partial h_2}{\partial w_2} (反向传播时，链式法则展开)
$$<p>
根据$\frac{\partial h_n}{\partial h_{n-1}}$ 讨论。当 $\frac{\partial h_n}{\partial h_{n-1}} &gt; 1$  ，随着网络层数增多，梯度会像滚雪球一样 <strong>指数级增大</strong> 。极端情况下，梯度过大可能让参数更新变得异常剧烈，模型参数值飙升，训练过程难以稳定，甚至无法收敛，这就是<strong>梯度爆炸</strong>现象。   当 $\frac{\partial h_n}{\partial h_{n-1}} &lt; 1$ ，随着网络层数不断加深，梯度会 <strong>指数级衰减</strong> 。传到前面层（比如靠近输入的层 ）时，梯度会变得极其微小，几乎接近 0 。这会导致这些层的参数更新停滞，模型很难学到深层有意义的特征，训练效果大打折扣，就是<strong>梯度消失问题</strong>。</p>
<p>RNN单元的变体：GRU、LSTM。核心：计算时，保存周围的记忆，来捕捉远距离的依赖性。</p>
<h4 id="门控循环单元gru">门控循环单元（GRU）
</h4><p>GRU:Gated Recurrent Unit</p>
<p>GRU是RNN的一个变体，其包括更新门以及重置门。</p>
<p>重置门：考虑到上一层的隐藏状态对当前激活，可通过计算获得一个新的临时的激活。当重置门$r_i \approx 0$时，新的激活值和上一状态几乎没有关系。
</p>
$$
r_i=\sigma(W_x^{(r)}x_i + W_h^{(r)}h_{i-1} + b^{(r)} )
$$<p>
更新门：权衡目前新得到的激活$h_i$和过去状态$h_{i-1}$的影响。
</p>
$$
z_i = \sigma(W_x^{(z)} x_i + W_h^{(z)} h_{i-1} + b^{(z)} )
$$<p>
则新的激活值
</p>
$$
\widetilde{h_i}=tanh(W_xx_i + r_i * W_h h_{i-1} + b )
$$<p>
最终隐藏层的状态：
</p>
$$
h_i = z_i * h_{i-1} + (1 - z_i)* \widetilde{h_i}
$$<h4 id="长短期记忆网络lstm">长短期记忆网络（LSTM）
</h4><p>LSTM：Long Short-Term Memory Network</p>
<p>也是RNN的变体，可以学习长期的数据一来关系，关键：<strong>cell</strong></p>
<p>遗忘门$f_t = \sigma(W \cdot [h_{t-1},x_t] + b_f )$，若$f_t =0$则直接丢弃上一个状态。</p>
<p>输入门，决定有哪些信息可以存入到cell状态中去</p>
<p>输入门$i_t$以及新的待选的cell状态如下：
</p>
$$
i_t= \sigma(W_i \cdot [h_{t-1},x_t] + b_i ) \\
\widetilde{C_t} = tanh(W_C \cdot [h_{t-1},x_t] + b_C )
$$<p>
则cell状态：$C_t = f_t * C_{t-1} + i_t * \widetilde{C_t}$</p>
<p>输出门，决定哪些信息可以进行输出
</p>
$$
o_t =\sigma(W_o \cdot [h_{t-1},x_t] + b_o ) \\
h_t = o_t * tamh(C_t)
$$<h4 id="双向rnn">双向RNN
</h4><p>RNN的变体</p>
<p>让当前的predit不仅取决于过去的状态，而且取决于未来的状态</p>
<h2 id="transformer">Transformer
</h2><h3 id="注意力机制">注意力机制
</h3><p>解决信息瓶颈。核心思想：在decoder的每一步都把encoder端所有的向量提供给decoder模型。</p>
<p>步骤：</p>
<ol>
<li>先利用RNN得到一个向量$s_t$</li>
<li>再利用向量$s_1$与encoder所有向量做点积，得到注意力分数$e^t=[s_t^Th_1,\cdots,s_t^Th_N]$</li>
<li>利用softmax将注意力分数变为一个概率分布$\alpha^t=softmax(e^t)$，decoder更关注概率越大的位置的encoder向量。</li>
<li>利用概率分布对encoder向量进行加权平均$o_t=\sum_{i=1}^N \alpha_i^th_i$</li>
<li>拼接$[o_t;s_t]$得到最终用于生成predict的向量</li>
</ol>
<h4 id="注意力机制的各种变式">注意力机制的各种变式
</h4><ol>
<li>若decoder端向量s<strong>维度</strong>与encoder端向量$h_i$不一样，则计算注意力分数时，需添加一个权重矩阵</li>
</ol>
$$
e_i=s^T W h_i
$$<ol start="2">
<li>Additive attention：使用一层前馈神经网络</li>
</ol>
$$
e_i=v^T tanh(W_1 h_i + W_2 s)
$$<h4 id="注意力机制的特点">注意力机制的特点
</h4><ul>
<li>
<p>decoder端每次生成的时候，可以关注到encoder端所有信息，解决信息瓶颈问题。</p>
</li>
<li>
<p>缓解RNN中的梯度消失问题。通过在encoder和decoder之间提供一种直接连接的方式，防止梯度在RNN中传播过长，进而导致梯度消失。</p>
</li>
<li>
<p>给神经网络模型，提供了可解释性。</p>
</li>
</ul>
<h3 id="transformer机制">Transformer机制
</h3><h4 id="模型结构">模型结构
</h4><ol>
<li>输入层，将一个文本序列切成一个小的单元token，然后通过embedding可以化为一个向量表示。
<ol>
<li>transformer采用Byte Pair Encoding的方式来对文本进行切分（BPE方法）</li>
<li>在每个位置加上一个token的位置向量，叫positional encoding，用来表示它在文本序列中的位置</li>
</ol>
</li>
<li>主要图层部分，主要由encoder或者decoder的Transformer block堆叠而成。</li>
<li>输出层，线性层的变换和softmax，输出一个在词表上的概率分布</li>
</ol>
<h4 id="输入编码-bpepe">输入编码 BPE，PE
</h4><p>BPE过程：首先将语料库中出现的所有单词切分为一个个字母，然后通过统计在语料库中每一个byte gram出现的数量，一个一个把频度最高的Byte gram抽象成一个词加入词表中。</p>
<p>byte gram：连续两个相邻位置字母拼到一起的组合</p>
<p>PE（Positional Encoding）：通过在原有的embedding上加上一个位置向量，让不同位置的单词具有不同的表示，进而让Transformer block可以进行区分。</p>
<p>首先假设经过BPE和embedding之后的向量维度d，则位置编码也是一个维度为的向量，Transformer采用基于三角函数的方法来得到位置向量。具体公式如下：
</p>
$$
PE_{(pos,2i)}=sin(pos/1000^{2i/d}) \\
PE_{pos,2i+1}=cos(pos/1000^{2i/d})
$$<p>
其中pos表示当前token在句子中的位置，是从0到这个序列长度的一个数。i为从0到d/2的一个数，表示当前这个位置在embedding中的index。</p>
<p><strong>Input = BPE + PE</strong></p>
<h4 id="encoder-block">Encoder Block
</h4><p>整体由两大块组成，分别为Muti-Head Attention网络，Feed-Forward Network前馈神经网络（本质上是一个带激活函数的MLP全连接）</p>
<p>两个技巧：</p>
<ol>
<li>
<p>残差连接：将输入和输出直接相加，缓解模型过深后带来的梯度消失问题。</p>
</li>
<li>
<p>正则化：将输入变为一个均值为0，方差为一的分布，解决梯度消失与梯度爆炸。</p>
</li>
</ol>
<h5 id="注意力机制-1"><strong>注意力机制</strong>
</h5><p><strong>输入</strong>为<strong>query q</strong>和key-value(k，v)的集合。query和key向量的维度为$d_k$，Value向量的维度为$d_v$。</p>
<p>**输出：**先对value向量做加权平均
</p>
$$
A(q,K,V)=\sum_i\frac{e^{q\cdot k_i}}{\sum_j e^{q\cdot k_j}}v_i
$$<p>
当有多个q时，其可以组成矩阵Q，则：
</p>
$$
A(Q,K,V)=softmax(QK^T)V
$$<p>
<strong>Scaled Dot-Product Attention:</strong>
若没有Scale，当$d_k$增大时，$q^T k$的方差也随之增大，则softmax后概率分布可能会更尖锐，导致梯度越来越小，使得参数不利于更新。</p>
<p>加入Scale，如下：
</p>
$$
A(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$<p>
使得注意力分数方差依然为1.</p>
<p><strong>注意力机制的流程如下：</strong></p>
<p><img src="/Hugo/assets/image-20250711172044512.png"
	
	
	
	loading="lazy"
	
		alt="image-20250711172044512"
	
	
></p>
<h5 id="多头注意力机制">多头注意力机制
</h5><p>采用多个结构相同，但参数不同的注意力模块，组成多头注意力机制。</p>
<p><img src="/Hugo/assets/image-20250711172243703.png"
	
	
	
	loading="lazy"
	
		alt="image-20250711172243703"
	
	
></p>
<p>每一个注意力头的计算方式为：
</p>
$$
head_i=A(QW_i^Q,KW_i^K,VW_i^V)
$$<p>
当每一个注意力头得到输出后，将输出在维度上进行拼接，然后通过一个线性层进行整合：
</p>
$$
MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O
$$<p>
一个Encodr Block的结构：</p>
<p><img src="/Hugo/assets/image-20250711173107761.png"
	
	
	
	loading="lazy"
	
		alt="image-20250711173107761"
	
	
></p>
<h4 id="decoder-block">Decoder Block
</h4><p>一个Decoder Block的结构：</p>
<p><img src="/Hugo/assets/image-20250711173227452.png"
	
	
	
	loading="lazy"
	
		alt="image-20250711173227452"
	
	
></p>
<p>Masked Attention Score：将注意力分数修改为$-\infty$，使得在softmax时，使之变0。用于decoder在文本生成时，不受前面生成文本的影响。</p>
<h3 id="优点和缺点">优点和缺点
</h3><h4 id="优点">优点
</h4><p>表示能力强，本身适合并行计算，成为预训练语言模型的一个主要框架</p>
<h4 id="缺点">缺点
</h4><p>对于参数敏感，优化困难。处理文本复杂度与文本长度$n$为平方关系$O(n^2)$，导致对长度特别长的文本束手无策。</p>
<h2 id="预训练语言模型">预训练语言模型
</h2><p>对语言模型的预训练的一个过程</p>
<h3 id="plmspre-trained-lanuage-models">PLMs（Pre-trained Lanuage Models）
</h3><p>好处：在语言模型的预训练之后，学习到的知识可以非常容易地去迁移到各种下游任务。</p>
<p>预训练语言模型整体可以分为两种范式：</p>
<ul>
<li>Feature-based：作为feature的提取器，在大规模的预料上预训练好模型参数后，把它编码的表示作为一个固定的feature，来交给下游做具体任务的模型，作为下游任务模型的输入。</li>
<li>Fine-turning（主流）：<strong>通过特定领域数据对预训练模型进行针对性优化，以提升其在特定任务上的性能。</strong></li>
</ul>
<h3 id="masked-lm">Masked LM
</h3><p>BERT为了解决LM是单向的过程，提出Masked LM。 其是Bert预训练中最核心的任务。</p>
<p>基本思想：利用双向信息来预测target token。<strong>掩盖掉$k %$（15% in Bert）的input word，然后去预测这些被掩盖的word。</strong></p>
<p>mask太少，预训练花的时间很长。</p>
<p>mask太多，文章可利用的信息太少。</p>
<h3 id="few-shotin-context-learning">few-shot/in-context learning
</h3><p>zero-shot：不给任何数据，只给任务的描述，就能完成任务</p>
<p>few-shot：给一点样例，使之完成任务。</p>
<p>in-context learning：没有针对下游任务做任何参数的更新，只是在上下文中给了这个任务的描述以及样例，通过语言模型自回归地生成去完成任务</p>
<p>Mixture of Experts：把模型参数分成一块一块的模块，每次模型的输入会调用其中的部分子模块来参与计算。每一个子模块相当于experts。</p>
<h2 id="transformer-使用">Transformer 使用
</h2><h3 id="pipeline">Pipeline
</h3><p>自动帮忙选择适配任务的模型</p>
<h3 id="tokenization">Tokenization
</h3><p>不同的模型有不同的tokenization方式</p>
<h2 id="prompt-learning">Prompt-learning
</h2><p>Promopt-learning给模型额外加一些上下文，去trigger一些想要的token，再用这些token进行后续的处理，进行分类等操作。</p>
<ol>
<li>prompt-learning使用预训练语言模型作为基础的encoders</li>
<li>其加入了一些额外的包含mask的内容，称为template</li>
<li>将标签映射为标签词的过程，称为verbalizer</li>
</ol>
<h3 id="verbalizer">Verbalizer
</h3><p>构成：</p>
<ol>
<li>人经验的选取一个初始词</li>
<li>利用初始词，进行同义词改写扩大词表</li>
<li>利用初始词，用额外的知识库扩大词表</li>
<li>把一个长label分解为多个label</li>
</ol>
<h2 id="bmtrain">BMtrain
</h2><h3 id="数据并行">数据并行
</h3><p>把数据分成3份，送至3张显卡上，且每张显卡复制所有的参数。每张显卡对数据进行前向传播和反向传播，得到各自的梯度。将所有梯度进行聚合，利用聚合好的参数调整模型。</p>
<h4 id="分布式数据并行">分布式数据并行
</h4><p>其没有参数服务器。</p>
<p>每张显卡各自完成参数的更新，让所有显卡参数更新保持一致。</p>
<h3 id="模型并行">模型并行
</h3><p>以线性层为例，把模型分成很多个小的部分，即将参数分成n份，n为显卡的个数，分别发送至每张显卡上，每张显卡上的参数与完整的输入数据进行乘法。最后进行聚合。 由于每张显卡都需要处理完整的输入数据，导致其中间计算结果并没有减少，仍然可能造成显存的溢出。</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/Hugo/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/Hugo/p/linux/">
        
        

        <div class="article-details">
            <h2 class="article-title">Linux</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/Hugo/p/machine-learning/">
        
        

        <div class="article-details">
            <h2 class="article-title">Machine-Learning</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2025 Wjj
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/Hugo/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>

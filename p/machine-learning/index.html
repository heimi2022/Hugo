<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="引言 什么是机器学习 定义如下，一个程序被认为能从 经验 E 中学习，解决 任务 T，达到 性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n">
<title>Machine-Learning</title>

<link rel='canonical' href='https://heimi2022.github.io/Hugo/p/machine-learning/'>

<link rel="stylesheet" href="/Hugo/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css"><meta property='og:title' content="Machine-Learning">
<meta property='og:description' content="引言 什么是机器学习 定义如下，一个程序被认为能从 经验 E 中学习，解决 任务 T，达到 性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n">
<meta property='og:url' content='https://heimi2022.github.io/Hugo/p/machine-learning/'>
<meta property='og:site_name' content='WJJ'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='Machine Learning' /><meta property='article:published_time' content='2025-05-22T16:36:28&#43;08:00'/><meta property='article:modified_time' content='2025-05-22T16:36:28&#43;08:00'/>
<meta name="twitter:title" content="Machine-Learning">
<meta name="twitter:description" content="引言 什么是机器学习 定义如下，一个程序被认为能从 经验 E 中学习，解决 任务 T，达到 性能度量值 P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。\n">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/Hugo/">
                
                    
                    
                    
                        
                        <img src="/Hugo/img/avatar_hu_be2515b413f73217.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">📖</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/Hugo">WJJ</a></h1>
            <h2 class="site-description">欢迎来到我的博客</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://github.com/heimi2022/'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/Hugo/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/%E5%85%B3%E4%BA%8E/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>关于</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>归档</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>搜索</span>
            </a>
        </li>
        
        
        <li >
            <a href='/Hugo/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>友情链接</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>暗色模式</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#引言">引言</a>
      <ol>
        <li><a href="#什么是机器学习">什么是机器学习</a></li>
        <li><a href="#监督学习">监督学习</a></li>
        <li><a href="#无监督学习">无监督学习</a></li>
      </ol>
    </li>
    <li><a href="#单变量线性回归linear-regression-with-one-variable">单变量线性回归(Linear Regression with One Variable)</a>
      <ol>
        <li><a href="#模型表示">模型表示</a></li>
        <li><a href="#代价函数">代价函数</a></li>
        <li><a href="#梯度下降">梯度下降</a>
          <ol>
            <li><a href="#批量梯度下降算法batch-gradient-descent">批量梯度下降算法(batch gradient descent)</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#多变量线性回归">多变量线性回归</a>
      <ol>
        <li><a href="#多维特征">多维特征</a></li>
        <li><a href="#多变量梯度下降">多变量梯度下降</a></li>
        <li><a href="#梯度下降法">梯度下降法</a>
          <ol>
            <li><a href="#特征缩放">特征缩放</a></li>
            <li><a href="#学习率">学习率</a></li>
          </ol>
        </li>
        <li><a href="#特征选择与多项式回归">特征选择与多项式回归</a>
          <ol>
            <li><a href="#特征选择">特征选择</a></li>
            <li><a href="#多项式回归">多项式回归</a></li>
          </ol>
        </li>
        <li><a href="#正规方程">正规方程</a>
          <ol>
            <li><a href="#梯度下降于正规方程的比较">梯度下降于正规方程的比较</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#逻辑回归">逻辑回归</a>
      <ol>
        <li><a href="#假说表示">假说表示</a></li>
        <li><a href="#决策边界">决策边界</a></li>
        <li><a href="#代价函数-1">代价函数</a>
          <ol>
            <li><a href="#逻辑回归中-cost-函数特点">逻辑回归中 Cost 函数特点</a></li>
            <li><a href="#简化后的代价函数">简化后的代价函数</a></li>
          </ol>
        </li>
        <li><a href="#高级优化">高级优化</a></li>
        <li><a href="#多类别分类">多类别分类</a></li>
      </ol>
    </li>
    <li><a href="#正则化">正则化</a>
      <ol>
        <li><a href="#过拟合">过拟合</a>
          <ol>
            <li><a href="#解决过拟合">解决过拟合</a></li>
          </ol>
        </li>
        <li><a href="#代价函数-2">代价函数</a>
          <ol>
            <li><a href="#正则化的直观理解">正则化的直观理解</a></li>
            <li><a href="#带正则化参数的代价函数">带正则化参数的代价函数</a></li>
          </ol>
        </li>
        <li><a href="#正则化线性回归">正则化线性回归</a>
          <ol>
            <li><a href="#梯度下降-1">梯度下降</a></li>
            <li><a href="#正规化">正规化</a></li>
          </ol>
        </li>
        <li><a href="#正则化逻辑回归">正则化逻辑回归</a></li>
      </ol>
    </li>
    <li><a href="#神经网络">神经网络</a>
      <ol>
        <li><a href="#模型表示-1">模型表示</a>
          <ol>
            <li><a href="#举例">举例</a></li>
          </ol>
        </li>
      </ol>
    </li>
    <li><a href="#神经网络的学习">神经网络的学习</a>
      <ol>
        <li><a href="#代价函数-3">代价函数</a></li>
        <li><a href="#反向传播算法">反向传播算法</a>
          <ol>
            <li><a href="#反向传播算法中误差的计算">反向传播算法中误差的计算</a></li>
            <li><a href="#忽略代价函数中的正则化项">忽略代价函数中的正则化项</a></li>
            <li><a href="#考虑正则化处理">考虑正则化处理</a></li>
          </ol>
        </li>
        <li><a href="#梯度检验">梯度检验</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/Hugo/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="background-color: #2a9d8f; color: #fff;">
                学习笔记
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/Hugo/p/machine-learning/">Machine-Learning</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">2025-05-22</time>
            </div>
        

        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h2 id="引言">引言
</h2><h3 id="什么是机器学习">什么是机器学习
</h3><p>定义如下，一个程序被认为能从 <strong>经验 E</strong> 中学习，解决 <strong>任务 T</strong>，达到 <strong>性能度量值 P</strong>，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。</p>
<p>例如下棋游戏:<br>
E 就是程序上万次的自我练习的经验。<br>
T 就是下棋。<br>
P 就是它在与一些新的对手比赛时，赢得比赛的概率。</p>
<h3 id="监督学习">监督学习
</h3><p><strong>监督学习</strong> 指的就是我们给学习算法一个 <strong>数据集</strong>。这个数据集由“正确答案”组成。学习算法再根据这个数据集作出预测，算出更多的正确答案。<br>
监督学习的两个例子, <strong>回归问题 与 分类问题</strong>。</p>
<p>回归问题 : 推测出一个 <strong>连续</strong> 值的输出结果。<br>
例如 : 卖水果，数据集 3 斤卖 10 元左右，预测 4 斤能卖多少。</p>
<p>分类问题 : 推测出一组 <strong>离散</strong> 的结果。<br>
例如 : 识别红绿灯。</p>
<h3 id="无监督学习">无监督学习
</h3><p><strong>无监督学习</strong> 指的是一种学习策略，它交给算法大量的数据，并让算法为我们从数据中找出某种结构。<br>
无监督学习中 <strong>已知数据没有任何的标签</strong> 是指数据 <strong>有相同的标签或者就是没标签</strong>。<br>
无监督学习的例子，<strong>聚类算法、鸡尾酒算法</strong></p>
<p>聚类算法 : 将数据分为几类不同的 <strong>簇</strong>。<br>
例子 : 谷歌新闻，同一主题新闻归为一类; 市场分割。</p>
<p>鸡尾酒算法 : 分离麦克风接收到的不同的人的声音与环境声音。</p>
<h2 id="单变量线性回归linear-regression-with-one-variable">单变量线性回归(Linear Regression with One Variable)
</h2><h3 id="模型表示">模型表示
</h3><p>样本数目 : 小写 <strong>m</strong><br>
训练集 : <strong>Training Set</strong><br>
特征/输入变量 : <strong>$x$</strong><br>
目标/输出变量 : <strong>$y$</strong><br>
训练集中的实例 : <strong>($x$, $y$)</strong><br>
第 $i$ 个观察实例 : <strong>($x^{(i)}$, $y^{(i)}$)</strong><br>
<strong>学习算法的解决方案或函数也称为假设(hypothesis) : h</strong></p>
<p>模型表示就是找到将训练集给学习算法找到一个合适的函数 $h$ , $y = h(x)$</p>
<p>函数 $h_\theta(x) = \theta_0 + \theta_1x$<br>
由于只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。</p>
<h3 id="代价函数">代价函数
</h3><p>建模误差: 模型所预测的值与训练集中实际值之间的差距 $h_\theta(x^{(i)}) - y^{(i)}$</p>
<p>平方误差函数 $J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$<br>
平方误差函数是代价函数之一，对于大多数问题，特别是回归问题，其都是个合理的选择。<br>
为什么是 $\frac{1}{2m}$:<br>
除以 m 是为了消除样本数量 m 对 J 的影响<br>
除以 2 是为了抵消对 J 关于 θ 求偏导数时式子多出的一个 2，使计算更方便</p>
<p>我们的目标就是找到一个合适的参数 $\theta_0,\theta_1$ 使得代价函数最小。 即 Goal: $\underset{\theta_0,\theta_1}{minimize}J(\theta_0,\theta_1)$</p>
<h3 id="梯度下降">梯度下降
</h3><p>梯度下降是一个用来 <strong>求函数最小值</strong> 的算法，可以用其来求出使代价函数 $J$ 最小的参数 $\theta_0$ 和 $\theta_1$ 的值。</p>
<p><strong>梯度下降的思想</strong> : 开始时，随机选择一组参数 $(\theta_0,\theta_1)$, 计算代价函数, 然后寻找下一个能让代价函数下降最多的参数组合。持续这种操作，直至找到一个局部最小值。<br>
局部最小值不等同于全局最小值。不同的初始参数会可能会得到完全不同的局部最小值。</p>
<h4 id="批量梯度下降算法batch-gradient-descent">批量梯度下降算法(batch gradient descent)
</h4><p>批量 batch 指的是，在梯度下降的每一步中，都用到来所有的训练样本。</p>
<p>公式为:
</p>
$$
\begin{aligned}
&\text{repeat until convergence（重复直至收敛）} \{ \\
&\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad (\text{for } j = 0 \text{ and } j = 1) \\
&\}
\end{aligned}
$$<p>其中 $\alpha$ 是学习率，其决定了我们沿代价函数梯度方向向下迈出的幅度有多大。<br>
$\alpha$ 太小，需要很多步才能够达到局部最低点。<br>
$\alpha$ 太大，梯度下降法可能会越过最低点，甚至可能无法收敛，导致发散。</p>
<p>在梯度下降法中，当参数取值接近局部最低点时，梯度下降法会自动采取更小的幅度，因为代价函数导数会趋向于 0。</p>
<h2 id="多变量线性回归">多变量线性回归
</h2><h3 id="多维特征">多维特征
</h3><p>新的注释：</p>
<p>$n$ 代表特征的数量</p>
<p>$x^{(i)}$ 代表第 $i$ 个训练实例，是特征数据的第 $i$ 行，是一个 **向量 **</p>
<p>$x_j^{(i)}$ 代表特征矩阵中第 $i$ 行第 $j$ 个特征</p>
<p>多变量的假设 $h$ 表示为：<br>
</p>
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
$$<p>
此时模型中参数为一个 $n+1$ 维向量</p>
<p>任何一个训练实例也都是一个 $n+1$ 维向量</p>
<p>特征矩阵 X 的维度是 $m*(n+1)$</p>
<p>因此公式可以简化为</p>
$$
h_\theta(x)=\theta^TX
$$<h3 id="多变量梯度下降">多变量梯度下降
</h3><p>代价函数为所有建模误差的平方和，即:<br>
</p>
$$
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2
$$<p>多变量线性回归的批量梯度下降算法为：
</p>
$$
\begin{aligned}
&\text{repeat} \{ \\
&\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1,...\theta_n) \\ 
&\}
\end{aligned}
$$<p>即:</p>
$$
\begin{aligned}
&\text{repeat} \{ \\
&\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2 \\
&\}
\end{aligned}
$$<p>
求导后得到：
</p>
$$
\begin{aligned}
&\text{repeat} \{ \\
&\quad \theta_j := \theta_j - \alpha  \frac{1}{m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)}) \cdot x_j^{(i)} \\
&\}
\end{aligned}
$$<h3 id="梯度下降法">梯度下降法
</h3><h4 id="特征缩放">特征缩放
</h4><p>使特征具有相近的尺度有助于梯度下降宣发更快的收敛。</p>
<p>方法：将所有特征的尺度都尽量缩放到-1 到 1 <strong>这种小范围</strong> 内，也不能太小。</p>
<p>最简单的，令 $x_n=\frac{x_n-\mu_n}{s_n}$, 其中 $\mu_n$ 是平均值，$s_n$ 是标准差。</p>
<h4 id="学习率">学习率
</h4><p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制 <strong>迭代次数-代价函数</strong> 的图表来观测算法在何时收敛。</p>
<p>梯度下降算法的每次迭代受学习率 $\alpha $ 影响：</p>
<ol>
<li>$\alpha$ 太小，则到达收敛所需的迭代次数很大，但理论上终会收敛。</li>
<li>$\alpha $ 太大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</li>
</ol>
<p>调整学习率时可以采用以下 <strong>策略</strong>：从 0.001 开始每次上在原基础上乘 3 倍，<strong>最终选择一个较大的可以使得代价函数收敛的 $\alpha $</strong>。</p>
<p>如：0.001，0.003，0.01，0.03，0.1</p>
<h3 id="特征选择与多项式回归">特征选择与多项式回归
</h3><h4 id="特征选择">特征选择
</h4><p>建立模型时，可以 <strong>根据需要选择适合模型的特征</strong> 而不局限于数据集中的特征。比如：数据集给出房子的长和宽，预测房价时，可以创造一个特征为 *<em>面积 = 长 <em>宽</em></em> 作为模型的新特征。</p>
<h4 id="多项式回归">多项式回归
</h4><p><strong>线性回归并不适合所有数据</strong>，有时需要曲线来适应数据，比如：</p>
<p>三次方模型 $h_\theta(x)=\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$</p>
<p>使用多项式回归模型，特征缩放很有必要。</p>
<h3 id="正规方程">正规方程
</h3><p>当特征数量小于 10000 时（对于目前的计算机），通常采用正规方程的方法找到一组最优参数 $\theta$。</p>
<p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial \theta_j} J(\theta_j)=0$，</p>
<p>其 <strong>解</strong> 为 $\theta=(X^TX)^{-1}X^Ty$。$X$ 为训练集特征矩阵，$y$ 为训练集结果。</p>
<h4 id="梯度下降于正规方程的比较">梯度下降于正规方程的比较
</h4><div class="table-wrapper"><table>
  <thead>
      <tr>
          <th style="text-align: center">梯度下降</th>
          <th style="text-align: center">正规方程</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">需要选择学习率 $\alpha $</td>
          <td style="text-align: center">不需要</td>
      </tr>
      <tr>
          <td style="text-align: center">需要多次迭代</td>
          <td style="text-align: center">一次运算得出</td>
      </tr>
      <tr>
          <td style="text-align: center">特征数量 n 大时也可用</td>
          <td style="text-align: center">矩阵逆的计算时间复杂度为 $O(n^3)$。因此 n 小于 10000 时，可以接受</td>
      </tr>
      <tr>
          <td style="text-align: center">使用于各种类型</td>
          <td style="text-align: center">只适用于线性模型。不适合逻辑回归等其它模型。</td>
      </tr>
  </tbody>
</table></div>
<p>以下情况时，正规方程不能用：</p>
<ol>
<li>有两个特征线性相关。  解决：使用正规方程前，剔除相关特征。</li>
<li>含有大量特征，进而出现 $m \leqslant n$ 时。  解决：剔除特征，用较少特征反应尽可能多的内容。</li>
</ol>
<h2 id="逻辑回归">逻辑回归
</h2><p>逻辑回归算法是一种分类算法。</p>
<p>分类问题预测的变量 $y$ 为 <strong>离散值</strong>。</p>
<h3 id="假说表示">假说表示
</h3><p>引入一个 <strong>逻辑函数 Sigmoid function</strong>，S 形函数，公式为 $g(z)=\frac{1}{1+e^{-z}}$。其图像如下所示：</p>
<center>
    <img src="https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_sigmoid_function.png"
         width="40%">
    <br>
    sigmoid function
</center>
<p>令逻辑回归模型的假设是：$h_\theta(x)=g(\theta^T X)$, $X$ 代表特征向量，$g$ 代表逻辑函数。</p>
<p>此时模型的输出变量范围始终在 0 和 1 之间。</p>
<p>$h_\theta(x)$ 表示，对于给定的输入变量，根据选择的参数，计算输出变量为 1 的可能性，即：
</p>
$$
h_\theta(x)= P(y = 1|x;\theta)
$$<h3 id="决策边界">决策边界
</h3><p>在逻辑回归中，假设：<br>
</p>
$$
y = \begin{cases}
1, & \text{if } h_\theta(x) \geq 0.5 \\
0, & \text{if } h_\theta(x) < 0.5
\end{cases}
$$<p>因此当 $\theta^T x \geqslant 0$ 时，预测 $y=1$，当 $\theta^T x &lt; 0$ 时，预测 $y=0$。</p>
<p>因此对于一组参数 $\theta$，有 $\theta^T x=0$，这条 <strong>曲线</strong> 即为模型的决策边界。</p>
<h3 id="代价函数-1">代价函数
</h3><p>为什么逻辑回归的代价函数和线性回归的不同？由于 sigmoid 函数的非线性，导致逻辑回归的假设函数带入代价函数时，会导致代价函数有很多局部最小值，其是一个非凸函数。因此需对代价函数进行修正。</p>
<p>定义逻辑回归的代价函数 $J(\theta)=\frac{1}{m} \sum^m_{i=1} Cost(h_\theta(x^{(i)}),y^{(i)})$，其中
</p>
$$
Cost(h_\theta(x^{(i)}), y^{(i)}) = \begin{cases}
   -log( h_\theta(x)) \quad &  if\quad y = 1, &  \\
   -log( 1- h_\theta(x)) &  if\quad y = 0, &  \\
\end{cases}
\overset{简化}{=} -ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))
$$<h4 id="逻辑回归中-cost-函数特点">逻辑回归中 Cost 函数特点
</h4><p>当实际的 $y=1$ 且 $h_\theta(x)=1$ 时，误差为 0，当 $y=1$ 但 $h_\theta(x)$ 不为 1 时，误差随着 $h_\theta(x)$ 的变小而变大。当 $h_\theta(x)$ 减小为 0，表示预测与实际完全相反，其误差为无穷大，这时代价函数将会受到很大的惩罚；当实际的 $y=0$ 且 $h_\theta(x)=0$ 时，误差为 0，当 $y=0$ 但 $h_\theta(x)$ 不为 0 时，误差随着 $h_\theta(x)$ 的变大而变大。</p>
<h4 id="简化后的代价函数">简化后的代价函数
</h4>$$
\begin{aligned}
J(\theta)& = \frac{1}{m} \sum^m_{i = 1} Cost(h_\theta(x^{(i)}), y^{(i)}) & \\
& = -\frac{1}{m} [\sum^m_{i = 1}-ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))] &\\
\end{aligned}
$$<p>最小化代价函数的方法 <strong>是梯度下降法</strong>：
</p>
$$
\begin{aligned}
&\text{Repeat} \{ \\
& \theta_j := \theta_j - \alpha \frac{1}{m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&\}
\end{aligned}
$$<p>虽然其形式上与线性回归的梯度下降法相同，但由于回归的模型假设发生了变化，两者是完全不同的函数。</p>
<h3 id="高级优化">高级优化
</h3><p>优化算法：共轭梯度法 BFGS（变尺度法）和 L-BFGS（限制遍尺度法）。</p>
<p>优点：不需要手动选择学习率。</p>
<h3 id="多类别分类">多类别分类
</h3><p>假设有 n 个类别，则将其分为 <strong>n 个</strong> 1 对（n-1）的 <strong>二元分类问题</strong>。</p>
<p>记每一个分类问题的模型为：$h_\theta ^{(i)} (x)= p(y=i|x;\theta)$，其中 $i=(1,2,3,&hellip;,k)$。</p>
<p>最终输出结果为让 $h_\theta ^{(i)} (x)$ 最大的 i，即 $\mathop{max}\limits_{i} ; h_\theta ^{(i)} (x)$。</p>
<h2 id="正则化">正则化
</h2><h3 id="过拟合">过拟合
</h3><p><strong>过拟合</strong> 指通过学习得到的假设可能能够非常好的适应训练集（代价函数可能几乎为 0），但是可能 <strong>不会</strong> 推广到预测新的数据。</p>
<p>当有 <strong>过多的变量但只有很少的训练数据</strong> 时很容易出现过拟合问题。</p>
<p><strong>欠拟合</strong> 指模型不能适应训练集。</p>
<h4 id="解决过拟合">解决过拟合
</h4><ol>
<li>减少特征变量的数目 &ndash; 人工选择去除一些变量；模型算法选择（如 PCA）。</li>
<li><strong>正则化</strong> &ndash; 保留所有的特征变量，但减小参数 $\theta$ 的大小。 其适用于有很多特征变量，且每一个特征变量看起来对预测结果都有一点用的情况。</li>
</ol>
<h3 id="代价函数-2">代价函数
</h3><h4 id="正则化的直观理解">正则化的直观理解
</h4><p>假设模型为 $h_\theta(x)=\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 + \theta_4 x^4$，且高次项导致了过拟合的产生，则 <strong>假如能让高次项的系数趋向于 0，就能够缓解过拟合的问题</strong> 了。 因此修改代价函数 ，对 $\theta_3 \text{和}\theta_4$ 设置一些惩罚，如下所示：
</p>
$$
J(\theta)=\frac{1}{2m}[\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2+1000 \theta_3^2 + 1000 \theta_4^2]
$$<p>
则通过这样的代价函数选择出来的 $\theta_3 \text{和}\theta_4$ 对预测结果的影响就很小了。</p>
<h4 id="带正则化参数的代价函数">带正则化参数的代价函数
</h4><p>修改后的代价函数如下：
</p>
$$
J(\theta)=\frac{1}{2m}[\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum^n_{j = 1}\theta_j^2]
$$<p>
其中，$\lambda$ 成为正则化参数。</p>
<p>假如 $\lambda$ 太大，则所有 $\theta$ 都会趋于 0，除了 $\theta_0$，这样最终模型为一条平行于 x 轴的直线，导致欠拟合。</p>
<h3 id="正则化线性回归">正则化线性回归
</h3><h4 id="梯度下降-1">梯度下降
</h4>$$
\begin{aligned}
& \text{Repeat until convergence} \{ \\
& \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
& \theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
& \}
\end{aligned}
$$<p>正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新柜子的基础上令 $\theta$ 值减少了一个额外的值。</p>
<h4 id="正规化">正规化
</h4>$$
\theta = (X^T X + \lambda \begin{bmatrix}
0 & & & & & \\
& 1 & & & & \\
& & 1 & & & \\
& & & \cdot & & \\
& & & & \cdot & \\
& & & & & 1 \\
\end{bmatrix}^{-1})^{-1} X^Ty
$$<p>图中矩阵尺寸为 $(n+1)*(n+1)$</p>
<h3 id="正则化逻辑回归">正则化逻辑回归
</h3><p>正则化逻辑回归的代价函数：
</p>
$$
J(\theta) = \frac{1}{m} [\sum^m_{i = 1}-ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))] + \frac{\lambda}{2m} \sum^n_{j = 1}\theta^2_j 
$$<p>
则梯度下降算法为：
</p>
$$
\begin{aligned}
& \text{Repeat until convergence} \{ \\
& \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
& \theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
& \}
\end{aligned}
$$<h2 id="神经网络">神经网络
</h2><h3 id="模型表示-1">模型表示
</h3><p>神经网络模型建立在很多神经元之上，每一个神经元又是一个学习模型。这些神经元也叫激活单元（activation unit）。</p>
<p>神经网络模型是许多逻辑单元按照不同的层级组织起来的网络，每一层的输出变量都是下一层的输入变量。</p>
<p>引入以下标记法来描述模型：</p>
<ol>
<li>$\alpha _i ^{(j)}$代表第$j$层的第$i$个激活单元。</li>
<li>$\theta^{(j)}$代表从第$j$层映射到第$j+1$层时的权重矩阵。其<strong>尺寸</strong>为以第$j+1$层的激活单元数量作为行数，以第$j$层的激活单元数量<strong>加一</strong>作为列数。</li>
</ol>
<h4 id="举例">举例
</h4><center>
    <img src="https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_Neural_Network_layer3_example"
         width="40%">
    <br>
    sigmoid function
</center>
<p>对于上图所示模型，激活单元和输出分别表达为：
</p>
$$
\begin{aligned}
& \alpha_1^{(2)}=g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1  + \Theta_{12}^{(1)}x_2  + \Theta_{13}^{(1)}x_3 ) \\
& \alpha_2^{(2)}=g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1  + \Theta_{22}^{(1)}x_2  + \Theta_{23}^{(1)}x_3 ) \\
& \alpha_3^{(2)}=g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1  + \Theta_{32}^{(1)}x_2  + \Theta_{33}^{(1)}x_3 ) \\
& h_\theta(x)=g(\Theta_{10}^{(2)}\alpha_0^{(2)} + \Theta_{11}^{(2)}\alpha_1^{(2)}  + \Theta_{12}^{(2)}\alpha_2^{(2)}  + \Theta_{13}^{(2)}\alpha_3^{(2)} ) \\
\end{aligned}
$$<p>
将例中这种从左往右的算法称为<strong>向前传播算法</strong>。</p>
<p>若只看输出部分，其输出方式类似逻辑回归模型，可以将$\alpha $看成是更高级的特征值。</p>
<p><strong>多类分类</strong>问题：输出层用n个神经元，表示n类。</p>
<h2 id="神经网络的学习">神经网络的学习
</h2><h3 id="代价函数-3">代价函数
</h3><p>新的标记方法：</p>
<p>假设神经网络的训练样本由$m$个，每个包含一组输入$x$和一组输出信号$y$</p>
<p>$L$表示神经网络的层数</p>
<p>$S_l$表示$l$层中神经元的个数，$S_L$表示最后一层中神经元的个数</p>
<p>神经网络的分类定义为两种情况：二类分类和多类分类，</p>
<pre><code>1. 二类分类：$S_L=1$,$y=0 \text{ or } 1$
1. $K$类分类：$S_L=k$,$y_i=1$表示分到第$i$类；$(K&gt;2)$
</code></pre>
<p>在神经网络中，$h_\theta(x)$是一个维度为$K$的向量，则：$h_\theta(x) \in R^K,(h_\theta(x))_i=i^{th} output$</p>
<p>代价函数如下：
</p>
$$
J(\Theta)=-\frac{1}{m}[\sum_{i=1}^m \sum_{k=1}^k y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})log(1 - (h_\Theta(x^{(i)}))_k) ] + \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{ji}^{(l)})^2
$$<ol>
<li>对于每一行特征：由于有$K$个特征，可以利用循环，对每一行特征都预测$K$个不同结果，然后利用循环将$K$个预测偏差累加。</li>
<li>对于正则化这一项，遍历所有层的所有参数。</li>
</ol>
<h3 id="反向传播算法">反向传播算法
</h3><p>为了<strong>计算代价函数的偏导数</strong>$\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)$，需要采用一种反向传播算法。</p>
<p>反向传播算法：首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。</p>
<h4 id="反向传播算法中误差的计算">反向传播算法中误差的计算
</h4><p>用$\delta$来表示误差。对于一个4层的网络，反向传播误差分3步计算：</p>
<ol>
<li>$\delta^{(4)}=a^{(4)}-y$</li>
<li>$\delta^{(3)}=(\Theta^{(3)})^T \delta^{(4)}*g&rsquo;(z^{(3)})$，$g&rsquo;(z^{(3)})$是sigmoid函数的导数，且$g&rsquo;(z^{(3)})= a^{(3)} * (1 - a^{(3)})$，$(\Theta^{(3)})^T \delta^{(4)}$是权重导致的误差的和。</li>
<li>$\delta^{(2)}=(\Theta^{(2)})^T \delta^{(4)}*g&rsquo;(z^{(2)})$</li>
</ol>
<p>对于4层的网络，设隐藏层都只有两个神经元，则$\delta_2^{(2)}=\Theta_{12}^{(2)} \delta_1^{(3)}+\Theta_{22}^{(2)} \delta_2^{(3)}$。</p>
<h4 id="忽略代价函数中的正则化项">忽略代价函数中的正则化项
</h4><p><strong>忽略代价函数中正则化项，则$\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)} \delta_i^{l+1}$。其中上下标的含义：</strong></p>
<p>$l$代表目前所计算的是第几层</p>
<p>$j$代表目前计算层中的激活单元的下标，也就是下一层第$j$个输入变量的下标</p>
<p>$i$代表下一层中误差单元的下标</p>
<h4 id="考虑正则化处理">考虑正则化处理
</h4><p>则用$\Delta_{ij}^{(l)}$来表示误差矩阵，则算法表示为：
</p>
$$
\begin{aligned}
\text{for } i = 1 : m \quad \{ \\
\quad & a^{(1)} := x^{(i)} \\
\quad & \text{向前传播计算 } a^{(l)} \text{ for } l = 1, 2, 3, \dots, L \\
\quad & \text{using } \delta^{(L)} := a^{(L)} - y^{(i)} \\
\quad & \text{执行向后传播算法计算直至第二层的所有误差} \\
\quad & \Delta^{(l)}_{ij} := \Delta^{(l)}_{ij} + a^{(l)}_j \delta^{(l+1)}_i \\
\}
\end{aligned}
$$<p>
在求出了$\Delta_{ij}^{(l)}$之后，则可求出代价函数的偏导数：
</p>
$$
\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}:= \begin{cases}
\frac{1}{m}\Delta_{ij}^{(l)}+\lambda \Theta_{ij}^{(l)}& ,\quad if \quad j \ne 0 \\
\frac{1}{m}\Delta_{ij}^{(l)}&, \quad if \quad j = 0
\end{cases}
$$<h3 id="梯度检验">梯度检验
</h3><p>对一个复杂模型使用梯度下降算法时，可能会存在一些不易察觉的错误，意味着虽然代价看上去不断减小，但是最终的结果可能并不是最优解。</p>
<p><strong>解决方法：梯度检验</strong>。</p>
<p><strong>思想</strong>：通过估计梯度值来检验我门计算的导数值是否真的是预期的。</p>
<p><strong>方法</strong>：在代价函数上，对于某个特定的$\theta$，我们计算出在$\theta - \epsilon $处和$\theta + \epsilon $的代价值，然后求两个代价的平均，即为估计值。</p>
<p>针对$\theta_1$进行检验的示例：
</p>
$$
\frac{\partial}{\partial_{\theta_1}}=\frac{J(\theta_1 + \epsilon,\theta_2,\cdots,\theta_n)-J(\theta_1 - \epsilon,\theta_2,\cdots,\theta_n)}{2 \epsilon}
$$<p>
对于反向传播算法，计算出的偏导数存储在矩阵$D_{ij}^{(l)}$中，检验时，将该矩阵展开成为向量；与此同时，将$\theta$矩阵也展开为向量，针对每一个$\theta$都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，同$D_{ij}^{(l)}$比较。</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/Hugo/tags/machine-learning/">Machine Learning</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2025 Wjj
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/Hugo/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>

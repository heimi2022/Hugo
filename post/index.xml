<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on WJJ</title>
        <link>https://heimi2022.github.io/Hugo/post/</link>
        <description>Recent content in Posts on WJJ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Wjj</copyright>
        <lastBuildDate>Fri, 23 May 2025 20:12:04 +0800</lastBuildDate><atom:link href="https://heimi2022.github.io/Hugo/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>LaTex语法</title>
        <link>https://heimi2022.github.io/Hugo/p/latex%E8%AF%AD%E6%B3%95/</link>
        <pubDate>Fri, 23 May 2025 20:12:04 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/latex%E8%AF%AD%E6%B3%95/</guid>
        <description>&lt;h2 id=&#34;公式&#34;&gt;公式
&lt;/h2&gt;&lt;h3 id=&#34;上标与下标&#34;&gt;上标与下标
&lt;/h3&gt;&lt;p&gt;用^来输出上标，使用_来输出下标，使用{}包含作用范围。&lt;/p&gt;
&lt;h3 id=&#34;空格表示&#34;&gt;空格表示
&lt;/h3&gt;&lt;p&gt;两个quad空格&amp;mdash;&amp;mdash;&amp;mdash;a \qquad b&amp;mdash;&amp;mdash;&amp;mdash;两个m的宽度&lt;br&gt;
quad空格&amp;mdash;&amp;mdash;&amp;mdash;a \quad b&amp;mdash;&amp;mdash;&amp;mdash;一个m的宽度&lt;br&gt;
大空格&amp;mdash;&amp;mdash;&amp;mdash;a\ b&amp;mdash;&amp;mdash;&amp;mdash;1/3m宽度&lt;br&gt;
中等空格&amp;mdash;&amp;mdash;&amp;mdash;a;b&amp;mdash;&amp;mdash;&amp;mdash;2/7m宽度&lt;br&gt;
小空格	a,b&amp;mdash;&amp;mdash;&amp;mdash;a,b&amp;mdash;&amp;mdash;&amp;mdash;1/6m宽度&lt;br&gt;
没有空格&amp;mdash;&amp;mdash;&amp;mdash;ab&lt;br&gt;
紧贴&amp;mdash;&amp;mdash;&amp;mdash;a!b&amp;mdash;&amp;mdash;&amp;mdash;缩进1/6m宽度&lt;/p&gt;
&lt;h3 id=&#34;数学结构&#34;&gt;数学结构
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;分式 \frac{a}{b}  :  $\frac{a}{b}$&lt;/li&gt;
&lt;li&gt;根式 \sqrt[n]{a}  :  $\sqrt[n]{a}$&lt;/li&gt;
&lt;li&gt;求导 f&amp;rsquo; : $f&#39;$&lt;/li&gt;
&lt;li&gt;求和$\sum$ ：\sum&lt;/li&gt;
&lt;li&gt;极限 $\lim\limits_{x\to\infty}$：\lim\limits_{x\to\infty}&lt;/li&gt;
&lt;li&gt;积分号$\int$：\int&lt;/li&gt;
&lt;li&gt;其它 上划线:\overline{a},下划线\underline{a},上右箭头\overrightarrow{a}:$\overrightarrow{a}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;数学函数&#34;&gt;数学函数
&lt;/h3&gt;&lt;p&gt;1.三角函数 $\sin$：\sin ; $\cos$：\cos;&lt;/p&gt;
&lt;h3 id=&#34;数学符号&#34;&gt;数学符号
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;偏导数 $\partial$  ：\partial&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;数学运算&#34;&gt;数学运算
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;点乘 $a \cdot b$：\cdot&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;叉乘 $a \times b$：\times&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;除以 $a \div b$：\div&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;希腊字母&#34;&gt;希腊字母
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\alpha \beta \gamma \delta \epsilon \zeta \eta \theta$：\alpha \beta \gamma \delta \epsilon \zeta \eta \theta&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\iota \kappa \lambda \mu \nu \omicron \xi \pi$  ：\iota \kappa \lambda \mu \nu \omicron \xi \pi&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\rho \sigma \tau \upsilon \phi \chi \psi \omega$：\rho \sigma \tau \upsilon \phi \chi \psi \omega&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Alpha \Beta \Gamma \Delta \Epsilon \Zeta \Eta \Theta$：\Alpha \Beta \Gamma \Delta \Epsilon \Zeta \Eta \Theta&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Iota \Kappa \Lambda \Mu \Nu \Xi \Omicron \Pi$：\Iota \Kappa \Lambda \Mu \Nu \Xi \Omicron \Pi&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\Rho \Sigma \Tau \Upsilon \Phi \Chi \Psi \Omega$：\Rho \Sigma \Tau \Upsilon \Phi \Chi \Psi \Omega&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;关系符号&#34;&gt;关系符号
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;$\leqslant ， \geqslant ，\le ，\ge$：\leqslant ， \geqslant ，\le ，\ge&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;括号&#34;&gt;括号
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;大括号 {&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;其它&#34;&gt;其它
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;\underset{123}{abc} : $\underset{123}{abc}$&lt;/li&gt;
&lt;li&gt;换行   $\text{\ \}$&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;\limits&lt;/strong&gt; 将下标放在某个文字或者符号的正下方。&lt;/li&gt;
&lt;li&gt;用**\mathop**{文本}命令将文本转化成数学符号&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>Markdown语法</title>
        <link>https://heimi2022.github.io/Hugo/p/markdown%E8%AF%AD%E6%B3%95/</link>
        <pubDate>Fri, 23 May 2025 20:12:04 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/markdown%E8%AF%AD%E6%B3%95/</guid>
        <description>&lt;h2 id=&#34;插入公式&#34;&gt;插入公式
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;行内公式 : 输入 &lt;strong&gt;&lt;code&gt;$&lt;/code&gt; + &lt;code&gt;esc&lt;/code&gt;&lt;/strong&gt; ,则可实现 &lt;code&gt;$&lt;/code&gt;  公式  &lt;code&gt;$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;行间公式 : 输入 &lt;strong&gt;&lt;code&gt;$$&lt;/code&gt; + 回车&lt;/strong&gt; ,则可实现  &lt;code&gt;$$&lt;/code&gt; 公式 &lt;code&gt;$$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;图片排版&#34;&gt;图片排版
&lt;/h2&gt;&lt;p&gt;适用html语言
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{&lt;center&gt;  创建中间对齐图片} \\
&amp;\text{\quad \quad &lt;img src = &#34;地址&#34;}   \\
&amp;\text{\quad \quad width =&#34;40\%&#34;&gt; 缩放比率，还有height可选} \\   
&amp;\text{\quad \quad &lt;br&gt;  插入回车，回车之前重复图片操作可并排放图}  \\
&amp;\text{\quad \quad 注释}  \\
&amp;\text{&lt;/center&gt;}
\end{aligned}
$$</description>
        </item>
        <item>
        <title>Git使用</title>
        <link>https://heimi2022.github.io/Hugo/p/git%E4%BD%BF%E7%94%A8/</link>
        <pubDate>Thu, 22 May 2025 17:20:11 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/git%E4%BD%BF%E7%94%A8/</guid>
        <description>&lt;h2 id=&#34;问题解决&#34;&gt;问题解决
&lt;/h2&gt;&lt;h3 id=&#34;git-push&#34;&gt;git push
&lt;/h3&gt;&lt;h4 id=&#34;fatal-unable-to-access-httpsgithubcomgit-could-not-resolve-host-githubcom&#34;&gt;fatal: unable to access ‘https://github.com/&amp;hellip;/.git‘: Could not resolve host: github.com
&lt;/h4&gt;&lt;p&gt;git config &amp;ndash;global &amp;ndash;unset http.proxy
git config &amp;ndash;global &amp;ndash;unset https.proxy&lt;/p&gt;
&lt;p&gt;解决方案：cmd下命令执行 ipconfig/flushdns
清理dns缓存&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Machine-Learning</title>
        <link>https://heimi2022.github.io/Hugo/p/machine-learning/</link>
        <pubDate>Thu, 22 May 2025 16:36:28 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/machine-learning/</guid>
        <description>&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;h3 id=&#34;什么是机器学习&#34;&gt;什么是机器学习
&lt;/h3&gt;&lt;p&gt;定义如下，一个程序被认为能从 &lt;strong&gt;经验 E&lt;/strong&gt; 中学习，解决 &lt;strong&gt;任务 T&lt;/strong&gt;，达到 &lt;strong&gt;性能度量值 P&lt;/strong&gt;，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。&lt;/p&gt;
&lt;p&gt;例如下棋游戏:&lt;br&gt;
E 就是程序上万次的自我练习的经验。&lt;br&gt;
T 就是下棋。&lt;br&gt;
P 就是它在与一些新的对手比赛时，赢得比赛的概率。&lt;/p&gt;
&lt;h3 id=&#34;监督学习&#34;&gt;监督学习
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;监督学习&lt;/strong&gt; 指的就是我们给学习算法一个 &lt;strong&gt;数据集&lt;/strong&gt;。这个数据集由“正确答案”组成。学习算法再根据这个数据集作出预测，算出更多的正确答案。&lt;br&gt;
监督学习的两个例子, &lt;strong&gt;回归问题 与 分类问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;回归问题 : 推测出一个 &lt;strong&gt;连续&lt;/strong&gt; 值的输出结果。&lt;br&gt;
例如 : 卖水果，数据集 3 斤卖 10 元左右，预测 4 斤能卖多少。&lt;/p&gt;
&lt;p&gt;分类问题 : 推测出一组 &lt;strong&gt;离散&lt;/strong&gt; 的结果。&lt;br&gt;
例如 : 识别红绿灯。&lt;/p&gt;
&lt;h3 id=&#34;无监督学习&#34;&gt;无监督学习
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;无监督学习&lt;/strong&gt; 指的是一种学习策略，它交给算法大量的数据，并让算法为我们从数据中找出某种结构。&lt;br&gt;
无监督学习中 &lt;strong&gt;已知数据没有任何的标签&lt;/strong&gt; 是指数据 &lt;strong&gt;有相同的标签或者就是没标签&lt;/strong&gt;。&lt;br&gt;
无监督学习的例子，&lt;strong&gt;聚类算法、鸡尾酒算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;聚类算法 : 将数据分为几类不同的 &lt;strong&gt;簇&lt;/strong&gt;。&lt;br&gt;
例子 : 谷歌新闻，同一主题新闻归为一类; 市场分割。&lt;/p&gt;
&lt;p&gt;鸡尾酒算法 : 分离麦克风接收到的不同的人的声音与环境声音。&lt;/p&gt;
&lt;h2 id=&#34;单变量线性回归linear-regression-with-one-variable&#34;&gt;单变量线性回归(Linear Regression with One Variable)
&lt;/h2&gt;&lt;h3 id=&#34;模型表示&#34;&gt;模型表示
&lt;/h3&gt;&lt;p&gt;样本数目 : 小写 &lt;strong&gt;m&lt;/strong&gt;&lt;br&gt;
训练集 : &lt;strong&gt;Training Set&lt;/strong&gt;&lt;br&gt;
特征/输入变量 : &lt;strong&gt;$x$&lt;/strong&gt;&lt;br&gt;
目标/输出变量 : &lt;strong&gt;$y$&lt;/strong&gt;&lt;br&gt;
训练集中的实例 : &lt;strong&gt;($x$, $y$)&lt;/strong&gt;&lt;br&gt;
第 $i$ 个观察实例 : &lt;strong&gt;($x^{(i)}$, $y^{(i)}$)&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;学习算法的解决方案或函数也称为假设(hypothesis) : h&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;模型表示就是找到将训练集给学习算法找到一个合适的函数 $h$ , $y = h(x)$&lt;/p&gt;
&lt;p&gt;函数 $h_\theta(x) = \theta_0 + \theta_1x$&lt;br&gt;
由于只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。&lt;/p&gt;
&lt;h3 id=&#34;代价函数&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;建模误差: 模型所预测的值与训练集中实际值之间的差距 $h_\theta(x^{(i)}) - y^{(i)}$&lt;/p&gt;
&lt;p&gt;平方误差函数 $J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$&lt;br&gt;
平方误差函数是代价函数之一，对于大多数问题，特别是回归问题，其都是个合理的选择。&lt;br&gt;
为什么是 $\frac{1}{2m}$:&lt;br&gt;
除以 m 是为了消除样本数量 m 对 J 的影响&lt;br&gt;
除以 2 是为了抵消对 J 关于 θ 求偏导数时式子多出的一个 2，使计算更方便&lt;/p&gt;
&lt;p&gt;我们的目标就是找到一个合适的参数 $\theta_0,\theta_1$ 使得代价函数最小。 即 Goal: $\underset{\theta_0,\theta_1}{minimize}J(\theta_0,\theta_1)$&lt;/p&gt;
&lt;h3 id=&#34;梯度下降&#34;&gt;梯度下降
&lt;/h3&gt;&lt;p&gt;梯度下降是一个用来 &lt;strong&gt;求函数最小值&lt;/strong&gt; 的算法，可以用其来求出使代价函数 $J$ 最小的参数 $\theta_0$ 和 $\theta_1$ 的值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;梯度下降的思想&lt;/strong&gt; : 开始时，随机选择一组参数 $(\theta_0,\theta_1)$, 计算代价函数, 然后寻找下一个能让代价函数下降最多的参数组合。持续这种操作，直至找到一个局部最小值。&lt;br&gt;
局部最小值不等同于全局最小值。不同的初始参数会可能会得到完全不同的局部最小值。&lt;/p&gt;
&lt;h4 id=&#34;批量梯度下降算法batch-gradient-descent&#34;&gt;批量梯度下降算法(batch gradient descent)
&lt;/h4&gt;&lt;p&gt;批量 batch 指的是，在梯度下降的每一步中，都用到来所有的训练样本。&lt;/p&gt;
&lt;p&gt;公式为:
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat until convergence（重复直至收敛）} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad (\text{for } j = 0 \text{ and } j = 1) \\
&amp;\}
\end{aligned}
$$&lt;p&gt;其中 $\alpha$ 是学习率，其决定了我们沿代价函数梯度方向向下迈出的幅度有多大。&lt;br&gt;
$\alpha$ 太小，需要很多步才能够达到局部最低点。&lt;br&gt;
$\alpha$ 太大，梯度下降法可能会越过最低点，甚至可能无法收敛，导致发散。&lt;/p&gt;
&lt;p&gt;在梯度下降法中，当参数取值接近局部最低点时，梯度下降法会自动采取更小的幅度，因为代价函数导数会趋向于 0。&lt;/p&gt;
&lt;h2 id=&#34;多变量线性回归&#34;&gt;多变量线性回归
&lt;/h2&gt;&lt;h3 id=&#34;多维特征&#34;&gt;多维特征
&lt;/h3&gt;&lt;p&gt;新的注释：&lt;/p&gt;
&lt;p&gt;$n$ 代表特征的数量&lt;/p&gt;
&lt;p&gt;$x^{(i)}$ 代表第 $i$ 个训练实例，是特征数据的第 $i$ 行，是一个 **向量 **&lt;/p&gt;
&lt;p&gt;$x_j^{(i)}$ 代表特征矩阵中第 $i$ 行第 $j$ 个特征&lt;/p&gt;
&lt;p&gt;多变量的假设 $h$ 表示为：&lt;br&gt;
&lt;/p&gt;
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
$$&lt;p&gt;
此时模型中参数为一个$n+1$维向量&lt;/p&gt;
&lt;p&gt;任何一个训练实例也都是一个$n+1$维向量&lt;/p&gt;
&lt;p&gt;特征矩阵X的维度是$m*(n+1)$&lt;/p&gt;
&lt;p&gt;因此公式可以简化为&lt;/p&gt;
$$
h_\theta(x)=\theta^TX
$$&lt;h3 id=&#34;多变量梯度下降&#34;&gt;多变量梯度下降
&lt;/h3&gt;&lt;p&gt;代价函数为所有建模误差的平方和，即:&lt;br&gt;
&lt;/p&gt;
$$
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
$$&lt;p&gt;多变量线性回归的批量梯度下降算法为：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1,...\theta_n) \\ 
&amp;\}
\end{aligned}
$$&lt;p&gt;即:&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2 \\
&amp;\}
\end{aligned}
$$&lt;p&gt;
求导后得到：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha  \frac{1}{m} \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)}) \cdot x_j^{(i)} \\
&amp;\}
\end{aligned}
$$&lt;h3 id=&#34;梯度下降法&#34;&gt;梯度下降法
&lt;/h3&gt;&lt;h4 id=&#34;特征缩放&#34;&gt;特征缩放
&lt;/h4&gt;&lt;p&gt;使特征具有相近的尺度有助于梯度下降宣发更快的收敛。&lt;/p&gt;
&lt;p&gt;方法：将所有特征的尺度都尽量缩放到-1到1&lt;strong&gt;这种小范围&lt;/strong&gt;内，也不能太小。&lt;/p&gt;
&lt;p&gt;最简单的，令$x_n=\frac{x_n-\mu_n}{s_n}$,其中$\mu_n$是平均值，$s_n$是标准差。&lt;/p&gt;
&lt;h4 id=&#34;学习率&#34;&gt;学习率
&lt;/h4&gt;&lt;p&gt;梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制&lt;strong&gt;迭代次数-代价函数&lt;/strong&gt;的图表来观测算法在何时收敛。&lt;/p&gt;
&lt;p&gt;梯度下降算法的每次迭代受学习率$\alpha $影响：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha$太小，则到达收敛所需的迭代次数很大，但理论上终会收敛。&lt;/li&gt;
&lt;li&gt;$\alpha $太大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;调整学习率时可以采用以下&lt;strong&gt;策略&lt;/strong&gt;：从0.001开始每次上在原基础上乘3倍，&lt;strong&gt;最终选择一个较大的可以使得代价函数收敛的$\alpha $&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如：0.001，0.003，0.01，0.03，0.1&lt;/p&gt;
&lt;h3 id=&#34;特征选择与多项式回归&#34;&gt;特征选择与多项式回归
&lt;/h3&gt;&lt;h4 id=&#34;特征选择&#34;&gt;特征选择
&lt;/h4&gt;&lt;p&gt;建立模型时，可以&lt;strong&gt;根据需要选择适合模型的特征&lt;/strong&gt;而不局限于数据集中的特征。比如：数据集给出房子的长和宽，预测房价时，可以创造一个特征为 &lt;strong&gt;面积=长*宽&lt;/strong&gt; 作为模型的新特征。&lt;/p&gt;
&lt;h4 id=&#34;多项式回归&#34;&gt;多项式回归
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;线性回归并不适合所有数据&lt;/strong&gt;，有时需要曲线来适应数据，比如：&lt;/p&gt;
&lt;p&gt;三次方模型$h_\theta(x)=\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$&lt;/p&gt;
&lt;p&gt;使用多项式回归模型，特征缩放很有必要。&lt;/p&gt;
&lt;h3 id=&#34;正规方程&#34;&gt;正规方程
&lt;/h3&gt;&lt;p&gt;当特征数量小于10000时（对于目前的计算机），通常采用正规方程的方法找到一组最优参数$\theta$。&lt;/p&gt;
&lt;p&gt;正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial \theta_j} J(\theta_j)=0$，&lt;/p&gt;
&lt;p&gt;其&lt;strong&gt;解&lt;/strong&gt;为$\theta=(X^TX)^{-1}X^Ty$。$X$为训练集特征矩阵，$y$为训练集结果。&lt;/p&gt;
&lt;h4 id=&#34;梯度下降于正规方程的比较&#34;&gt;梯度下降于正规方程的比较
&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;梯度下降&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;正规方程&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;需要选择学习率$\alpha $&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;不需要&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;需要多次迭代&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一次运算得出&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;特征数量n大时也可用&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;矩阵逆的计算时间复杂度为$O(n^3)$。因此n小于10000时，可以接受&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;使用于各种类型&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;只适用于线性模型。不适合逻辑回归等其它模型。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;以下情况时，正规方程不能用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有两个特征线性相关。  解决：使用正规方程前，剔除相关特征。&lt;/li&gt;
&lt;li&gt;含有大量特征，进而出现$m \leqslant n$时。  解决：剔除特征，用较少特征反应尽可能多的内容。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;逻辑回归&#34;&gt;逻辑回归
&lt;/h2&gt;&lt;p&gt;逻辑回归算法是一种分类算法。&lt;/p&gt;
&lt;p&gt;分类问题预测的变量$y$为&lt;strong&gt;离散值&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;假说表示&#34;&gt;假说表示
&lt;/h3&gt;&lt;p&gt;引入一个&lt;strong&gt;逻辑函数Sigmoid function&lt;/strong&gt;，S形函数，公式为$g(z)=\frac{1}{1+e^{-z}}$。&lt;/p&gt;
&lt;p&gt;令逻辑回归模型的假设是：$h_\theta(x)=g(\theta^T X)$,$X$代表特征向量，$g$代表逻辑函数。&lt;/p&gt;
&lt;p&gt;此时模型的输出变量范围始终在0和1之间。&lt;/p&gt;
&lt;p&gt;$h_\theta(x)$表示，对于给定的输入变量，根据选择的参数，计算输出变量为1的可能性，即：
&lt;/p&gt;
$$
h_\theta(x)=P(y=1|x;\theta)
$$&lt;h3 id=&#34;决策边界&#34;&gt;决策边界
&lt;/h3&gt;&lt;p&gt;在逻辑回归中，假设：&lt;br&gt;
&lt;/p&gt;
$$
y = \begin{cases}
1, &amp; \text{if } h_\theta(x) \geq 0.5 \\
0, &amp; \text{if } h_\theta(x) &lt; 0.5
\end{cases}
$$&lt;p&gt;因此当$\theta^T x \geqslant 0$时，预测$y=1$，当$\theta^T x &amp;lt; 0$时，预测$y=0$。&lt;/p&gt;
&lt;p&gt;因此对于一组参数$\theta$，有$\theta^T x=0$，这条&lt;strong&gt;曲线&lt;/strong&gt;即为模型的决策边界。&lt;/p&gt;
&lt;h3 id=&#34;代价函数-1&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;为什么逻辑回归的代价函数和线性回归的不同？由于sigmoid函数的非线性，导致逻辑回归的假设函数带入代价函数时，会导致代价函数有很多局部最小值，其是一个非凸函数。因此需对代价函数进行修正。&lt;/p&gt;
&lt;p&gt;定义逻辑回归的代价函数$J(\theta)=\frac{1}{m} \sum^m_{i=1} Cost(h_\theta(x^{(i)}),y^{(i)})$，其中
&lt;/p&gt;
$$
Cost(h_\theta(x^{(i)}),y^{(i)}) = \begin{cases}
   -log( h_\theta(x)) \quad &amp;  if\quad y=1, &amp;  \\
   -log( 1- h_\theta(x)) &amp;  if\quad y=0, &amp;  \\
\end{cases}
\overset{简化}{=} -ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))
$$&lt;h4 id=&#34;逻辑回归中cost函数特点&#34;&gt;逻辑回归中Cost函数特点
&lt;/h4&gt;&lt;p&gt;当实际的$y=1$且$h_\theta(x)=1$时，误差为0，当$y=1$但$h_\theta(x)$不为1时，误差随着$h_\theta(x)$的变小而变大。当$h_\theta(x)$减小为0，表示预测与实际完全相反，其误差为无穷大，这时代价函数将会受到很大的惩罚；当实际的$y=0$且$h_\theta(x)=0$时，误差为0，当$y=0$但$h_\theta(x)$不为0时，误差随着$h_\theta(x)$的变大而变大。&lt;/p&gt;
&lt;h4 id=&#34;简化后的代价函数&#34;&gt;简化后的代价函数
&lt;/h4&gt;$$
\begin{aligned}
J(\theta)&amp; = \frac{1}{m} \sum^m_{i=1} Cost(h_\theta(x^{(i)}),y^{(i)}) &amp; \\
&amp; = -\frac{1}{m} [\sum^m_{i=1}-ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))] &amp;\\
\end{aligned}
$$&lt;p&gt;最小化代价函数的方法&lt;strong&gt;是梯度下降法&lt;/strong&gt;：
&lt;/p&gt;
$$
\begin{aligned}
\text{Repeat}&amp; \{ \\
&amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp;\}
\end{aligned}
$$&lt;p&gt;虽然其形式上与线性回归的梯度下降法相同，但由于回归的模型假设发生了变化，两者是完全不同的函数。&lt;/p&gt;
&lt;h3 id=&#34;高级优化&#34;&gt;高级优化
&lt;/h3&gt;&lt;p&gt;优化算法：共轭梯度法BFGS（变尺度法）和L-BFGS（限制遍尺度法）。&lt;/p&gt;
&lt;p&gt;优点：不需要手动选择学习率。&lt;/p&gt;
&lt;h3 id=&#34;多类别分类&#34;&gt;多类别分类
&lt;/h3&gt;&lt;p&gt;假设有n个类别，则将其分为&lt;strong&gt;n个&lt;/strong&gt;1对（n-1）的&lt;strong&gt;二元分类问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;记每一个分类问题的模型为：$h_\theta ^{(i)} (x)= p(y=i|x;\theta)$，其中$i=(1,2,3,&amp;hellip;,k)$。&lt;/p&gt;
&lt;p&gt;最终输出结果为让$h_\theta ^{(i)} (x)$最大的i，即$\mathop{max}\limits_{i} ; h_\theta ^{(i)} (x)$。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Hugo使用</title>
        <link>https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/</link>
        <pubDate>Thu, 22 May 2025 15:53:57 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/</guid>
        <description>&lt;img src="https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/cover.png" alt="Featured image of post Hugo使用" /&gt;&lt;h2 id=&#34;启动-hugo-服务&#34;&gt;启动 Hugo 服务
&lt;/h2&gt;&lt;p&gt;hugo server -D&lt;/p&gt;
&lt;h2 id=&#34;创建文章&#34;&gt;创建文章
&lt;/h2&gt;&lt;h3 id=&#34;命令创建&#34;&gt;命令创建
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;命令 : hugo new post/foldername/*.md 
其模板来自 archetypes/default.md
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;手动创建&#34;&gt;手动创建
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;在post目录下，手动创建一个文件夹，在文件夹里创建md文件，并将需要的图片资源放入其中。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;文章推送&#34;&gt;文章推送
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;git add .&lt;/li&gt;
&lt;li&gt;git commit -m&amp;quot;update&amp;quot;&lt;/li&gt;
&lt;li&gt;git push&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>To_YSM</title>
        <link>https://heimi2022.github.io/Hugo/p/to_ysm/</link>
        <pubDate>Tue, 20 May 2025 21:27:55 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/to_ysm/</guid>
        <description>&lt;h1 id=&#34;我的第一条博客送给杨思敏同学&#34;&gt;我的第一条博客送给杨思敏同学
&lt;/h1&gt;</description>
        </item>
        
    </channel>
</rss>

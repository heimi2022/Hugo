<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>WJJ</title>
        <link>https://heimi2022.github.io/Hugo/</link>
        <description>Recent content on WJJ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Wjj</copyright>
        <lastBuildDate>Fri, 11 Jul 2025 15:00:00 +0800</lastBuildDate><atom:link href="https://heimi2022.github.io/Hugo/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Linux</title>
        <link>https://heimi2022.github.io/Hugo/p/linux/</link>
        <pubDate>Fri, 11 Jul 2025 15:00:00 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/linux/</guid>
        <description>&lt;h2 id=&#34;shell命令&#34;&gt;shell命令
&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;1. ls : 目录信息查看
   1. ls -a : 显示目录所有文件及文件夹，包括隐藏文件
   2. ls -l : 所有文件信息用单列格式输出
   3. ll = ls -l
2. cd : 目录切换
   1. cd / 进入根目录
   2. cd - 进入用户目录
   3. cd ../ 返回上一级目录
3. pwd : 显示当前路径
4. uname : 系统信息查看
5. clear : 清屏
6. cat : 显示文件内容
7. sudo : 切换用户身份
8. su : 切换用户
9. cp : 文件拷贝
10. touch : 创建文件
11. cp : 拷贝文件
12. rm : 删除文件
    1.  rm -f 删除只读
    2.  rm -r 删除目录
13. rmdir : 删除目录
14. mv : 移动文件(重命名)
15. ifconfig : 显示网络信息 
16. reboot : 重启系统
17. poweroff : 关机
18. man : 系统帮助
19. sync : 数据同步写入磁盘
20. find : 查找文件
    1.  find -name 文件 (./表示当前目录)
21. grep : 查找内容
    1.  grep -nr
    2.  grep -ir 忽略大小写
    3.  grep -nir
22. du : 查看文件夹大小
    1.  du -sh 加单位
23. df : 磁盘空间检测
24. gedit : 打开某个文件
25. ps : 查看当前系统进程
26. top : 进程实时运行状态查看
27. file : 文件类型查看
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;ubuntu-软件安装&#34;&gt;Ubuntu 软件安装
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;sudo apt-get install&lt;/li&gt;
&lt;li&gt;sudo dpkg -i xxx.deb&lt;/li&gt;
&lt;li&gt;自己下载源码安装
&lt;ol&gt;
&lt;li&gt;make&lt;/li&gt;
&lt;li&gt;make install&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;appstore&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;ubuntu-文件系统结构&#34;&gt;Ubuntu 文件系统结构
&lt;/h2&gt;&lt;h3 id=&#34;根目录&#34;&gt;根目录
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;&amp;quot;/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;文件系统&#34;&gt;文件系统
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;/bin 可执行文件&lt;/li&gt;
&lt;li&gt;/boot 启动&lt;/li&gt;
&lt;li&gt;/dev 设备驱动文件&lt;/li&gt;
&lt;li&gt;/etc 配置文件&lt;/li&gt;
&lt;li&gt;/home 用户文件夹&lt;/li&gt;
&lt;li&gt;/lib lib64 库文件&lt;/li&gt;
&lt;li&gt;/media 可插拔设备&lt;/li&gt;
&lt;li&gt;/proc 系统运行文件、运行信息&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;绝对路径和相对路径&#34;&gt;绝对路径和相对路径
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;绝对路径 : 从根目录&amp;quot;/&amp;ldquo;算起的路径&lt;/li&gt;
&lt;li&gt;相对路径 : 不是以&amp;rdquo;/&amp;quot; 开头的路径
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;.&amp;rdquo; &amp;ldquo;./&amp;ldquo;代表当前路径&lt;/li&gt;
&lt;li&gt;&amp;ldquo;..&amp;rdquo; &amp;ldquo;../&amp;ldquo;代表上一层目录&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;磁盘管理&#34;&gt;磁盘管理
&lt;/h2&gt;&lt;h3 id=&#34;磁盘文件&#34;&gt;磁盘文件
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;/dev/sd* 磁盘设备文件，不能直接访问磁盘，必须先挂载到某一目录&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;磁盘和目录容量查询&#34;&gt;磁盘和目录容量查询
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;df 主要查看文件系统使用量&lt;/li&gt;
&lt;li&gt;du 主要查看单个文件大小
&lt;ol&gt;
&lt;li&gt;du -h &amp;ndash;max-depth=1(查看目录后1层)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;磁盘挂载卸载分区格式化&#34;&gt;磁盘挂载、卸载、分区、格式化
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;挂载、卸载
&lt;ol&gt;
&lt;li&gt;sudo umount /media/heimi/udisk&lt;/li&gt;
&lt;li&gt;sudo mount -o iocharset=utf8 /dev/sdb1 /media/heimi/udisk/&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;分区
fdisk
&lt;ol&gt;
&lt;li&gt;sudo fdisk /dev/sdb&lt;/li&gt;
&lt;li&gt;sudo fdisk -l 查看分区大小&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;格式化
&lt;ol&gt;
&lt;li&gt;磁盘分区创建好后可以格式化磁盘 mkfs
&lt;ol&gt;
&lt;li&gt;mkfs -t vfta /dev/sdx&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;压缩-与-解压缩&#34;&gt;压缩 与 解压缩
&lt;/h2&gt;&lt;p&gt;linux 常用压缩格式 : .tar  .tar.bz2  .tar.gz&lt;/p&gt;
&lt;h3 id=&#34;gzip&#34;&gt;gzip
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;gzip xxx  压缩&lt;/li&gt;
&lt;li&gt;gzip -d 解压缩&lt;/li&gt;
&lt;li&gt;gzip -r 对文件夹里的文件进行压缩但不打包&lt;/li&gt;
&lt;li&gt;gzip -rd 对文件夹里的文件进行解压缩&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;bzip2&#34;&gt;bzip2
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;bzip2 -z 压缩文件&lt;/li&gt;
&lt;li&gt;bzip2 -d 解压缩文件&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;tar-打包工具&#34;&gt;tar 打包工具
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;参数
&lt;ol&gt;
&lt;li&gt;-c 创建新归档,创建压缩文件&lt;/li&gt;
&lt;li&gt;-x 从归档中解出文件，解压缩&lt;/li&gt;
&lt;li&gt;-j 使用bzip 压缩格式&lt;/li&gt;
&lt;li&gt;-z 使用gzip&lt;/li&gt;
&lt;li&gt;-v 打印出命令执行过程&lt;/li&gt;
&lt;li&gt;-f 使用归档文件&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;bzip2
&lt;ol&gt;
&lt;li&gt;vcjf 压缩
&lt;ol&gt;
&lt;li&gt;tar -vcjf xxx.tar.bz2 xxx&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;vxjf 解压缩&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;gzip
&lt;ol&gt;
&lt;li&gt;vczf 压缩
&lt;ol&gt;
&lt;li&gt;tar -vczf xxx.tar.gz xxx&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;vxzf 解压缩
&lt;ol&gt;
&lt;li&gt;tar -vxzf xxx.tar.gz xxx&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;rar&#34;&gt;rar
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;rar a xxx.rar xxx 压缩&lt;/li&gt;
&lt;li&gt;rar x 解压缩&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;zip&#34;&gt;zip
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;unzip 解压缩&lt;/li&gt;
&lt;li&gt;zip -rv 压缩&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;文件权限&#34;&gt;文件权限
&lt;/h2&gt;&lt;p&gt;r=4  w=2  x=1
读 写 执行&lt;/p&gt;
&lt;h3 id=&#34;-rw-rw-r&#34;&gt;-rw-rw-r&amp;ndash;
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;用户 - 用户组 - 其他用户&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;用户 :  能读写 不能执行&lt;/li&gt;
&lt;li&gt;用户组 : 能读写 不能执行&lt;/li&gt;
&lt;li&gt;其他用户 : 只能读&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;lrw-rw-rwx&#34;&gt;lrw-rw-rwx
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;l 表示 该文件是个链接文件
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;修改权限&#34;&gt;修改权限
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;chmod 777(权限全开) xxx  修改文件权限&lt;/li&gt;
&lt;li&gt;chown             修改文件所属用户
&lt;ol&gt;
&lt;li&gt;sudo chown heimi.heimi hello 修改用户和用户组&lt;/li&gt;
&lt;li&gt;sudo chown .root hello 修改用户组&lt;/li&gt;
&lt;li&gt;sudo chown root hello 修改用户&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;linux-连接文件&#34;&gt;linux 连接文件
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;软连接 ln -s 快捷方式
&lt;ol&gt;
&lt;li&gt;要使用绝对路径&lt;/li&gt;
&lt;li&gt;cp 默认复制，如果为软链接，会将源文件一同复制过去&lt;/li&gt;
&lt;li&gt;cp -d，代表复制时在源文件中除了links不复制其他符号链接&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;硬连接 ln  inode 产生新的文件名
防止文件被删除
其中一个文件被修改 其他也会被改
ll -i xx* 打印文件节点信息&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;vim-编辑器&#34;&gt;vim 编辑器
&lt;/h2&gt;&lt;h3 id=&#34;vi&#34;&gt;vi
&lt;/h3&gt;&lt;p&gt;退出 vi ESC + Shitf + :  wq&lt;/p&gt;
&lt;h3 id=&#34;vim模式&#34;&gt;vim模式
&lt;/h3&gt;&lt;h4 id=&#34;输入模式&#34;&gt;输入模式
&lt;/h4&gt;&lt;p&gt;i 在当前光标所在字符的前面，转为输入模式。
I 在当前光标所在行的行首转换为输入模式。
a 在当前光标所在字符的后面，转为输入模式。
A 在光标所在行的行尾，转换为输入模式。
o 在当前光标所在行的下方，新建一行，并转为输入模式。
O 在当前光标所在行的上方，新建一行，并转为输入模式。
s 删除光标所在字符。
r 替换光标处字符。&lt;/p&gt;
&lt;h4 id=&#34;底行模式&#34;&gt;底行模式
&lt;/h4&gt;&lt;p&gt;x 保存当前文档并且退出
q 退出。
w 保存文档。
q! 退出 VI/VIM，不保存文档&lt;/p&gt;
&lt;h4 id=&#34;指令模式&#34;&gt;指令模式
&lt;/h4&gt;&lt;p&gt;1、移动光标指令：
h(或左方向键) 光标左移一个字符。
l(或右方向键) 光标右移一个字符。
j(或下方向键) 光标下移一行。
k(或上方向键) 光标上移一行。
nG 光标移动到第 n 行首。
n+ 光标下移 n 行。
n- 光标上移 n 行。
2、屏幕翻滚指令
Ctrl+f 屏幕向下翻一页，相当于下一页。
Ctrl+b 屏幕向上翻一页，相当于上一页。
3、复制、删除和粘贴指令
cc 删除整行，并且修改整行内容。
dd 删除该行，不提供修改功能。
ndd 删除当前行向下 n 行。
x 删除光标所在的字符。
X 删除光标前面的一个字符。
nyy 复制当前行及其下面 n 行。
p 粘贴最近复制的内容。&lt;/p&gt;
&lt;h2 id=&#34;linux-c&#34;&gt;linux c
&lt;/h2&gt;&lt;h3 id=&#34;gcc&#34;&gt;gcc
&lt;/h3&gt;&lt;p&gt;-c   编译和汇编, 不链接   生成.o
-o   output 生成可执行文件&lt;/p&gt;
&lt;h3 id=&#34;makefile&#34;&gt;makefile
&lt;/h3&gt;&lt;h4 id=&#34;命令&#34;&gt;命令
&lt;/h4&gt;&lt;p&gt;make
make clean
行首不能用空格&lt;/p&gt;
&lt;h4 id=&#34;语法&#34;&gt;语法
&lt;/h4&gt;&lt;p&gt;目标 : 依赖文件集合
命令1
命令2
&amp;hellip;&lt;/p&gt;
&lt;p&gt;最主要目标放第一条规则&lt;/p&gt;
&lt;h4 id=&#34;变量&#34;&gt;变量
&lt;/h4&gt;&lt;p&gt;makeflie变量类似c语言宏定义&lt;/p&gt;
&lt;p&gt;变量引用 $(变量)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;=&amp;rdquo;  变量类似指针&lt;/li&gt;
&lt;li&gt;&amp;ldquo;:=&amp;rdquo; 变量类似c语言等号&lt;/li&gt;
&lt;li&gt;&amp;ldquo;?=&amp;rdquo; 如果变量没被赋值 ,则赋值变量&lt;/li&gt;
&lt;li&gt;&amp;ldquo;+=&amp;rdquo; 变量追加  字符串后拼接字符串&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;ldquo;%&amp;rdquo; 通配符&lt;/p&gt;
&lt;p&gt;自动化变量&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$@规则中的目标集合，在模式规则中，如果有多个目标的话，“$@”表示匹配模
式中定义的目标集合。&lt;/li&gt;
&lt;li&gt;$&amp;lt; 依赖文件集合中的第一个文件 如果依赖文件是以模式(即“%”)定义的，那么
“$&amp;lt;”就是符合模式的一系列的文件集合。&lt;/li&gt;
&lt;li&gt;$^所有依赖文件的集合，使用空格分开，如果在依赖文件中有多个重复的文件，
“$^”会去除重复的依赖文件，值保留一份。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;伪目标创建&#34;&gt;伪目标创建
&lt;/h4&gt;&lt;p&gt;.PHONY 伪目标&lt;/p&gt;
&lt;h4 id=&#34;函数&#34;&gt;函数
&lt;/h4&gt;&lt;h4 id=&#34;例&#34;&gt;例
&lt;/h4&gt;&lt;p&gt;object = main.o input.o caclu.o
main: $(object)
gcc -o main $(object)&lt;/p&gt;
&lt;p&gt;main.o: main.c
gcc -c main.c&lt;/p&gt;
&lt;p&gt;input.o: input.c
gcc -c input.c&lt;/p&gt;
&lt;p&gt;caclu.o: caclu.c
gcc -c caclu.c&lt;/p&gt;
&lt;p&gt;clean:
rm *.o
rm main&lt;/p&gt;
&lt;h4 id=&#34;例-1&#34;&gt;例
&lt;/h4&gt;&lt;p&gt;1 object = main.o input.o caclu.o
2 main: $(object)
3     gcc -o main $(object)
4
5 %.o: %.c
6     gcc -c $&amp;lt;
7
8 clean:
9     rm *.o
10     rm main&lt;/p&gt;
&lt;h2 id=&#34;环境变量&#34;&gt;环境变量
&lt;/h2&gt;&lt;p&gt;echo $PATH 查看环境变量&lt;/p&gt;
&lt;p&gt;export PATH=命令行路径:$PATH  添加环境变量&lt;/p&gt;
&lt;h2 id=&#34;shell-脚本&#34;&gt;shell 脚本
&lt;/h2&gt;&lt;p&gt;.sh
第一行 #!/bin/bash&lt;/p&gt;
&lt;h3 id=&#34;语法-1&#34;&gt;语法
&lt;/h3&gt;&lt;p&gt;echo 在终端上显示字符串]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;cmd1 &amp;amp;&amp;amp; cmd2 : cmd1执行完且执行正确再开始执行cmd2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cmd1 || cmd2 : cmd1执行完且执行正确则cmd2不执行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;read 输入&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;read -p 输出提示符&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数值计算 $(表达式) 变量=$(表达式) &amp;ldquo;=&amp;ldquo;两边不能有空格&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;test 文件,数值, 字符&lt;br&gt;
[test]:(&lt;a class=&#34;link&#34; href=&#34;https://www.runoob.com/linux/linux-shell-test.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.runoob.com/linux/linux-shell-test.html&lt;/a&gt;)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;文件测试
&lt;ol&gt;
&lt;li&gt;test -e&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;中括号[]&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;只能输 == 或 !=&lt;/li&gt;
&lt;li&gt;[&amp;quot;$input1&amp;rdquo; == &amp;ldquo;$input2&amp;rdquo;] 需要加双引号&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;默认参数&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$# 表示最后参数的编号&lt;/li&gt;
&lt;li&gt;$@ 输出各个参数&lt;/li&gt;
&lt;li&gt;$0-$n shell脚本参数  $0为shell本身&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;条件判断
exit 0 退出条件
1.
if 条件判断 ; then
&amp;hellip;
elif 条件判断 ; then
&amp;hellip;
else
&amp;hellip;
fi
2.
case $变量 in
&amp;ldquo;变量值&amp;rdquo;)
程序段
;; //表示该程序块结束
&amp;ldquo;变量值&amp;rdquo;)
程序段
;;
*)  //其他
程序段
esac&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;脚本函数
function fname()
{
函数代码段
}
10 .循环
while [条件]
do
循环代码段
done&lt;/p&gt;
&lt;p&gt;for var in con1 con2 con3
do
&amp;hellip;
done&lt;/p&gt;
&lt;p&gt;for((初始值;限制值;执行步长))
do
&amp;hellip;
done&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;linux-alias命令查看和设定别名&#34;&gt;linux alias命令查看和设定别名
&lt;/h2&gt;&lt;h3 id=&#34;alias&#34;&gt;alias
&lt;/h3&gt;&lt;p&gt;:查看系统中所有的命令别名&lt;/p&gt;
&lt;h3 id=&#34;设定别名&#34;&gt;设定别名
&lt;/h3&gt;&lt;p&gt;alias 别名=&amp;lsquo;原命令&amp;rsquo;&lt;/p&gt;
&lt;h3 id=&#34;删除别名&#34;&gt;删除别名
&lt;/h3&gt;&lt;p&gt;unalias 别名&lt;/p&gt;
&lt;h3 id=&#34;使别名永久生效&#34;&gt;使别名永久生效
&lt;/h3&gt;&lt;p&gt;vi  ~/.bashrc  写入这个文件中即可永久生效   编辑完之后记得使环境变量生效: source .bashrc&lt;/p&gt;
&lt;h2 id=&#34;pip-换源&#34;&gt;pip 换源
&lt;/h2&gt;&lt;p&gt;pip install [需要安装的库] -i &lt;a class=&#34;link&#34; href=&#34;https://pypi.tuna.tsinghua.edu.cn/simple&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://pypi.tuna.tsinghua.edu.cn/simple&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;重启网络服务&#34;&gt;重启网络服务
&lt;/h2&gt;&lt;p&gt;service network-manager restart&lt;/p&gt;
&lt;h2 id=&#34;利用wget命令即可从网页下载下面的例子是将网页上的该文件下载到当前路径下&#34;&gt;利用wget命令即可从网页下载，下面的例子是将网页上的该文件下载到当前路径下
&lt;/h2&gt;&lt;p&gt;wget &lt;a class=&#34;link&#34; href=&#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://download.pytorch.org/models/resnet18-5c106cde.pth&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Big_Model_System</title>
        <link>https://heimi2022.github.io/Hugo/p/big_model_system/</link>
        <pubDate>Mon, 07 Jul 2025 16:00:00 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/big_model_system/</guid>
        <description>&lt;h2 id=&#34;自然语言处理nlp基础&#34;&gt;自然语言处理（NLP）基础
&lt;/h2&gt;&lt;h3 id=&#34;基础与应用&#34;&gt;基础与应用
&lt;/h3&gt;&lt;p&gt;自然语言处理是让计算机来理解人类所说的语言，然后像人一样去交互，对话，生成自然语言。&lt;/p&gt;
&lt;p&gt;自然语言处理的基础任务：词性标注、命名实体的识别、共指消解（代词与实体之间的连接）、依赖关系（语法等）；中文的自动分词（将中文的每个词区分出来，像英文一样）；机器翻译；情感分类；意见挖掘&lt;/p&gt;
&lt;h3 id=&#34;词表示&#34;&gt;词表示
&lt;/h3&gt;&lt;p&gt;词表示：把词转换为机器所能理解的意思&lt;/p&gt;
&lt;p&gt;词表示的目标：词相似度的计算；发现词与词之间的语义的关系&lt;/p&gt;
&lt;p&gt;word Embedding：&lt;strong&gt;分布式表示&lt;/strong&gt;，建立一个低维的一个稠密的向量空间，然后把每一个词都学到这个空间里，用空间里某个位置所对应的向量来表示词。代表方法：word2vec。&lt;/p&gt;
&lt;h3 id=&#34;语言模型&#34;&gt;语言模型
&lt;/h3&gt;&lt;p&gt;语言模型：根据前文预测下一个词是什么。&lt;/p&gt;
&lt;h4 id=&#34;语言模型的工作&#34;&gt;语言模型的工作
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;完成计算一个序列的词成为一句话的概率（joint probability）&lt;/li&gt;
&lt;/ol&gt;
$$
P(W)=P(w_1,w_2,\cdots,w_n)
$$&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;根据前文预测下一个词（Conditional Probability）&lt;/li&gt;
&lt;/ol&gt;
$$
P(w_n|w_1,w_2,\cdots,w_{n-1})
$$&lt;h4 id=&#34;语言模型的假设&#34;&gt;语言模型的假设
&lt;/h4&gt;&lt;p&gt;即将出现的词只受它之前出现词的影响。因此语言模型可表示为：
&lt;/p&gt;
$$
P(w_1,w_2,\cdots,w_n)=\prod_ip(w_i|w_1,w_2,\cdots,w_{i-1})
$$&lt;h4 id=&#34;n-gram-model&#34;&gt;N-gram Model
&lt;/h4&gt;&lt;p&gt;模型的搭建：统计前面出现了几个词之后后面出现的那个词的频度是什么样的。&lt;/p&gt;
&lt;p&gt;例如4-gram：
&lt;/p&gt;
$$
p(w_j|too \, late \, to)=\frac{count(too \, late \, to \, w_j)}{count(too \, late \, to)}
$$&lt;p&gt;
其满足Markov assumption：
&lt;/p&gt;
$$
P(w_1,w_2,\cdots,w_n) \approx \prod_i p(w_i|w_1,w_2,\cdots,w_{i-1})
$$&lt;p&gt;
并且有：
&lt;/p&gt;
$$
p(w_i|w_1,w_2,\cdots,w_{i-1}) \approx p(w_i|w_{i-k},\cdots,w_{i-1})
$$&lt;p&gt;
问题：不能发现句子间的相似度。类似用独热码来表示词。&lt;/p&gt;
&lt;h4 id=&#34;neural-language-model&#34;&gt;Neural Language Model
&lt;/h4&gt;&lt;p&gt;用分布式表示词，将词表示为一个低维向量，在把低维向量拼在一起，形成一个更高维的上下文向量，再进行非线性转换，就可以用其预测下一个词。类似神经网络的过程，其可基于神经网络可调参数来学习上下文间的向量的关系。&lt;/p&gt;
&lt;h2 id=&#34;神经网络&#34;&gt;神经网络
&lt;/h2&gt;&lt;h3 id=&#34;神经网络基础&#34;&gt;神经网络基础
&lt;/h3&gt;&lt;p&gt;为什么要有激活函数？若不存在激活函数，则多层神经网络都为线性运算，其最终可以被转化为一个单层的神经网络。也就是说，在每一激活函数的情况下，多层神经网络和单层神经网络表达能力是一致的。举例如下：
&lt;/p&gt;
$$
h_1=W_1 x + b_1,h_2=W_2 h_1 + b_2  \quad -&gt; \quad h_2=W_2 W_1 x + W_2 b_1 + b_2
$$&lt;h3 id=&#34;word2vec&#34;&gt;Word2vec
&lt;/h3&gt;&lt;p&gt;word2vec有两类模型：Continuous bag-of-words(CBOW)以及continuous skip-gram。&lt;/p&gt;
&lt;p&gt;word2vec用滑动窗口的方式构造训练数据，一个滑动窗口是一个文本中连续出现的几个单词，在窗口中间的词叫做target，其它叫做context。&lt;/p&gt;
&lt;h4 id=&#34;cbow&#34;&gt;CBOW
&lt;/h4&gt;&lt;p&gt;CBOW：根据context推测target。其不考虑context词的次序。&lt;/p&gt;
&lt;p&gt;下图是CBOW的模型，其是一个多分类问题。类别数为词表大小。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/Big_Model_CBOW&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708175947075&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;continuous-skip-gram&#34;&gt;continuous skip-gram
&lt;/h4&gt;&lt;p&gt;continuous skip-gram：根据target推出context。其一次预测多个context时，先将问题进行分解，即一个一个预测context。&lt;/p&gt;
&lt;p&gt;下图是continuous skip-gram的模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/Big_Model_continuous_skip_gram&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708180328724&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上述模型中，若词表非常大，则训练效率很慢。采用负采样来提高训练效率。&lt;/p&gt;
&lt;p&gt;负采样：不把整个词表作为负例，只选几个词表中不是需要predict的词来作为负例来提高计算效率。&lt;/p&gt;
&lt;h4 id=&#34;sub-sampling&#34;&gt;Sub-Sampling
&lt;/h4&gt;&lt;p&gt;Sub-Sampling：平衡常见词和罕见词出现的概率&lt;/p&gt;
&lt;p&gt;常见词，可能包含的语义比较少，如&amp;quot;的&amp;quot;等，需在训练时去掉，下面是词被去掉的概率
&lt;/p&gt;
$$
p=1- \sqrt{\frac{t}{f(w)}}
$$&lt;p&gt;
f(w)为一个词出现的频度，t是可自己定义的值。&lt;/p&gt;
&lt;h3 id=&#34;循环神经网络rnn&#34;&gt;循环神经网络RNN
&lt;/h3&gt;&lt;p&gt;RNN:Recurrent Neural Network.&lt;/p&gt;
&lt;p&gt;其处理序列数据时，会进行顺序记忆。&lt;/p&gt;
&lt;p&gt;下面是一个常见的模型结构&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250708183236474.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708183236474&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;rnn单元&#34;&gt;RNN单元
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://heimi2022.github.io/Hugo/assets/image-20250708183317584.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20250708183317584&#34;
	
	
&gt;
&lt;/p&gt;
$$
h_i = tanh(W_x x_i + W_h h_{i-1} + b) \\
y_i =F(h_i)
$$&lt;p&gt;
RNN模型是每一个RNN单元的复制，其参数是一样的，有利于实现参数共享，使得模型能够泛化到不同长度的样本，节省参数量&lt;/p&gt;
&lt;h4 id=&#34;rnn的应用与问题&#34;&gt;RNN的应用与问题
&lt;/h4&gt;&lt;p&gt;应用场景：序列标注，序列预测，图片描述，文本分类。&lt;/p&gt;
&lt;p&gt;问题：容易产生梯度消失、梯度爆炸。&lt;/p&gt;
&lt;p&gt;进行反向传播时，有：
&lt;/p&gt;
$$
h_i = \tanh(W_x x_i + W_h h_{i-1} + b) \\
\Delta w_1 = \frac{\partial Loss}{\partial w_2} = \frac{\partial Loss}{\partial h_n} \cdot \frac{\partial h_n}{\partial h_{n-1}} \cdot \frac{\partial h_{n-1}}{\partial h_{n-2}} \cdot \; \cdots \; \cdot \frac{\partial h_3}{\partial h_2} \cdot \frac{\partial h_2}{\partial w_2} (反向传播时，链式法则展开)
$$&lt;p&gt;
根据$\frac{\partial h_n}{\partial h_{n-1}}$ 讨论。当 $\frac{\partial h_n}{\partial h_{n-1}} &amp;gt; 1$  ，随着网络层数增多，梯度会像滚雪球一样 &lt;strong&gt;指数级增大&lt;/strong&gt; 。极端情况下，梯度过大可能让参数更新变得异常剧烈，模型参数值飙升，训练过程难以稳定，甚至无法收敛，这就是&lt;strong&gt;梯度爆炸&lt;/strong&gt;现象。   当 $\frac{\partial h_n}{\partial h_{n-1}} &amp;lt; 1$ ，随着网络层数不断加深，梯度会 &lt;strong&gt;指数级衰减&lt;/strong&gt; 。传到前面层（比如靠近输入的层 ）时，梯度会变得极其微小，几乎接近 0 。这会导致这些层的参数更新停滞，模型很难学到深层有意义的特征，训练效果大打折扣，就是&lt;strong&gt;梯度消失问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;RNN单元的变体：GRU、LSTM。核心：计算时，保存周围的记忆，来捕捉远距离的依赖性。&lt;/p&gt;
&lt;h4 id=&#34;门控循环单元gru&#34;&gt;门控循环单元（GRU）
&lt;/h4&gt;&lt;p&gt;GRU:Gated Recurrent Unit&lt;/p&gt;
&lt;p&gt;GRU是RNN的一个变体，其包括更新门以及重置门。&lt;/p&gt;
&lt;p&gt;重置门：考虑到上一层的隐藏状态对当前激活，可通过计算获得一个新的临时的激活。当重置门$r_i \approx 0$时，新的激活值和上一状态几乎没有关系。
&lt;/p&gt;
$$
r_i=\sigma(W_x^{(r)}x_i + W_h^{(r)}h_{i-1} + b^{(r)} )
$$&lt;p&gt;
更新门：权衡目前新得到的激活$h_i$和过去状态$h_{i-1}$的影响。
&lt;/p&gt;
$$
z_i = \sigma(W_x^{(z)} x_i + W_h^{(z)} h_{i-1} + b^{(z)} )
$$&lt;p&gt;
则新的激活值
&lt;/p&gt;
$$
\widetilde{h_i}=tanh(W_xx_i + r_i * W_h h_{i-1} + b )
$$&lt;p&gt;
最终隐藏层的状态：
&lt;/p&gt;
$$
h_i = z_i * h_{i-1} + (1 - z_i)* \widetilde{h_i}
$$&lt;h4 id=&#34;长短期记忆网络lstm&#34;&gt;长短期记忆网络（LSTM）
&lt;/h4&gt;&lt;p&gt;LSTM：Long Short-Term Memory Network&lt;/p&gt;
&lt;p&gt;也是RNN的变体，可以学习长期的数据一来关系，关键：&lt;strong&gt;cell&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;遗忘门$f_t = \sigma(W \cdot [h_{t-1},x_t] + b_f )$，若$f_t =0$则直接丢弃上一个状态。&lt;/p&gt;
&lt;p&gt;输入门，决定有哪些信息可以存入到cell状态中去&lt;/p&gt;
&lt;p&gt;输入门$i_t$以及新的待选的cell状态如下：
&lt;/p&gt;
$$
i_t= \sigma(W_i \cdot [h_{t-1},x_t] + b_i ) \\
\widetilde{C_t} = tanh(W_C \cdot [h_{t-1},x_t] + b_C )
$$&lt;p&gt;
则cell状态：$C_t = f_t * C_{t-1} + i_t * \widetilde{C_t}$&lt;/p&gt;
&lt;p&gt;输出门，决定哪些信息可以进行输出
&lt;/p&gt;
$$
o_t =\sigma(W_o \cdot [h_{t-1},x_t] + b_o ) \\
h_t = o_t * tamh(C_t)
$$&lt;h4 id=&#34;双向rnn&#34;&gt;双向RNN
&lt;/h4&gt;&lt;p&gt;RNN的变体&lt;/p&gt;
&lt;p&gt;让当前的predit不仅取决于过去的状态，而且取决于未来的状态&lt;/p&gt;
&lt;h2 id=&#34;transformer&#34;&gt;Transformer
&lt;/h2&gt;&lt;h3 id=&#34;注意力机制&#34;&gt;注意力机制
&lt;/h3&gt;&lt;p&gt;解决信息瓶颈。核心思想：在decoder的每一步都把encoder端所有的向量提供给decoder模型。&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先利用RNN得到一个向量$s_t$&lt;/li&gt;
&lt;li&gt;再利用向量$s_1$与encoder所有向量做点积，得到注意力分数$e^t=[s_t^Th_1,\cdots,s_t^Th_N]$&lt;/li&gt;
&lt;li&gt;利用softmax将注意力分数变为一个概率分布$\alpha^t=softmax(e^t)$，decoder更关注概率越大的位置的encoder向量。&lt;/li&gt;
&lt;li&gt;利用概率分布对encoder向量进行加权平均$o_t=\sum_{i=1}^N \alpha_i^th_i$&lt;/li&gt;
&lt;li&gt;拼接$[o_t;s_t]$得到最终用于生成predict的向量&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;注意力机制的各种变式&#34;&gt;注意力机制的各种变式
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;若decoder端向量s&lt;strong&gt;维度&lt;/strong&gt;与encoder端向量$h_i$不一样，则计算注意力分数时，需添加一个权重矩阵&lt;/li&gt;
&lt;/ol&gt;
$$
e_i=s^T W h_i
$$&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Additive attention：使用一层前馈神经网络&lt;/li&gt;
&lt;/ol&gt;
$$
e_i=v^T tanh(W_1 h_i + W_2 s)
$$&lt;h4 id=&#34;注意力机制的特点&#34;&gt;注意力机制的特点
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;decoder端每次生成的时候，可以关注到encoder端所有信息，解决信息瓶颈问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;缓解RNN中的梯度消失问题。通过在encoder和decoder之间提供一种直接连接的方式，防止梯度在RNN中传播过长，进而导致梯度消失。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;给神经网络模型，提供了可解释性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;transformer机制&#34;&gt;Transformer机制
&lt;/h3&gt;&lt;h4 id=&#34;模型结构&#34;&gt;模型结构
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;输入层，将一个文本序列切成一个小的单元token，然后通过embedding可以化为一个向量表示。
&lt;ol&gt;
&lt;li&gt;transformer采用Byte Pair Encoding的方式来对文本进行切分（BPE方法）&lt;/li&gt;
&lt;li&gt;在每个位置加上一个token的位置向量，叫positional encoding，用来表示它在文本序列中的位置&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;主要图层部分，主要由encoder或者decoder的Transformer block堆叠而成。&lt;/li&gt;
&lt;li&gt;输出层，线性层的变换和softmax，输出一个在词表上的概率分布&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;输入编码-bpepe&#34;&gt;输入编码 BPE，PE
&lt;/h4&gt;&lt;p&gt;BPE过程：首先将语料库中出现的所有单词切分为一个个字母，然后通过统计在语料库中每一个byte gram出现的数量，一个一个把频度最高的Byte gram抽象成一个词加入词表中。&lt;/p&gt;
&lt;p&gt;byte gram：连续两个相邻位置字母拼到一起的组合&lt;/p&gt;
&lt;p&gt;PE（Positional Encoding）：通过在原有的embedding上加上一个位置向量，让不同位置的单词具有不同的表示，进而让Transformer block可以进行区分。&lt;/p&gt;
&lt;p&gt;首先假设经过BPE和embedding之后的向量维度d，则位置编码也是一个维度为的向量，Transformer采用基于三角函数的方法来得到位置向量。具体公式如下：
&lt;/p&gt;
$$
PE_{(pos,2i)}=sin(pos/1000^{2i/d}) \\
PE_{pos,2i+1}=cos(pos/1000^{2i/d})
$$&lt;p&gt;
其中pos表示当前token在句子中的位置，是从0到这个序列长度的一个数。i为从0到d/2的一个数，表示当前这个位置在embedding中的index。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input = BPE + PE&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;encoder-block&#34;&gt;Encoder Block
&lt;/h4&gt;&lt;p&gt;整体由两大块组成，分别为Muti-Head Attention网络，Feed-Forward Network前馈神经网络（本质上是一个带激活函数的MLP全连接）&lt;/p&gt;
&lt;p&gt;残差连接：将输入和输出直接相加，缓解模型过深后带来的梯度消失问题。&lt;/p&gt;
&lt;p&gt;正则化：将输入变为一个均值为0，方差为一的分布，解决梯度消失与梯度爆炸。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LaTex语法</title>
        <link>https://heimi2022.github.io/Hugo/p/latex%E8%AF%AD%E6%B3%95/</link>
        <pubDate>Fri, 23 May 2025 20:12:04 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/latex%E8%AF%AD%E6%B3%95/</guid>
        <description>&lt;h2 id=&#34;公式&#34;&gt;公式
&lt;/h2&gt;&lt;h3 id=&#34;上标与下标&#34;&gt;上标与下标
&lt;/h3&gt;&lt;p&gt;用^来输出上标，使用_来输出下标，使用{}包含作用范围。&lt;/p&gt;
&lt;h3 id=&#34;空格表示&#34;&gt;空格表示
&lt;/h3&gt;&lt;p&gt;两个quad空格&amp;mdash;&amp;mdash;&amp;mdash;a \qquad b&amp;mdash;&amp;mdash;&amp;mdash;两个m的宽度&lt;br&gt;
quad空格&amp;mdash;&amp;mdash;&amp;mdash;a \quad b&amp;mdash;&amp;mdash;&amp;mdash;一个m的宽度&lt;br&gt;
大空格&amp;mdash;&amp;mdash;&amp;mdash;a\ b&amp;mdash;&amp;mdash;&amp;mdash;1/3m宽度&lt;br&gt;
中等空格&amp;mdash;&amp;mdash;&amp;mdash;a;b&amp;mdash;&amp;mdash;&amp;mdash;2/7m宽度&lt;br&gt;
小空格	a,b&amp;mdash;&amp;mdash;&amp;mdash;a,b&amp;mdash;&amp;mdash;&amp;mdash;1/6m宽度&lt;br&gt;
没有空格&amp;mdash;&amp;mdash;&amp;mdash;ab&lt;br&gt;
紧贴&amp;mdash;&amp;mdash;&amp;mdash;a!b&amp;mdash;&amp;mdash;&amp;mdash;缩进1/6m宽度&lt;/p&gt;
&lt;h3 id=&#34;数学结构&#34;&gt;数学结构
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;分式 \frac{a}{b}  :  $\frac{a}{b}$&lt;/li&gt;
&lt;li&gt;根式 \sqrt[n]{a}  :  $\sqrt[n]{a}$&lt;/li&gt;
&lt;li&gt;求导 f&amp;rsquo; : $f&#39;$&lt;/li&gt;
&lt;li&gt;累加$\sum$ ：\sum&lt;/li&gt;
&lt;li&gt;累乘$\prod$：&lt;code&gt;\prod&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;极限 $\lim\limits_{x\to\infty}$：\lim\limits_{x\to\infty}&lt;/li&gt;
&lt;li&gt;积分号$\int$：\int&lt;/li&gt;
&lt;li&gt;其它 上划线:\overline{a},下划线\underline{a},上右箭头\overrightarrow{a}:$\overrightarrow{a}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;数学函数&#34;&gt;数学函数
&lt;/h3&gt;&lt;p&gt;1.三角函数 $\sin$：\sin ; $\cos$：\cos;&lt;/p&gt;
&lt;h3 id=&#34;数学符号&#34;&gt;数学符号
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;偏导数 $\partial$  ：\partial&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;数学运算&#34;&gt;数学运算
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;点乘 $a \cdot b$：\cdot&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;叉乘 $a \times b$：\times&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;除以 $a \div b$：\div&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;希腊字母&#34;&gt;希腊字母
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;$\alpha \beta \gamma \delta \epsilon \zeta \eta \theta$：\alpha \beta \gamma \delta \epsilon \zeta \eta \theta&lt;/li&gt;
&lt;li&gt;$\iota \kappa \lambda \mu \nu \omicron \xi \pi$  ：\iota \kappa \lambda \mu \nu \omicron \xi \pi&lt;/li&gt;
&lt;li&gt;$\rho \sigma \tau \upsilon \phi \chi \psi \omega$：\rho \sigma \tau \upsilon \phi \chi \psi \omega&lt;/li&gt;
&lt;li&gt;$\Alpha \Beta \Gamma \Delta \Epsilon \Zeta \Eta \Theta$：\Alpha \Beta \Gamma \Delta \Epsilon \Zeta \Eta \Theta&lt;/li&gt;
&lt;li&gt;$\Iota \Kappa \Lambda \Mu \Nu \Xi \Omicron \Pi$：\Iota \Kappa \Lambda \Mu \Nu \Xi \Omicron \Pi&lt;/li&gt;
&lt;li&gt;$\Rho \Sigma \Tau \Upsilon \Phi \Chi \Psi \Omega$：\Rho \Sigma \Tau \Upsilon \Phi \Chi \Psi \Omega&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;关系符号&#34;&gt;关系符号
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;$\leqslant ， \geqslant ，\le ，\ge$：\leqslant ， \geqslant ，\le ，\ge&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\in&lt;/code&gt; ：$\in$&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\ne&lt;/code&gt;：$\ne$&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\approx&lt;/code&gt;：$\approx$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;括号&#34;&gt;括号
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;大括号 &lt;code&gt;\{&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;其它&#34;&gt;其它
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;\underset{123}{abc} : $\underset{123}{abc}$&lt;/li&gt;
&lt;li&gt;使用 &lt;strong&gt;\limits&lt;/strong&gt; 将下标放在某个文字或者符号的正下方。&lt;/li&gt;
&lt;li&gt;用**\mathop**{文本}命令将文本转化成数学符号&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;省略符号&lt;/strong&gt;： &lt;code&gt;\cdots&lt;/code&gt; $\cdots$ , &lt;code&gt;\ddots&lt;/code&gt; $\ddots$ , &lt;code&gt;\vdots&lt;/code&gt; $\vdots$&lt;/li&gt;
&lt;li&gt;$\widetilde{x}$：&lt;code&gt;\widetilde{x}&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;矩阵条件表达式方程组&#34;&gt;矩阵、条件表达式、方程组
&lt;/h3&gt;&lt;p&gt;语法：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\begin{类型}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	公式内容
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\end{类型}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;无框矩阵-matrix&#34;&gt;无框矩阵 matrix
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\begin{matrix}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x &amp;amp; y \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;z &amp;amp; v
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\end{matrix}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;$$
\begin{matrix}
x &amp; y \\
z &amp; v
\end{matrix}
$$&lt;h4 id=&#34;有框矩阵-pmatrixbmatrixvmatrix&#34;&gt;有框矩阵 pmatrix、bmatrix、vmatrix
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;vmatrix&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\begin{vmatrix}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;x &amp;amp; y \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;z &amp;amp; v
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\end{vmatrix}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;$$
\begin{vmatrix}
x &amp; y \\
z &amp; v
\end{vmatrix}
$$&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;V&lt;/strong&gt;matrix&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{Vmatrix}
x &amp; y \\
z &amp; v
\end{Vmatrix}
$$&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;bmatrix&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{bmatrix}
0      &amp; \cdots &amp; 0      \\
\vdots &amp; \ddots &amp; \vdots \\
0      &amp; \cdots &amp; 0
\end{bmatrix}
$$&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;matrix&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{Bmatrix}
a      &amp; b      \\
c      &amp; d
\end{Bmatrix}
$$&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;pmatrix&lt;/li&gt;
&lt;/ol&gt;
$$
\begin{pmatrix}
x &amp; y \\
z &amp; v
\end{pmatrix}
$$&lt;h4 id=&#34;条件表达式方程组cases&#34;&gt;条件表达式、方程组：cases
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;f(n) =
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\begin{cases} 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1,  &amp;amp; \text{if n is 1} \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2, &amp;amp; \text{if n is 2}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\end{cases}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;$$
f(n) =
\begin{cases} 
1,  &amp; \text{if n is 1} \\
2, &amp; \text{if n is 2}
\end{cases}
$$&lt;h4 id=&#34;多行等式-aligned&#34;&gt;多行等式 aligned
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\begin{aligned}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;f(x) &amp;amp; = 2 \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &amp;amp; = 1+1 \\
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;\end{aligned}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;$$
\begin{aligned}
f(x) &amp; = 2 \\
     &amp; = 1+1 \\
\end{aligned}
$$</description>
        </item>
        <item>
        <title>Markdown语法</title>
        <link>https://heimi2022.github.io/Hugo/p/markdown%E8%AF%AD%E6%B3%95/</link>
        <pubDate>Fri, 23 May 2025 20:12:04 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/markdown%E8%AF%AD%E6%B3%95/</guid>
        <description>&lt;h2 id=&#34;插入公式&#34;&gt;插入公式
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;行内公式 : 输入 &lt;strong&gt;&lt;code&gt;$&lt;/code&gt; + &lt;code&gt;esc&lt;/code&gt;&lt;/strong&gt; ,则可实现 &lt;code&gt;$&lt;/code&gt;  公式  &lt;code&gt;$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;行间公式 : 输入 &lt;strong&gt;&lt;code&gt;$$&lt;/code&gt; + 回车&lt;/strong&gt; ,则可实现  &lt;code&gt;$$&lt;/code&gt; 公式 &lt;code&gt;$$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;图片排版&#34;&gt;图片排版
&lt;/h2&gt;&lt;p&gt;适用html语言&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;center&amp;gt;  创建中间对齐图片 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;lt;img src = &amp;#34;地址&amp;#34;   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    width =&amp;#34;40\%&amp;#34;&amp;gt; 缩放比率，还有height可选  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;lt;br&amp;gt;  插入回车，回车之前重复图片操作可并排放图
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    图注
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;/center&amp;gt; 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;列表&#34;&gt;列表
&lt;/h2&gt;&lt;h3 id=&#34;无序列表&#34;&gt;无序列表
&lt;/h3&gt;&lt;p&gt;无序列表&lt;strong&gt;使用星号(*)、加号(+)或是减号(-)作为列表标记&lt;/strong&gt;，这些标记后面要添加一个空格，然后再填写内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建议统一使用减号 &lt;strong&gt;-&lt;/strong&gt;，因为它在视觉上更清晰&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;有序列表&#34;&gt;有序列表
&lt;/h3&gt;&lt;p&gt;数字加一点。 数字可以不从1开始。&lt;/p&gt;
&lt;h3 id=&#34;列表嵌套&#34;&gt;列表嵌套
&lt;/h3&gt;&lt;p&gt;列表可以嵌套使用，创建多层次的结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;水果
&lt;ul&gt;
&lt;li&gt;苹果
&lt;ul&gt;
&lt;li&gt;青苹果&lt;/li&gt;
&lt;li&gt;红苹果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;水果
&lt;ul&gt;
&lt;li&gt;苹果
&lt;ol&gt;
&lt;li&gt;青苹果&lt;/li&gt;
&lt;li&gt;红苹果&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;任务列表&#34;&gt;任务列表
&lt;/h3&gt;&lt;p&gt;使用&lt;code&gt;- [ ] 1&lt;/code&gt;创建&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用&lt;code&gt;- [x] 2&lt;/code&gt;创建&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 2&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Git使用</title>
        <link>https://heimi2022.github.io/Hugo/p/git%E4%BD%BF%E7%94%A8/</link>
        <pubDate>Thu, 22 May 2025 17:20:11 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/git%E4%BD%BF%E7%94%A8/</guid>
        <description>&lt;h2 id=&#34;问题解决&#34;&gt;问题解决
&lt;/h2&gt;&lt;h3 id=&#34;git-push&#34;&gt;git push
&lt;/h3&gt;&lt;h4 id=&#34;fatal-unable-to-access-httpsgithubcomgit-could-not-resolve-host-githubcom&#34;&gt;fatal: unable to access ‘https://github.com/&amp;hellip;/.git‘: Could not resolve host: github.com
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git config --global --unset http.proxy   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git config --global --unset https.proxy    
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;解决方案：cmd下命令执行 ipconfig/flushdns
清理dns缓存&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Machine-Learning</title>
        <link>https://heimi2022.github.io/Hugo/p/machine-learning/</link>
        <pubDate>Thu, 22 May 2025 16:36:28 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/machine-learning/</guid>
        <description>&lt;h2 id=&#34;引言&#34;&gt;引言
&lt;/h2&gt;&lt;h3 id=&#34;什么是机器学习&#34;&gt;什么是机器学习
&lt;/h3&gt;&lt;p&gt;定义如下，一个程序被认为能从 &lt;strong&gt;经验 E&lt;/strong&gt; 中学习，解决 &lt;strong&gt;任务 T&lt;/strong&gt;，达到 &lt;strong&gt;性能度量值 P&lt;/strong&gt;，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T 时的性能有所提升。&lt;/p&gt;
&lt;p&gt;例如下棋游戏:&lt;br&gt;
E 就是程序上万次的自我练习的经验。&lt;br&gt;
T 就是下棋。&lt;br&gt;
P 就是它在与一些新的对手比赛时，赢得比赛的概率。&lt;/p&gt;
&lt;h3 id=&#34;监督学习&#34;&gt;监督学习
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;监督学习&lt;/strong&gt; 指的就是我们给学习算法一个 &lt;strong&gt;数据集&lt;/strong&gt;。这个数据集由“正确答案”组成。学习算法再根据这个数据集作出预测，算出更多的正确答案。&lt;br&gt;
监督学习的两个例子, &lt;strong&gt;回归问题 与 分类问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;回归问题 : 推测出一个 &lt;strong&gt;连续&lt;/strong&gt; 值的输出结果。&lt;br&gt;
例如 : 卖水果，数据集 3 斤卖 10 元左右，预测 4 斤能卖多少。&lt;/p&gt;
&lt;p&gt;分类问题 : 推测出一组 &lt;strong&gt;离散&lt;/strong&gt; 的结果。&lt;br&gt;
例如 : 识别红绿灯。&lt;/p&gt;
&lt;h3 id=&#34;无监督学习&#34;&gt;无监督学习
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;无监督学习&lt;/strong&gt; 指的是一种学习策略，它交给算法大量的数据，并让算法为我们从数据中找出某种结构。&lt;br&gt;
无监督学习中 &lt;strong&gt;已知数据没有任何的标签&lt;/strong&gt; 是指数据 &lt;strong&gt;有相同的标签或者就是没标签&lt;/strong&gt;。&lt;br&gt;
无监督学习的例子，&lt;strong&gt;聚类算法、鸡尾酒算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;聚类算法 : 将数据分为几类不同的 &lt;strong&gt;簇&lt;/strong&gt;。&lt;br&gt;
例子 : 谷歌新闻，同一主题新闻归为一类; 市场分割。&lt;/p&gt;
&lt;p&gt;鸡尾酒算法 : 分离麦克风接收到的不同的人的声音与环境声音。&lt;/p&gt;
&lt;h2 id=&#34;单变量线性回归linear-regression-with-one-variable&#34;&gt;单变量线性回归(Linear Regression with One Variable)
&lt;/h2&gt;&lt;h3 id=&#34;模型表示&#34;&gt;模型表示
&lt;/h3&gt;&lt;p&gt;样本数目 : 小写 &lt;strong&gt;m&lt;/strong&gt;&lt;br&gt;
训练集 : &lt;strong&gt;Training Set&lt;/strong&gt;&lt;br&gt;
特征/输入变量 : &lt;strong&gt;$x$&lt;/strong&gt;&lt;br&gt;
目标/输出变量 : &lt;strong&gt;$y$&lt;/strong&gt;&lt;br&gt;
训练集中的实例 : &lt;strong&gt;($x$, $y$)&lt;/strong&gt;&lt;br&gt;
第 $i$ 个观察实例 : &lt;strong&gt;($x^{(i)}$, $y^{(i)}$)&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;学习算法的解决方案或函数也称为假设(hypothesis) : h&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;模型表示就是找到将训练集给学习算法找到一个合适的函数 $h$ , $y = h(x)$&lt;/p&gt;
&lt;p&gt;函数 $h_\theta(x) = \theta_0 + \theta_1x$&lt;br&gt;
由于只含有一个特征/输入变量，因此这样的问题叫做单变量线性回归问题。&lt;/p&gt;
&lt;h3 id=&#34;代价函数&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;建模误差: 模型所预测的值与训练集中实际值之间的差距 $h_\theta(x^{(i)}) - y^{(i)}$&lt;/p&gt;
&lt;p&gt;平方误差函数 $J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$&lt;br&gt;
平方误差函数是代价函数之一，对于大多数问题，特别是回归问题，其都是个合理的选择。&lt;br&gt;
为什么是 $\frac{1}{2m}$:&lt;br&gt;
除以 m 是为了消除样本数量 m 对 J 的影响&lt;br&gt;
除以 2 是为了抵消对 J 关于 θ 求偏导数时式子多出的一个 2，使计算更方便&lt;/p&gt;
&lt;p&gt;我们的目标就是找到一个合适的参数 $\theta_0,\theta_1$ 使得代价函数最小。 即 Goal: $\underset{\theta_0,\theta_1}{minimize}J(\theta_0,\theta_1)$&lt;/p&gt;
&lt;h3 id=&#34;梯度下降&#34;&gt;梯度下降
&lt;/h3&gt;&lt;p&gt;梯度下降是一个用来 &lt;strong&gt;求函数最小值&lt;/strong&gt; 的算法，可以用其来求出使代价函数 $J$ 最小的参数 $\theta_0$ 和 $\theta_1$ 的值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;梯度下降的思想&lt;/strong&gt; : 开始时，随机选择一组参数 $(\theta_0,\theta_1)$, 计算代价函数, 然后寻找下一个能让代价函数下降最多的参数组合。持续这种操作，直至找到一个局部最小值。&lt;br&gt;
局部最小值不等同于全局最小值。不同的初始参数会可能会得到完全不同的局部最小值。&lt;/p&gt;
&lt;h4 id=&#34;批量梯度下降算法batch-gradient-descent&#34;&gt;批量梯度下降算法(batch gradient descent)
&lt;/h4&gt;&lt;p&gt;批量 batch 指的是，在梯度下降的每一步中，都用到来所有的训练样本。&lt;/p&gt;
&lt;p&gt;公式为:
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat until convergence（重复直至收敛）} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1) \quad (\text{for } j = 0 \text{ and } j = 1) \\
&amp;\}
\end{aligned}
$$&lt;p&gt;其中 $\alpha$ 是学习率，其决定了我们沿代价函数梯度方向向下迈出的幅度有多大。&lt;br&gt;
$\alpha$ 太小，需要很多步才能够达到局部最低点。&lt;br&gt;
$\alpha$ 太大，梯度下降法可能会越过最低点，甚至可能无法收敛，导致发散。&lt;/p&gt;
&lt;p&gt;在梯度下降法中，当参数取值接近局部最低点时，梯度下降法会自动采取更小的幅度，因为代价函数导数会趋向于 0。&lt;/p&gt;
&lt;h2 id=&#34;多变量线性回归&#34;&gt;多变量线性回归
&lt;/h2&gt;&lt;h3 id=&#34;多维特征&#34;&gt;多维特征
&lt;/h3&gt;&lt;p&gt;新的注释：&lt;/p&gt;
&lt;p&gt;$n$ 代表特征的数量&lt;/p&gt;
&lt;p&gt;$x^{(i)}$ 代表第 $i$ 个训练实例，是特征数据的第 $i$ 行，是一个 **向量 **&lt;/p&gt;
&lt;p&gt;$x_j^{(i)}$ 代表特征矩阵中第 $i$ 行第 $j$ 个特征&lt;/p&gt;
&lt;p&gt;多变量的假设 $h$ 表示为：&lt;br&gt;
&lt;/p&gt;
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
$$&lt;p&gt;
此时模型中参数为一个 $n+1$ 维向量&lt;/p&gt;
&lt;p&gt;任何一个训练实例也都是一个 $n+1$ 维向量&lt;/p&gt;
&lt;p&gt;特征矩阵 X 的维度是 $m*(n+1)$&lt;/p&gt;
&lt;p&gt;因此公式可以简化为&lt;/p&gt;
$$
h_\theta(x)=\theta^TX
$$&lt;h3 id=&#34;多变量梯度下降&#34;&gt;多变量梯度下降
&lt;/h3&gt;&lt;p&gt;代价函数为所有建模误差的平方和，即:&lt;br&gt;
&lt;/p&gt;
$$
J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2
$$&lt;p&gt;多变量线性回归的批量梯度下降算法为：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1,...\theta_n) \\ 
&amp;\}
\end{aligned}
$$&lt;p&gt;即:&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \frac{1}{2m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2 \\
&amp;\}
\end{aligned}
$$&lt;p&gt;
求导后得到：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{repeat} \{ \\
&amp;\quad \theta_j := \theta_j - \alpha  \frac{1}{m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)}) \cdot x_j^{(i)} \\
&amp;\}
\end{aligned}
$$&lt;h3 id=&#34;梯度下降法&#34;&gt;梯度下降法
&lt;/h3&gt;&lt;h4 id=&#34;特征缩放&#34;&gt;特征缩放
&lt;/h4&gt;&lt;p&gt;使特征具有相近的尺度有助于梯度下降宣发更快的收敛。&lt;/p&gt;
&lt;p&gt;方法：将所有特征的尺度都尽量缩放到-1 到 1 &lt;strong&gt;这种小范围&lt;/strong&gt; 内，也不能太小。&lt;/p&gt;
&lt;p&gt;最简单的，令 $x_n=\frac{x_n-\mu_n}{s_n}$, 其中 $\mu_n$ 是平均值，$s_n$ 是标准差。&lt;/p&gt;
&lt;h4 id=&#34;学习率&#34;&gt;学习率
&lt;/h4&gt;&lt;p&gt;梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，可以绘制 &lt;strong&gt;迭代次数-代价函数&lt;/strong&gt; 的图表来观测算法在何时收敛。&lt;/p&gt;
&lt;p&gt;梯度下降算法的每次迭代受学习率 $\alpha $ 影响：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha$ 太小，则到达收敛所需的迭代次数很大，但理论上终会收敛。&lt;/li&gt;
&lt;li&gt;$\alpha $ 太大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;调整学习率时可以采用以下 &lt;strong&gt;策略&lt;/strong&gt;：从 0.001 开始每次上在原基础上乘 3 倍，&lt;strong&gt;最终选择一个较大的可以使得代价函数收敛的 $\alpha $&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如：0.001，0.003，0.01，0.03，0.1&lt;/p&gt;
&lt;h3 id=&#34;特征选择与多项式回归&#34;&gt;特征选择与多项式回归
&lt;/h3&gt;&lt;h4 id=&#34;特征选择&#34;&gt;特征选择
&lt;/h4&gt;&lt;p&gt;建立模型时，可以 &lt;strong&gt;根据需要选择适合模型的特征&lt;/strong&gt; 而不局限于数据集中的特征。比如：数据集给出房子的长和宽，预测房价时，可以创造一个特征为 *&lt;em&gt;面积 = 长 &lt;em&gt;宽&lt;/em&gt;&lt;/em&gt; 作为模型的新特征。&lt;/p&gt;
&lt;h4 id=&#34;多项式回归&#34;&gt;多项式回归
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;线性回归并不适合所有数据&lt;/strong&gt;，有时需要曲线来适应数据，比如：&lt;/p&gt;
&lt;p&gt;三次方模型 $h_\theta(x)=\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3$&lt;/p&gt;
&lt;p&gt;使用多项式回归模型，特征缩放很有必要。&lt;/p&gt;
&lt;h3 id=&#34;正规方程&#34;&gt;正规方程
&lt;/h3&gt;&lt;p&gt;当特征数量小于 10000 时（对于目前的计算机），通常采用正规方程的方法找到一组最优参数 $\theta$。&lt;/p&gt;
&lt;p&gt;正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial \theta_j} J(\theta_j)=0$，&lt;/p&gt;
&lt;p&gt;其 &lt;strong&gt;解&lt;/strong&gt; 为 $\theta=(X^TX)^{-1}X^Ty$。$X$ 为训练集特征矩阵，$y$ 为训练集结果。&lt;/p&gt;
&lt;h4 id=&#34;梯度下降于正规方程的比较&#34;&gt;梯度下降于正规方程的比较
&lt;/h4&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;梯度下降&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;正规方程&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;需要选择学习率 $\alpha $&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;不需要&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;需要多次迭代&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;一次运算得出&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;特征数量 n 大时也可用&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;矩阵逆的计算时间复杂度为 $O(n^3)$。因此 n 小于 10000 时，可以接受&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;使用于各种类型&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;只适用于线性模型。不适合逻辑回归等其它模型。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;以下情况时，正规方程不能用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有两个特征线性相关。  解决：使用正规方程前，剔除相关特征。&lt;/li&gt;
&lt;li&gt;含有大量特征，进而出现 $m \leqslant n$ 时。  解决：剔除特征，用较少特征反应尽可能多的内容。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;逻辑回归&#34;&gt;逻辑回归
&lt;/h2&gt;&lt;p&gt;逻辑回归算法是一种分类算法。&lt;/p&gt;
&lt;p&gt;分类问题预测的变量 $y$ 为 &lt;strong&gt;离散值&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;假说表示&#34;&gt;假说表示
&lt;/h3&gt;&lt;p&gt;引入一个 &lt;strong&gt;逻辑函数 Sigmoid function&lt;/strong&gt;，S 形函数，公式为 $g(z)=\frac{1}{1+e^{-z}}$。其图像如下所示：&lt;/p&gt;
&lt;center&gt;
    &lt; img src = &#34;https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_sigmoid_function.png&#34;
         width = &#34;40%&#34; &gt;
    &lt;br&gt;
    sigmoid function
&lt;/center&gt;
&lt;p&gt;令逻辑回归模型的假设是：$h_\theta(x)=g(\theta^T X)$, $X$ 代表特征向量，$g$ 代表逻辑函数。&lt;/p&gt;
&lt;p&gt;此时模型的输出变量范围始终在 0 和 1 之间。&lt;/p&gt;
&lt;p&gt;$h_\theta(x)$ 表示，对于给定的输入变量，根据选择的参数，计算输出变量为 1 的可能性，即：
&lt;/p&gt;
$$
h_\theta(x)= P(y = 1|x;\theta)
$$&lt;h3 id=&#34;决策边界&#34;&gt;决策边界
&lt;/h3&gt;&lt;p&gt;在逻辑回归中，假设：&lt;br&gt;
&lt;/p&gt;
$$
y = \begin{cases}
1, &amp; \text{if } h_\theta(x) \geq 0.5 \\
0, &amp; \text{if } h_\theta(x) &lt; 0.5
\end{cases}
$$&lt;p&gt;因此当 $\theta^T x \geqslant 0$ 时，预测 $y=1$，当 $\theta^T x &amp;lt; 0$ 时，预测 $y=0$。&lt;/p&gt;
&lt;p&gt;因此对于一组参数 $\theta$，有 $\theta^T x=0$，这条 &lt;strong&gt;曲线&lt;/strong&gt; 即为模型的决策边界。&lt;/p&gt;
&lt;h3 id=&#34;代价函数-1&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;为什么逻辑回归的代价函数和线性回归的不同？由于 sigmoid 函数的非线性，导致逻辑回归的假设函数带入代价函数时，会导致代价函数有很多局部最小值，其是一个非凸函数。因此需对代价函数进行修正。&lt;/p&gt;
&lt;p&gt;定义逻辑回归的代价函数 $J(\theta)=\frac{1}{m} \sum^m_{i=1} Cost(h_\theta(x^{(i)}),y^{(i)})$，其中
&lt;/p&gt;
$$
Cost(h_\theta(x^{(i)}), y^{(i)}) = \begin{cases}
   -log( h_\theta(x)) \quad &amp;  if\quad y = 1, &amp;  \\
   -log( 1- h_\theta(x)) &amp;  if\quad y = 0, &amp;  \\
\end{cases}
\overset{简化}{=} -ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))
$$&lt;h4 id=&#34;逻辑回归中-cost-函数特点&#34;&gt;逻辑回归中 Cost 函数特点
&lt;/h4&gt;&lt;p&gt;当实际的 $y=1$ 且 $h_\theta(x)=1$ 时，误差为 0，当 $y=1$ 但 $h_\theta(x)$ 不为 1 时，误差随着 $h_\theta(x)$ 的变小而变大。当 $h_\theta(x)$ 减小为 0，表示预测与实际完全相反，其误差为无穷大，这时代价函数将会受到很大的惩罚；当实际的 $y=0$ 且 $h_\theta(x)=0$ 时，误差为 0，当 $y=0$ 但 $h_\theta(x)$ 不为 0 时，误差随着 $h_\theta(x)$ 的变大而变大。&lt;/p&gt;
&lt;h4 id=&#34;简化后的代价函数&#34;&gt;简化后的代价函数
&lt;/h4&gt;$$
\begin{aligned}
J(\theta)&amp; = \frac{1}{m} \sum^m_{i = 1} Cost(h_\theta(x^{(i)}), y^{(i)}) &amp; \\
&amp; = -\frac{1}{m} [\sum^m_{i = 1}-ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))] &amp;\\
\end{aligned}
$$&lt;p&gt;最小化代价函数的方法 &lt;strong&gt;是梯度下降法&lt;/strong&gt;：
&lt;/p&gt;
$$
\begin{aligned}
&amp;\text{Repeat} \{ \\
&amp; \theta_j := \theta_j - \alpha \frac{1}{m} \sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp;\}
\end{aligned}
$$&lt;p&gt;虽然其形式上与线性回归的梯度下降法相同，但由于回归的模型假设发生了变化，两者是完全不同的函数。&lt;/p&gt;
&lt;h3 id=&#34;高级优化&#34;&gt;高级优化
&lt;/h3&gt;&lt;p&gt;优化算法：共轭梯度法 BFGS（变尺度法）和 L-BFGS（限制遍尺度法）。&lt;/p&gt;
&lt;p&gt;优点：不需要手动选择学习率。&lt;/p&gt;
&lt;h3 id=&#34;多类别分类&#34;&gt;多类别分类
&lt;/h3&gt;&lt;p&gt;假设有 n 个类别，则将其分为 &lt;strong&gt;n 个&lt;/strong&gt; 1 对（n-1）的 &lt;strong&gt;二元分类问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;记每一个分类问题的模型为：$h_\theta ^{(i)} (x)= p(y=i|x;\theta)$，其中 $i=(1,2,3,&amp;hellip;,k)$。&lt;/p&gt;
&lt;p&gt;最终输出结果为让 $h_\theta ^{(i)} (x)$ 最大的 i，即 $\mathop{max}\limits_{i} ; h_\theta ^{(i)} (x)$。&lt;/p&gt;
&lt;h2 id=&#34;正则化&#34;&gt;正则化
&lt;/h2&gt;&lt;h3 id=&#34;过拟合&#34;&gt;过拟合
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;过拟合&lt;/strong&gt; 指通过学习得到的假设可能能够非常好的适应训练集（代价函数可能几乎为 0），但是可能 &lt;strong&gt;不会&lt;/strong&gt; 推广到预测新的数据。&lt;/p&gt;
&lt;p&gt;当有 &lt;strong&gt;过多的变量但只有很少的训练数据&lt;/strong&gt; 时很容易出现过拟合问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;欠拟合&lt;/strong&gt; 指模型不能适应训练集。&lt;/p&gt;
&lt;h4 id=&#34;解决过拟合&#34;&gt;解决过拟合
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;减少特征变量的数目 &amp;ndash; 人工选择去除一些变量；模型算法选择（如 PCA）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;正则化&lt;/strong&gt; &amp;ndash; 保留所有的特征变量，但减小参数 $\theta$ 的大小。 其适用于有很多特征变量，且每一个特征变量看起来对预测结果都有一点用的情况。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;代价函数-2&#34;&gt;代价函数
&lt;/h3&gt;&lt;h4 id=&#34;正则化的直观理解&#34;&gt;正则化的直观理解
&lt;/h4&gt;&lt;p&gt;假设模型为 $h_\theta(x)=\theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 + \theta_4 x^4$，且高次项导致了过拟合的产生，则 &lt;strong&gt;假如能让高次项的系数趋向于 0，就能够缓解过拟合的问题&lt;/strong&gt; 了。 因此修改代价函数 ，对 $\theta_3 \text{和}\theta_4$ 设置一些惩罚，如下所示：
&lt;/p&gt;
$$
J(\theta)=\frac{1}{2m}[\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2+1000 \theta_3^2 + 1000 \theta_4^2]
$$&lt;p&gt;
则通过这样的代价函数选择出来的 $\theta_3 \text{和}\theta_4$ 对预测结果的影响就很小了。&lt;/p&gt;
&lt;h4 id=&#34;带正则化参数的代价函数&#34;&gt;带正则化参数的代价函数
&lt;/h4&gt;&lt;p&gt;修改后的代价函数如下：
&lt;/p&gt;
$$
J(\theta)=\frac{1}{2m}[\sum^m_{i = 1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum^n_{j = 1}\theta_j^2]
$$&lt;p&gt;
其中，$\lambda$ 成为正则化参数。&lt;/p&gt;
&lt;p&gt;假如 $\lambda$ 太大，则所有 $\theta$ 都会趋于 0，除了 $\theta_0$，这样最终模型为一条平行于 x 轴的直线，导致欠拟合。&lt;/p&gt;
&lt;h3 id=&#34;正则化线性回归&#34;&gt;正则化线性回归
&lt;/h3&gt;&lt;h4 id=&#34;梯度下降-1&#34;&gt;梯度下降
&lt;/h4&gt;$$
\begin{aligned}
&amp; \text{Repeat until convergence} \{ \\
&amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
&amp; \theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp; \}
\end{aligned}
$$&lt;p&gt;正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新柜子的基础上令 $\theta$ 值减少了一个额外的值。&lt;/p&gt;
&lt;h4 id=&#34;正规化&#34;&gt;正规化
&lt;/h4&gt;$$
\theta = (X^T X + \lambda \begin{bmatrix}
0 &amp; &amp; &amp; &amp; &amp; \\
&amp; 1 &amp; &amp; &amp; &amp; \\
&amp; &amp; 1 &amp; &amp; &amp; \\
&amp; &amp; &amp; \cdot &amp; &amp; \\
&amp; &amp; &amp; &amp; \cdot &amp; \\
&amp; &amp; &amp; &amp; &amp; 1 \\
\end{bmatrix}^{-1})^{-1} X^Ty
$$&lt;p&gt;图中矩阵尺寸为 $(n+1)*(n+1)$&lt;/p&gt;
&lt;h3 id=&#34;正则化逻辑回归&#34;&gt;正则化逻辑回归
&lt;/h3&gt;&lt;p&gt;正则化逻辑回归的代价函数：
&lt;/p&gt;
$$
J(\theta) = \frac{1}{m} [\sum^m_{i = 1}-ylog( h_\theta(x))-(1-y)log( 1- h_\theta(x))] + \frac{\lambda}{2m} \sum^n_{j = 1}\theta^2_j 
$$&lt;p&gt;
则梯度下降算法为：
&lt;/p&gt;
$$
\begin{aligned}
&amp; \text{Repeat until convergence} \{ \\
&amp; \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)} \\
&amp; \theta_j := \theta_j(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m} \sum^m_{i = 1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp; \}
\end{aligned}
$$&lt;h2 id=&#34;神经网络&#34;&gt;神经网络
&lt;/h2&gt;&lt;h3 id=&#34;模型表示-1&#34;&gt;模型表示
&lt;/h3&gt;&lt;p&gt;神经网络模型建立在很多神经元之上，每一个神经元又是一个学习模型。这些神经元也叫激活单元（activation unit）。&lt;/p&gt;
&lt;p&gt;神经网络模型是许多逻辑单元按照不同的层级组织起来的网络，每一层的输出变量都是下一层的输入变量。&lt;/p&gt;
&lt;p&gt;引入以下标记法来描述模型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha _i ^{(j)}$ 代表第 $j$ 层的第 $i$ 个激活单元。&lt;/li&gt;
&lt;li&gt;$\theta^{(j)}$ 代表从第 $j$ 层映射到第 $j+1$ 层时的权重矩阵。其 &lt;strong&gt;尺寸&lt;/strong&gt; 为以第 $j+1$ 层的激活单元数量作为行数，以第 $j$ 层的激活单元数量 &lt;strong&gt;加一&lt;/strong&gt; 作为列数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;举例&#34;&gt;举例
&lt;/h4&gt;&lt;center&gt;
    &lt; img src = &#34;https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_Neural_Network_layer3_example&#34;
         width = &#34;40%&#34; &gt;
    &lt;br&gt;
    3-Layer Neural model
&lt;/center&gt;
&lt;p&gt;对于上图所示模型，激活单元和输出分别表达为：
&lt;/p&gt;
$$
\begin{aligned}
&amp; \alpha_1^{(2)}= g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1  + \Theta_{12}^{(1)}x_2  + \Theta_{13}^{(1)}x_3 ) \\
&amp; \alpha_2^{(2)}= g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1  + \Theta_{22}^{(1)}x_2  + \Theta_{23}^{(1)}x_3 ) \\
&amp; \alpha_3^{(2)}= g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1  + \Theta_{32}^{(1)}x_2  + \Theta_{33}^{(1)}x_3 ) \\
&amp; h_\theta(x)= g(\Theta_{10}^{(2)}\alpha_0^{(2)} + \Theta_{11}^{(2)}\alpha_1^{(2)}  + \Theta_{12}^{(2)}\alpha_2^{(2)}  + \Theta_{13}^{(2)}\alpha_3^{(2)} ) \\
\end{aligned}
$$&lt;p&gt;
将例中这种从左往右的算法称为 &lt;strong&gt;向前传播算法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;若只看输出部分，其输出方式类似逻辑回归模型，可以将 $\alpha $ 看成是更高级的特征值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;多类分类&lt;/strong&gt; 问题：输出层用 n 个神经元，表示 n 类。&lt;/p&gt;
&lt;h2 id=&#34;神经网络的学习&#34;&gt;神经网络的学习
&lt;/h2&gt;&lt;h3 id=&#34;代价函数-3&#34;&gt;代价函数
&lt;/h3&gt;&lt;p&gt;新的标记方法：&lt;/p&gt;
&lt;p&gt;假设神经网络的训练样本由 $m$ 个，每个包含一组输入 $x$ 和一组输出信号 $y$&lt;/p&gt;
&lt;p&gt;$L$ 表示神经网络的层数&lt;/p&gt;
&lt;p&gt;$S_l$ 表示 $l$ 层中神经元的个数，$S_L$ 表示最后一层中神经元的个数&lt;/p&gt;
&lt;p&gt;神经网络的分类定义为两种情况：二类分类和多类分类，&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;二类分类：$S_L=1$, $y=0 \text{ or } 1$&lt;/li&gt;
&lt;li&gt;$K$ 类分类：$S_L=k$, $y_i=1$ 表示分到第 $i$ 类；$(K&amp;gt;2)$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在神经网络中，$h_\theta(x)$ 是一个维度为 $K$ 的向量，则：$h_\theta(x) \in R^K,(h_\theta(x))_i=i^{th} output$&lt;/p&gt;
&lt;p&gt;代价函数如下：
&lt;/p&gt;
$$
J(\Theta)=-\frac{1}{m}[\sum_{i = 1}^m \sum_{k = 1}^k y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})log(1 - (h_\Theta(x^{(i)}))_k) ] + \frac{\lambda}{2m}\sum_{l = 1}^{L-1}\sum_{i = 1}^{s_l}\sum_{j = 1}^{s_l+1}(\Theta_{ji}^{(l)})^2
$$&lt;ol&gt;
&lt;li&gt;对于每一行特征：由于有 $K$ 个特征，可以利用循环，对每一行特征都预测 $K$ 个不同结果，然后利用循环将 $K$ 个预测偏差累加。&lt;/li&gt;
&lt;li&gt;对于正则化这一项，遍历所有层的所有参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;反向传播算法&#34;&gt;反向传播算法
&lt;/h3&gt;&lt;p&gt;为了 &lt;strong&gt;计算代价函数的偏导数&lt;/strong&gt; $\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)$，需要采用一种反向传播算法。&lt;/p&gt;
&lt;p&gt;反向传播算法：首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。&lt;/p&gt;
&lt;h4 id=&#34;反向传播算法中误差的计算&#34;&gt;反向传播算法中误差的计算
&lt;/h4&gt;&lt;p&gt;用 $\delta$ 来表示误差。对于一个 4 层的网络，反向传播误差分 3 步计算：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\delta^{(4)}=a^{(4)}-y$&lt;/li&gt;
&lt;li&gt;$\delta^{(3)}=(\Theta^{(3)})^T \delta^{(4)}*g&amp;rsquo;(z^{(3)})$，$g&amp;rsquo;(z^{(3)})$ 是 sigmoid 函数的导数，且 $g&amp;rsquo;(z^{(3)})= a^{(3)} * (1 - a^{(3)})$，$(\Theta^{(3)})^T \delta^{(4)}$ 是权重导致的误差的和。&lt;/li&gt;
&lt;li&gt;$\delta^{(2)}=(\Theta^{(2)})^T \delta^{(4)}*g&amp;rsquo;(z^{(2)})$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于 4 层的网络，设隐藏层都只有两个神经元，则 $\delta_2^{(2)}=\Theta_{12}^{(2)} \delta_1^{(3)}+\Theta_{22}^{(2)} \delta_2^{(3)}$。&lt;/p&gt;
&lt;h4 id=&#34;忽略代价函数中的正则化项&#34;&gt;忽略代价函数中的正则化项
&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;忽略代价函数中正则化项，则 $\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)} \delta_i^{l+1}$。其中上下标的含义：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$l$ 代表目前所计算的是第几层&lt;/p&gt;
&lt;p&gt;$j$ 代表目前计算层中的激活单元的下标，也就是下一层第 $j$ 个输入变量的下标&lt;/p&gt;
&lt;p&gt;$i$ 代表下一层中误差单元的下标&lt;/p&gt;
&lt;h4 id=&#34;考虑正则化处理&#34;&gt;考虑正则化处理
&lt;/h4&gt;&lt;p&gt;则用 $\Delta_{ij}^{(l)}$ 来表示误差矩阵，则算法表示为：
&lt;/p&gt;
$$
\begin{aligned}
\text{for } i = 1 : m \quad \{ \\
\quad &amp; a^{(1)} := x^{(i)} \\
\quad &amp; \text{向前传播计算 } a^{(l)} \text{ for } l = 1, 2, 3, \dots, L \\
\quad &amp; \text{using } \delta^{(L)} := a^{(L)} - y^{(i)} \\
\quad &amp; \text{执行向后传播算法计算直至第二层的所有误差} \\
\quad &amp; \Delta^{(l)}_{ij} := \Delta^{(l)}_{ij} + a^{(l)}_j \delta^{(l+1)}_i \\
\}
\end{aligned}
$$&lt;p&gt;
在求出了 $\Delta_{ij}^{(l)}$ 之后，则可求出代价函数的偏导数：
&lt;/p&gt;
$$
\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\Theta)= D_{ij}^{(l)}:= \begin{cases}
\frac{1}{m}\Delta_{ij}^{(l)}+\lambda \Theta_{ij}^{(l)}&amp; ,\quad if \quad j \ne 0 \\
\frac{1}{m}\Delta_{ij}^{(l)}&amp;, \quad if \quad j = 0
\end{cases}
$$&lt;h3 id=&#34;梯度检验&#34;&gt;梯度检验
&lt;/h3&gt;&lt;p&gt;对一个复杂模型使用梯度下降算法时，可能会存在一些不易察觉的错误，意味着虽然代价看上去不断减小，但是最终的结果可能并不是最优解。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方法：梯度检验&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;思想&lt;/strong&gt;：通过估计梯度值来检验我门计算的导数值是否真的是预期的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法&lt;/strong&gt;：在代价函数上，对于某个特定的 $\theta$，我们计算出在 $\theta - \epsilon $ 处和 $\theta + \epsilon $ 的代价值，然后求两个代价的平均，即为估计值。&lt;/p&gt;
&lt;p&gt;针对 $\theta_1$ 进行检验的示例：
&lt;/p&gt;
$$
\frac{\partial}{\partial_{\theta_1}}=\frac{J(\theta_1 + \epsilon,\theta_2,\cdots,\theta_n)-J(\theta_1 - \epsilon,\theta_2,\cdots,\theta_n)}{2 \epsilon}
$$&lt;p&gt;
对于反向传播算法，计算出的偏导数存储在矩阵 $D_{ij}^{(l)}$ 中，检验时，将该矩阵展开成为向量；与此同时，将 $\theta$ 矩阵也展开为向量，针对每一个 $\theta$ 都计算一个近似的梯度值，将这些值存储于一个近似梯度矩阵中，同 $D_{ij}^{(l)}$ 比较。&lt;/p&gt;
&lt;h3 id=&#34;随机初始化&#34;&gt;随机初始化
&lt;/h3&gt;&lt;p&gt;任何优化算法都需要一些初始的参数。&lt;/p&gt;
&lt;p&gt;对于神经网络，&lt;strong&gt;令所有的初试参数都为一个相同的值是不可行的&lt;/strong&gt;，这意味着对于一个层的所有激活单元都为相同的值。进而在梯度下降时，对于每一个激活单元的误差都是相同的，这意味着高度的冗余。因此需要对参数进行随机初始化。&lt;/p&gt;
&lt;h3 id=&#34;神经网络的训练&#34;&gt;神经网络的训练
&lt;/h3&gt;&lt;h4 id=&#34;网络结构&#34;&gt;网络结构
&lt;/h4&gt;&lt;p&gt;第一层的单元数：训练集的特征数量&lt;/p&gt;
&lt;p&gt;最后一层的单元数：训练集的结果的类的数量&lt;/p&gt;
&lt;p&gt;隐藏层：该层大小最好为 1。如果层数大于 1，应确保每个隐藏层的单元个数相同，通常情况下隐藏层单元个数越多越好。&lt;/p&gt;
&lt;h4 id=&#34;训练神经网络&#34;&gt;训练神经网络
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;参数随机初始化&lt;/li&gt;
&lt;li&gt;正向传播算法，计算所有的 $h_\theta(x)$&lt;/li&gt;
&lt;li&gt;选择代价函数&lt;/li&gt;
&lt;li&gt;反向传播算法，计算所有偏导数&lt;/li&gt;
&lt;li&gt;梯度检验&lt;/li&gt;
&lt;li&gt;使用优化算法来最小化代价函数&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;机器学习诊断法&#34;&gt;机器学习诊断法
&lt;/h2&gt;&lt;h3 id=&#34;评估一个假设&#34;&gt;评估一个假设
&lt;/h3&gt;&lt;p&gt;如何 &lt;strong&gt;评估一个假设是否过拟合&lt;/strong&gt; 呢？ 将训练数据分为训练集和测试集。 一般训练集：测试集 = 7：3。&lt;/p&gt;
&lt;h4 id=&#34;用测试集计算误差&#34;&gt;用测试集计算误差
&lt;/h4&gt;&lt;p&gt;对于线性回归模型，可以利用测试集数据计算代价函数 $J$。&lt;/p&gt;
&lt;p&gt;对于逻辑回归模型，除了可以用测试集数据计算代价函数外，还可利用误分类的比率对每一个测试集实例计算：
&lt;/p&gt;
$$
err(h_\theta(x), y)= \begin{cases}
1,&amp;if \quad h_\theta(x) \ge 0.5 \quad and \quad y = 0, or \quad if \quad h_\theta(x) &lt; 0.5 \quad and \quad y = 1  \\
0，&amp;其它
\end{cases}
$$&lt;p&gt;
然后对计算结果求平均。&lt;/p&gt;
&lt;h3 id=&#34;模型选择和交叉验证集&#34;&gt;模型选择和交叉验证集
&lt;/h3&gt;&lt;p&gt;越高次数的多项式模型越能够适应训练数据集，但适应训练数据集的模型不一定能够推广到一般状况。&lt;strong&gt;可以使用交叉验证集，简称验证集，来帮助选择模型&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;典型的数据集的 &lt;strong&gt;比例&lt;/strong&gt;，训练集：验证集：测试集 = 6：2：2。&lt;/p&gt;
&lt;h4 id=&#34;模型选择方法&#34;&gt;模型选择方法
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;使用训练集训练出几个模型&lt;/li&gt;
&lt;li&gt;用几个模型分别对验证集计算得出交叉验证误差（代价函数的值）&lt;/li&gt;
&lt;li&gt;选取代价函数值最小的模型&lt;/li&gt;
&lt;li&gt;用步骤 3 中选出的模型对测试集计算得出推广误差。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;模型阶数和偏差方差&#34;&gt;模型阶数和偏差/方差
&lt;/h3&gt;&lt;p&gt;对于训练集，当模型阶数 $d$ 较小时，模型的拟合程度低，误差较大。随着 $d$ 的增长。拟合程度提高，误差减小。&lt;/p&gt;
&lt;p&gt;对于验证集，误差随着 $d$ 先减小后增大，转折点为模型开始过拟合的时候。&lt;/p&gt;
&lt;p&gt;训练集误差和验证集误差都比较大，且接近时： 偏差/欠拟合。&lt;/p&gt;
&lt;p&gt;验证集误差远大于训练集误差时：方差/过拟合。&lt;/p&gt;
&lt;center&gt;
    &lt; img src = &#34;https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_bias_var_polynomial_d_curve&#34;
         width = &#34;40%&#34; &gt;
    &lt;br&gt;
    polynomial_d
&lt;/center&gt;
&lt;h3 id=&#34;正则化和偏差方差&#34;&gt;正则化和偏差/方差
&lt;/h3&gt;&lt;h4 id=&#34;选择正则化参数&#34;&gt;选择正则化参数
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;使用训练集训练几个不同程度正则化的模型。正则化参数一般从小到达，按 2 的倍数增加。&lt;/li&gt;
&lt;li&gt;用这几个模型分别对验证集计算交叉验证误差&lt;/li&gt;
&lt;li&gt;选择得出交叉验证误差最小的模型&lt;/li&gt;
&lt;li&gt;用步骤 3 中选出的模型对测试集计算得出推广误差&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;偏差和方差&#34;&gt;偏差和方差
&lt;/h4&gt;&lt;p&gt;当 $\lambda$ 较小时。表现为方差，训练集误差较小（过拟合），交叉验证集误差较大。&lt;/p&gt;
&lt;p&gt;随着 $\lambda$ 的增加，训练集误差不断增加（欠拟合），交叉验证集误差先减小后增大。&lt;/p&gt;
&lt;center&gt;
    &lt; img src = &#34;https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_bias_var_regularization_curve&#34;
         width = &#34;40%&#34; &gt;
    &lt;br&gt;
    Bias/variance as function of the regularzation 
&lt;/center&gt;
&lt;h3 id=&#34;学习曲线&#34;&gt;学习曲线
&lt;/h3&gt;&lt;h4 id=&#34;高偏差欠拟合&#34;&gt;高偏差，欠拟合
&lt;/h4&gt;&lt;p&gt;对于高偏差，欠拟合的情况，曲线如下图所示：&lt;/p&gt;
&lt;center&gt;
    &lt; img src = &#34;https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_High_bias_learning_curve&#34;
         width = &#34;40%&#34; &gt;
    &lt;br&gt;
    High bias
&lt;/center&gt;
&lt;p&gt;曲线有如下特点：对于验证集和训练集，误差都很大，且二者很接近。无论训练集多大，其误差不会有太大改变&lt;/p&gt;
&lt;h4 id=&#34;高方差过拟合&#34;&gt;高方差，过拟合
&lt;/h4&gt;&lt;p&gt;对于高偏差，欠拟合的情况，曲线如下图所示：&lt;/p&gt;
&lt;center&gt;
    &lt; img src =&#34; https://raw.githubusercontent.com/heimi2022/img-repo0/main/Machine_Learning_High__variance_learning_curve &#34;
         width = &#34;40%&#34; &gt;
    &lt;br&gt;
    High variance
&lt;/center&gt;
&lt;p&gt;曲线有以下特点，训练集误差很小，验证集误差很大。但是增加训练集数据可以提高模型效果。&lt;/p&gt;
&lt;h3 id=&#34;总结&#34;&gt;总结
&lt;/h3&gt;&lt;p&gt;解决高方差：获取更多的训练数据，尝试减少特征的数量，增加正则化程度&lt;/p&gt;
&lt;p&gt;解决高偏差：增加特征的数量，增加多项式特征，减小正则化程度&lt;/p&gt;
&lt;p&gt;较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合。
较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，但可以通过正则化手段来调整而更加适应数据。&lt;/p&gt;
&lt;p&gt;通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。&lt;/p&gt;
&lt;p&gt;对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数。&lt;/p&gt;
&lt;h2 id=&#34;误差分析&#34;&gt;误差分析
&lt;/h2&gt;&lt;h3 id=&#34;构建一个学习算法的方法&#34;&gt;构建一个学习算法的方法：
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;从一个简单且能快速实现的算法开始，实现并用验证集测试这个算法&lt;/li&gt;
&lt;li&gt;绘制学习曲线，决定增加更多数据 or 添加更多特征&amp;hellip;..&lt;/li&gt;
&lt;li&gt;进行误差分析&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;类偏斜的误差度量&#34;&gt;类偏斜的误差度量
&lt;/h3&gt;&lt;p&gt;类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Positive（预测）&lt;/strong&gt;&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;Negtive（预测）&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Positive（实际）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;TP&lt;/td&gt;
          &lt;td&gt;FN&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Negtive（实际）&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;FP&lt;/td&gt;
          &lt;td&gt;TN&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;TP&lt;/strong&gt;：正确肯定，预测为真，实际为真&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FP&lt;/strong&gt;：错误肯定，预测为真，实际为假&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TN&lt;/strong&gt;：正确否定，预测为假，实际为假&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FN&lt;/strong&gt;：错误否定，预测为假，实际为真&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;查准率（Precision）：&lt;/strong&gt; $P=\frac{TP}{TP+FP}$。在预测的所有真的里面，实际上为真的比例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;查全率(Recall)：&lt;/strong&gt; $R=\frac{TP}{TP+FN}$。在所有实际上为真的里面，预测为真的比例。&lt;/p&gt;
&lt;h3 id=&#34;查准率和查全率之间的平衡&#34;&gt;查准率和查全率之间的平衡
&lt;/h3&gt;&lt;p&gt;假如 $h_\theta(x)\ge p$ 预测为真，那么将 p 调大，查准率上升，查全率下降；将 p 调小，查准率下降，查全率上升。
又因为我们希望两者都比较大，则我们需要做一个权衡。&lt;/p&gt;
&lt;p&gt;选择阈值的方法，一种是 &lt;strong&gt;计算 F1 值(F1 Score)&lt;/strong&gt;。其计算公式为：
&lt;/p&gt;
$$
F1 \, Score = 2\frac{PR}{P+R}
$$&lt;p&gt;
应选择使得 F1 值最高的阈值。&lt;/p&gt;
&lt;h2 id=&#34;支持向量机svm&#34;&gt;支持向量机（SVM）
&lt;/h2&gt;&lt;h3 id=&#34;svm-定义&#34;&gt;SVM 定义
&lt;/h3&gt;&lt;p&gt;SVM：
&lt;/p&gt;
$$
\underset{\theta}{min}C\sum^m_{i = 1}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac12\sum^n_{i = 1}\theta^2_j, C =\frac1\lambda
$$&lt;p&gt;
这里将正则化常数移到了第一项。在逻辑回归中，若 $\lambda$ 很大，则意味着给正则化项一个很大的权重。在这里若 $C$ 很小, 则也等同于给正则化项一个很大的权重。&lt;/p&gt;
&lt;p&gt;$C$ 较大时，相当于 $\lambda$ 很小，可能导致过拟合，高方差。
$C$ 较小时，相当于 $\lambda$ 很大，可能导致欠拟合，高偏差。&lt;/p&gt;
&lt;p&gt;对于 SVM，若存在一个正样本，我们会希望代价函数中 $\theta^T x \ge 1$ 而不仅仅要求 $\theta^T x &amp;gt; 0$。这相当于在 SVM 中嵌入了一个额外的安全因子。
同样，对于一个负样本，我们会希望代价函数中 $\theta^T x \le 1$ 而不仅仅要求 $\theta^T x &amp;lt; 0$。这在 SVM 中表现为决策边界的间距。&lt;/p&gt;
&lt;p&gt;支持向量机具有鲁棒性？因为其用一个最大间距来分离样本，因此有时支持向量机被称为大间距分类器。&lt;/p&gt;
&lt;h3 id=&#34;svm-原理&#34;&gt;SVM 原理
&lt;/h3&gt;&lt;p&gt;支持向量机做的事情是 &lt;strong&gt;极小化&lt;/strong&gt; 参数向量 $\theta$ 范数（长度）的平方。&lt;/p&gt;
&lt;p&gt;支持向量机中参数向量 $\theta$ 是与决策边界 90 度 &lt;strong&gt;正交&lt;/strong&gt; 的。
若决策边界选择不合理，那意味着样本在参数向量上的投影数值很小，若要求 $\theta^T x \ge 1$ 或者 $\theta^T x \le 1$ 则需要使得参数向量 $\theta$ 范数的平方很大，则会舍弃该决策边界。&lt;/p&gt;
&lt;h3 id=&#34;核函数&#34;&gt;核函数
&lt;/h3&gt;&lt;p&gt;用一系列新的特征 $f$ 来替代模型中的每一项，则
&lt;/p&gt;
$$
h_\theta(x)=\theta_1 f_1 + \theta_2 f_2 + \cdots + \theta_n f_n
$$&lt;h4 id=&#34;构造-f&#34;&gt;构造 $f$
&lt;/h4&gt;&lt;p&gt;给定一个训练实例 $x$，利用 $x$ 的各个特征与我们预先选定的地标（landmarks）$l^{(1)}，l^{(2)},l^{(3)}$ 的近似程度来选取新的特征 $f_1,f_2,f_3$。&lt;/p&gt;
&lt;p&gt;一个核函数&amp;mdash;&lt;strong&gt;高斯核函数&lt;/strong&gt;
&lt;/p&gt;
$$
f_1 = similarity(x, l^{(1)})= exp(-\frac{||x-l^{(1)}||^2}{2 \sigma^2})
$$&lt;p&gt;
其中 $||x-l^{(1)}||^2=\sum_{j=1}^n(x_j-l_j^{(1)})^2$，为实例 $x$ 中所有特征与地标 $l^{(1)}$ 之间距离的和。&lt;/p&gt;
&lt;p&gt;则，对于 $x$，如果其离地标 $l$ 距离近似为 0，则新特征 $f=1$；如果其离地标 $l$ 距离较远，则新特征 $f=0$。&lt;/p&gt;
&lt;p&gt;随着 $x$ 的改变，$f$ 值改变的速率受到 $\sigma^2$ 的控制。
$\sigma^2$ 越大，改变速率越慢，可能导致高偏差。
$\sigma^2$ 越小，改变速率越快，可能导致高方差。&lt;/p&gt;
&lt;h4 id=&#34;如何选择地标&#34;&gt;如何选择地标
&lt;/h4&gt;&lt;p&gt;通常根据训练集的数量选择地标的数量，如果训练集中有 $m$ 个实例，则选择 $m$ 个地标，
并且令：$l^{(1)}=x^{(1)},l^{(2)}=x^{(3)},\cdots,l^{(m)}=x^{(m)}$。&lt;/p&gt;
&lt;p&gt;好处：此时的新特征是建立在原有特征与训练集中所有其它特征之间距离的基础之上的。&lt;/p&gt;
&lt;h3 id=&#34;修改支持向量机&#34;&gt;修改支持向量机
&lt;/h3&gt;&lt;p&gt;给定 $x$，计算新特征 $f$，当 $\theta^Tf \ge 0$ 时，预测 $y=1$。否则反之。
相应的修改正则化项为：$\sum_{j=1}^{n=m}\theta_j^2=\theta^T\theta$。则有如下式子：
&lt;/p&gt;
$$
\underset{\theta}{min}C\sum^m_{i = 1}[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac12\theta^TM\theta
$$&lt;p&gt;
其中 $M$ 是随核函数变化的一个矩阵。&lt;/p&gt;
&lt;h3 id=&#34;使用支持向量机&#34;&gt;使用支持向量机
&lt;/h3&gt;&lt;p&gt;$n$ 为特征数，$m$ 为训练样本数。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若相较于 $m$ 而言，$n$ 大很多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。&lt;/li&gt;
&lt;li&gt;若 $n$ 较小，而且 $m$ 大小中等，例如 $n$ 在 1-1000 之间，而 $m$ 在 10-10000 之间，选用高斯核函数的支持向量机。&lt;/li&gt;
&lt;li&gt;若 $n$ 较小，而且 $m$ 较大，例如 $n$ 在 1-1000 之间，而 $m$ 大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;选择支持向量机的原因在于它的代价函数为凸函数，不存在局部最小值。&lt;/p&gt;
&lt;h2 id=&#34;聚类算法&#34;&gt;聚类算法
&lt;/h2&gt;&lt;p&gt;这是一个非监督学习算法，其数据没有附带任何标签。&lt;/p&gt;
&lt;h3 id=&#34;k-均值算法&#34;&gt;K-均值算法
&lt;/h3&gt;&lt;p&gt;K-均值算法是最普及的聚类算法，其将未标记的数据聚类成不同组。&lt;/p&gt;
&lt;h4 id=&#34;k-均值算法的思想&#34;&gt;K-均值算法的思想
&lt;/h4&gt;&lt;p&gt;K-均值算法是一个迭代算法，假设将数据分为 K 组，其方法为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择 K 个随机点，称为聚类中心&lt;/li&gt;
&lt;li&gt;对于数据集中的每一个数据，按照距离 K 个中心点的距离，将其与距离最近的中心点关起来，与同一个中心点关联的所有数据聚成一类。&lt;/li&gt;
&lt;li&gt;计算每一个类的平均值，将该类的中心点移到其平均值的位置。&lt;/li&gt;
&lt;li&gt;重复 2-3，直至中心点不再变化。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;k-均值算法的优化目标&#34;&gt;K-均值算法的优化目标
&lt;/h4&gt;&lt;p&gt;K-均值算法最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和。因此 K-均值算法的代价函数（又称畸变函数 Distortion function）为：
&lt;/p&gt;
$$
min \, J(c^{(1)}, c^{(2)},\cdots, c^{(n)},\mu^1,\mu^2,\cdots,\mu^k)=\frac 1m\sum_{i = 1}^m||X^{(i)}-\mu_{c^{(i)}}||^2
$$&lt;h4 id=&#34;k-均值算法的伪代码&#34;&gt;K-均值算法的伪代码
&lt;/h4&gt;$$
\begin{aligned}
&amp; \text{Repeat} \{ \\
&amp; \text{for i = 1 to m}  \\
&amp; \text{c(i) := 距离实例数据最近的聚类中心的索引}  \\
&amp; \text{for k = 1 to k}  \\
&amp; \mu_k \text{ := 聚到该类的数据均值}  \\
&amp; \} 
\end{aligned}
$$&lt;p&gt;
其中第一个循环是用于减小 $c^{(i)}$ 引起的代价，第二个循环则是用于减小 $\mu_i$ 引起的代价。&lt;/p&gt;
&lt;h4 id=&#34;随机初始化-1&#34;&gt;随机初始化
&lt;/h4&gt;&lt;p&gt;在运行 K-均值算法时，我们要随机初始化所有聚类中心点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;应使得聚类中心点的个数 K 小于所有训练集实例的数量 m。&lt;/li&gt;
&lt;li&gt;随机选择 K 个训练实例，然后令 K 个聚类中心分别与 K 个训练实例相等。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;K-均值算法可能会停留在局部最小值处，因此需要多次运行该算法，每一次都重新进行随机初始化，最后在选择使代价函数最小的结果。&lt;/p&gt;
&lt;h4 id=&#34;选择聚类数&#34;&gt;选择聚类数
&lt;/h4&gt;&lt;p&gt;利用 &amp;ldquo;肘部法则&amp;rdquo;，画出代价函数 J-聚类数目 K 的图像，当图像中出现一个拐点时，则可以选择该 K 值。&lt;/p&gt;
&lt;h2 id=&#34;降维&#34;&gt;降维
&lt;/h2&gt;&lt;p&gt;特征之间可能存在高度冗余，可以采取降维将数据从高纬度降到低维度。&lt;/p&gt;
&lt;p&gt;降维的原因：数据压缩、数据可视化。
数据压缩意义：减少数据占用的内存，加快学习算法。
数据可视化意义：将数据降低至低维，能够使得数据可视化，便于找到一个更好的解决方案。&lt;/p&gt;
&lt;h3 id=&#34;主成分分析pca&#34;&gt;主成分分析（PCA）
&lt;/h3&gt;&lt;p&gt;主成分分析（PCA）是最常见的降维算法。&lt;/p&gt;
&lt;p&gt;在 PCA 中，我们需要找到一个方向向量，当把所有数据都投射到该向量上时，能够使得投射平均均方误差尽可能地小。
方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。&lt;/p&gt;
&lt;p&gt;当要将 n 维数据降至 k 维数据时，其目标是找到向量 $u^{(1)},u^{(2)},\cdots,u^{(k)}$ 使得总的投射误差最小。相比于线性回归，其不做任何预测。&lt;/p&gt;
&lt;p&gt;PCA 的优点：其保证数据降维后特性损失最小；降维后结果只与数据相关，与用户是独立的。&lt;/p&gt;
&lt;h3 id=&#34;pca-算法&#34;&gt;PCA 算法
&lt;/h3&gt;&lt;p&gt;PCA 减少 n 维到 k 维：&lt;/p&gt;
&lt;p&gt;训练集 $x^{(1)},\cdots,x^{(m)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;均值归一化，计算所有特征的均值 $\mu_j=\frac1m\sum_{i=1}^m x_j^{(i)}$，然后令 $x_j=x_j - ,u_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $\sigma^2$。&lt;/li&gt;
&lt;li&gt;计算协方差矩阵 $\Sigma= \frac1m \sum_{i=1}^m (x^{(i)})(x^{(i)})^T$&lt;/li&gt;
&lt;li&gt;对协方差矩阵 $\Sigma$ 进行求特征向量，得到特征矩阵（按特征值从大到小排列），提取千 K 列组成矩阵 $P_{n*k}$，$P$ 就相当于是一个坐标系，P 中的每一列就是一个坐标轴。&lt;/li&gt;
&lt;li&gt;将原始数据投影到 $P$ 坐标系下即可得到降维后的数据，$Y_{k&lt;em&gt;m}=P_{n&lt;/em&gt;k}^T X_{n*m}$。其中降维后的向量 $z^{(i)}=P^T x^{(i)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;选择主成分的数量&#34;&gt;选择主成分的数量
&lt;/h3&gt;&lt;p&gt;训练集的方差为：$\frac1m \sum_{i=1}^m||x^{(i)}||^2$&lt;/p&gt;
&lt;p&gt;平均均方误差为：$\frac1m \sum_{i=1}^m||x^{(i)} - x_{approx}^{(i)}||^2$&lt;/p&gt;
&lt;p&gt;选择的依据：在平均均方误差与训练集的比例尽可能小的情况下选择尽可能小的 k 值。&lt;/p&gt;
&lt;p&gt;如果希望这个比例小于 1%，就意味着原本数据的偏差有 99%都保留下来了。&lt;/p&gt;
&lt;p&gt;步骤：先确定保留的偏差，再从小到大选择 k，运行 PCA 算法，观察比例。&lt;/p&gt;
&lt;p&gt;也可以采用特征矩阵（对角矩阵），则只需要修改上述方法为：
&lt;/p&gt;
$$
\frac{\frac1m \sum_{i = 1}^m||x^{(i)} - x_{approx}^{(i)}||^2}{\frac1m \sum_{i = 1}^m||x^{(i)}||^2}= 1-\frac{\sum_{i = 1}^ks_{ii}}{\sum_{i = 1}^ms_{ii}}\le1 \%
$$&lt;h3 id=&#34;pca-重建&#34;&gt;PCA 重建
&lt;/h3&gt;&lt;p&gt;$x_{approx}^{(i)}=Pz^{(i)}$&lt;/p&gt;
&lt;h3 id=&#34;pca-应用&#34;&gt;PCA 应用
&lt;/h3&gt;&lt;p&gt;当逻辑回归等算法，输入特征过大，可以先采用 PCA 将其输入特征压缩，再连带着原来的标注一起训练模型。利用验证集以及测试集验证该模型时，注意需要使用训练集得出的 PCA 特征矩阵的 $P$。&lt;/p&gt;
&lt;h2 id=&#34;异常检测&#34;&gt;异常检测
&lt;/h2&gt;&lt;p&gt;虽然异常检测主要用于非监督学习问题，但从一些角度，其又类似于一些监督学习问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;异常检测定义&lt;/strong&gt;：给定数据集$x^{(1)},\cdots,x^{(m)}$，假设数据集是正常的，我们希望知道$x_{text}$是否正常，即这个测试数据不属于该组数据的几率如何 。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性$p(x)$。&lt;/p&gt;
&lt;p&gt;检测方法&amp;ndash;密度估计
$$
if \quad p(x) \begin{cases}
&amp;lt; \epsilon \quad anomaly \
\ge \epsilon \quad normal&lt;/p&gt;
&lt;p&gt;\end{cases}
$$&lt;/p&gt;
&lt;h3 id=&#34;高斯分布&#34;&gt;高斯分布
&lt;/h3&gt;&lt;p&gt;若$x \sim N(\mu,\sigma^2)$，则其概率密度分布函数为$p(x,\mu,\sigma^2)=\frac{1}{\sqrt{2\pi} \sigma}exp(-\frac{(x-\mu)^2}{2 \sigma^2})$。&lt;/p&gt;
&lt;p&gt;则 $\mu = \frac{1}{m}\sum^m_{i=1}x^{(i)}$，$\sigma^2=\frac1m\sum^m_{i=1}(x^{(i)}-\mu)^2$。
在机器学习中对于方差我们通常指除以$m$而非统计学中的$m-1$。&lt;/p&gt;
&lt;h3 id=&#34;异常检测算法&#34;&gt;异常检测算法
&lt;/h3&gt;&lt;p&gt;应该高斯分布开发异常检测算法。&lt;/p&gt;
&lt;p&gt;对于给定的数据集$x^{(1)},\cdots,x^{(m)}$，需要针对每一特征计算$\mu$和$\sigma^2$。则：
&lt;/p&gt;
$$
\mu_j = \frac{1}{m}\sum^m_{i=1}x^{(i)}_j
$$$$
\sigma^2_j=\frac1m\sum^m_{i=1}(x^{(i)}_j-\mu_j)^2
$$&lt;p&gt;则对于一个新的实例有
&lt;/p&gt;
$$
p(x)=\prod_{j=1}^np(x_j;\mu_j;\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi} \sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2 \sigma^2_j})
$$&lt;h3 id=&#34;评价一个异常检测系统&#34;&gt;评价一个异常检测系统
&lt;/h3&gt;&lt;p&gt;使用带标记（正常/异常）的数集着手，选择其中一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据来构建验证集以及测试集。&lt;/p&gt;
&lt;p&gt;具体评价方法如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数。&lt;/li&gt;
&lt;li&gt;对交叉检验集，我们尝试使用不同的$\epsilon$值作为阈值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择$\epsilon $。&lt;/li&gt;
&lt;li&gt;选出$\epsilon$后，针对测试集进行预测，计算异常检验系统的F1值，或者查准率与查全率之比。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;异常检测与监督学习对比&#34;&gt;异常检测与监督学习对比
&lt;/h3&gt;&lt;p&gt;异常检测：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有非常少量的正向类（异常数据 $y=1$），大量的负向类（$y=0$）。&lt;/li&gt;
&lt;li&gt;有许多不同种类的异常。根据非常少量的正向类数据来训练算法。&lt;/li&gt;
&lt;li&gt;未来遇到的异常可能与已掌握的异常、非常的不同。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;监督学习：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;同时又大量的正向类和负向类&lt;/li&gt;
&lt;li&gt;有足够多的正向类实例，足够用于训练算法，未来遇到的正向类实例可能与训练集中的非常近似。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;选择特征&#34;&gt;选择特征
&lt;/h3&gt;&lt;p&gt;异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布。可以对特征进行一些小转换，如变为log，或者取一些指数等方法。&lt;/p&gt;
&lt;p&gt;误差分析：有些异常的数据可能也会有较高的$p(x)$值，因而被算法认为是正常的。此时可通过误差分析，发现问题，并从中增加一些新的特征。将一些相关的特征进行组合。&lt;/p&gt;
&lt;h2 id=&#34;推荐系统&#34;&gt;推荐系统
&lt;/h2&gt;&lt;p&gt;假设有5部电影，4个用户。要求根据用户对电影的打分情况，为用户推荐电影，给指定用户未打分的电影ai打分。&lt;/p&gt;
&lt;p&gt;引入一些标记：&lt;/p&gt;
&lt;p&gt;$n_u$：用户的数量&lt;/p&gt;
&lt;p&gt;$n_m$：电影的数量&lt;/p&gt;
&lt;p&gt;$r(i,j)$：若用户$j$给电影$i$评过分，则$r(i,j)=1$.&lt;/p&gt;
&lt;p&gt;$y^{(i,j)}$ ：用户$j$给电影$i$的评分&lt;/p&gt;
&lt;p&gt;$m_j$：用户$j$评过分的电影的总数&lt;/p&gt;
&lt;h3 id=&#34;基于内容的推荐系统&#34;&gt;基于内容的推荐系统
&lt;/h3&gt;&lt;p&gt;对于每部电影，假设有特征$x^{(i)}$。则基于这些特征来构建一个推荐系统算法。假设采用线性回归模型，且针对每一个用户都训练一个线性回归模型。有：&lt;/p&gt;
&lt;p&gt;$\theta^{(j)}$为用户$j$的参数向量。&lt;/p&gt;
&lt;p&gt;$x^{(i)}$ 为电影$i$的特征向量&lt;/p&gt;
&lt;p&gt;对于用户$j$和电影$i$，我们的预测评分为$(\theta^{(j)})^Tx^{(i)}$&lt;/p&gt;
&lt;p&gt;则对于用户$j$，其代价函数为：
&lt;/p&gt;
$$
\min_{\theta^{(j)}} \frac{1}{2} \sum_{i: r(i,j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)} \right)^2 + \frac{\lambda}{2} (\theta^{(j)}_k)^2
$$&lt;p&gt;
其中$i: r(i,j)=1$表示只计算哪些用户$j$评价过的电影。&lt;/p&gt;
&lt;p&gt;为了学习所有用户，我们将所有用户的代价函数求和：
&lt;/p&gt;
$$
\min_{\theta^{(1)}, \ldots, \theta^{(n_u)}} \frac{1}{2} \sum_{j=1}^{n_u}\sum_{i: r(i,j)=1} \left( (\theta^{(j)})^T x^{(i)} - y^{(i,j)} \right)^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^{n} \left( \theta_k^{(j)} \right)^2
$$&lt;p&gt;
用梯度下降算法求最优解，则：
&lt;/p&gt;
$$
\theta_k^{(j)}:=\theta_k^{(j)} - \alpha\sum_{i:r(i,j)=1}((\theta^{j})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} \quad(for \, k = 0)
$$$$
\theta_k^{(j)}:=\theta_k^{(j)} - \alpha(\sum_{i:r(i,j)=1}((\theta^{j})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)}) \quad(for \, k \ne 0)
$$&lt;h3 id=&#34;协同过滤&#34;&gt;协同过滤
&lt;/h3&gt;&lt;p&gt;前面，基于每一部电影可用的特征，我们可训练得到每一个用户的参数。同样，如果我们有用户的参数，我们则可以学习出电影的特征。
&lt;/p&gt;
$$
\min_{x^{(1)},\cdots,x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j: r(i,j)=1} \left( \left( \theta^{(j)} \right)^T x^{(i)} - y^{(i,j)} \right)^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n \left( x_k^{(i)} \right)^2
$$&lt;p&gt;
假如既没有用户的参数，也没有电影的特征。则不能采用上述的两种方法。则可采用协同过滤来同时学习这两者，其代价函数为：
&lt;/p&gt;
$$
J(x^{(1)},\ldots,x^{(n_m)},\theta^{(1)},\ldots,\theta^{(n_u)}) \\= \frac{1}{2} \sum_{(i:j):r(i,j)=1} \left( \left( \theta^{(j)} \right)^T x^{(i)} - y^{(i,j)} \right)^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n \left( x_k^{(i)} \right)^2 \\
\quad + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n \left( \theta_k^{(j)} \right)^2
$$&lt;p&gt;
采用梯度下降法寻找最小值：
&lt;/p&gt;
$$
x_k^{(i)} := x_k^{(i)} - \alpha \left( \sum_{j: r(i,j)=1} \left( \left( \theta^{(j)} \right)^T x^{(i)} - y^{(i,j)} \right) \theta_k^{(j)} + \lambda x_k^{(i)} \right) \\
\theta_k^{(j)} := \theta_k^{(j)} - \alpha \left( \sum_{i: r(i,j)=1} \left( \left( \theta^{(j)} \right)^T x^{(i)} - y^{(i,j)} \right) x_k^{(i)} + \lambda \theta_k^{(j)} \right) \\
\min_{x^{(1)},\ldots,x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j: r(i,j)=1} \left( \left( \theta^{(j)} \right)^T x^{(i)} - y^{(i,j)} \right)^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n \left( x_k^{(i)} \right)^2
$$&lt;p&gt;
协同过滤算法使用步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;初始$x^{(1)}, x^{(2)}, \ldots, x^{(nm)}, \theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(n_u)}$为一些随机最小值。&lt;/li&gt;
&lt;li&gt;使用梯度下降法来最小化代价函数&lt;/li&gt;
&lt;li&gt;训练完算法后，我们预测$(\theta^{(j)})^Tx^{(i)}$为用户$j$给电影$i$评分&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;均值归一化&#34;&gt;均值归一化
&lt;/h3&gt;&lt;p&gt;若新增一个用户，其没有为任何电影评过分。可用均值归一化的方法为其推荐电影。&lt;/p&gt;
&lt;p&gt;步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对结果Y矩阵进行均值归一化处理，将每一个用户对某一电影的评分减去所有用户对该电影评分的平均值。&lt;/li&gt;
&lt;li&gt;然后利用新的Y矩阵来训练算法。&lt;/li&gt;
&lt;li&gt;训练完毕后需要将平均值加回去。&lt;/li&gt;
&lt;li&gt;对于新用户，该算法认为他给每部电影的评分都是该电影的平均值。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;大规模机器学习&#34;&gt;大规模机器学习
&lt;/h2&gt;&lt;p&gt;对于大型数据集的学习，首先要做的事是，去检查一个大规模的训练集是否真的有必要。可以绘制学习曲线来帮助判断。&lt;/p&gt;
&lt;p&gt;若一定需要一个大规模的训练集，可以采用**随机梯度下降法（SGD）**来代替批量梯度下降法。&lt;/p&gt;
&lt;h3 id=&#34;随机梯度下降法&#34;&gt;随机梯度下降法
&lt;/h3&gt;&lt;p&gt;在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：
&lt;/p&gt;
$$
cost(\theta,(x^{(i)},y{(i)}))=\frac12(h_\theta(x^{(i)})-y^{(i)})^2
$$&lt;p&gt;
随机梯度下降算法为：
首先对训练集进行随机洗牌，然后：
&lt;/p&gt;
$$
\begin{aligned}
&amp; Repeat \{ \\
&amp; \quad for \, i=1:m \{ \\
&amp; \quad  \theta_j:=\theta_j - \alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp; \quad  (for \, j=0:n) \\
&amp; \quad  \} \\
&amp; \}
\end{aligned}
$$&lt;p&gt;
随机梯度下降算法在每一次计算之后便更新参数$\theta$，不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。&lt;/p&gt;
&lt;h3 id=&#34;小批量梯度下降算法&#34;&gt;小批量梯度下降算法
&lt;/h3&gt;&lt;p&gt;小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算
常数b次训练实例，便会更新一次参数$\theta$。
&lt;/p&gt;
$$
\begin{aligned}
&amp; Repeat \{ \\
&amp; \quad for \, i=1:m \{ \\
&amp; \quad  \theta_j:=\theta_j - \alpha\frac{1}{b}\sum_{k=i}^{i+b-1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp; \quad  (for \, j=0:n) \\
&amp; \quad i+=b \\
&amp; \quad  \} \\
&amp; \}
\end{aligned}
$$&lt;p&gt;
通常会令b在2-100之间。&lt;/p&gt;
&lt;h3 id=&#34;随机梯度下降收敛&#34;&gt;随机梯度下降收敛
&lt;/h3&gt;&lt;p&gt;如何对随机梯度下降算法进行调试以及选取学习率$\alpha$？&lt;/p&gt;
&lt;p&gt;在随机梯度下降中，我们在每一次更新$\theta$前都计算一次代价，然后每x次迭代后，求出这x次对训练实例计算代价的平均值，然后绘制这些平均值宇x次迭代的次数之间的函数图表。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若曲线发散，则选取较小学习率。&lt;/li&gt;
&lt;li&gt;曲线在收敛，但噪声比较大（曲线小幅度上下起伏），选取较小学习率，或增大x。&lt;/li&gt;
&lt;li&gt;曲线基本保持平坦，模型有错误，或者增大x。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;在线学习&#34;&gt;在线学习
&lt;/h3&gt;&lt;p&gt;在线学习算法指的是对数据流而非离线的静态数据集的学习。&lt;/p&gt;
&lt;p&gt;若有一个由连续的用户流引发的连续的数据流，则可以使用在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。&lt;/p&gt;
&lt;p&gt;其与随机梯度下降算法有些类似，都对单一的实例进行学习。
&lt;/p&gt;
$$
\begin{aligned}
&amp; Repeat \,forever(\text{若网站一直在运行}) \{ \\
&amp; \quad \text{获取数据}(x,y)  \\
&amp; \quad  \theta_j:=\theta_j - \alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&amp; \quad  (for \, j=0:n) \\
&amp; \}
\end{aligned}
$$&lt;h3 id=&#34;映射简化&#34;&gt;映射简化
&lt;/h3&gt;&lt;p&gt;映射简化：将数据集分配给不同台计算机，让每一台计算机处理数据集的一个子集，然后将计算结果汇总求和。（或是一台计算机的多个核）&lt;/p&gt;
&lt;h2 id=&#34;其它&#34;&gt;其它
&lt;/h2&gt;&lt;h3 id=&#34;大量数据获取&#34;&gt;大量数据获取
&lt;/h3&gt;&lt;p&gt;利用已有的数据，对其进行修改，如将已又的字符图片进行一些扭曲、旋转、模糊处理。只要认为实际数据又可能核经过这样处理后的数据类似，我们就可以采用这样的方法来创造大量数据。&lt;/p&gt;
&lt;p&gt;有关获取数据的几种方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;人工数据合成&lt;/li&gt;
&lt;li&gt;手动收集、标注数据&lt;/li&gt;
&lt;li&gt;众包&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;上限分析&#34;&gt;上限分析
&lt;/h3&gt;&lt;p&gt;在机器学习应用中，我们通常需要通过几个步骤才能进行最终的预测，&lt;strong&gt;利用上限分析可以知道哪一个步骤最值得投入时间&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;每一个步骤中，上一个步骤的输出是下一个步骤的输入。在上限分析中，我们选取一个步骤，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。若提升效果较大，则可投入大量精力至该步骤中；反之，该步骤对模型整体性能提升效果不大。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Hugo使用</title>
        <link>https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/</link>
        <pubDate>Thu, 22 May 2025 15:53:57 +0800</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/</guid>
        <description>&lt;img src="https://heimi2022.github.io/Hugo/p/hugo%E4%BD%BF%E7%94%A8/cover.png" alt="Featured image of post Hugo使用" /&gt;&lt;h2 id=&#34;启动-hugo-服务&#34;&gt;启动 Hugo 服务
&lt;/h2&gt;&lt;p&gt;hugo server -D&lt;/p&gt;
&lt;h2 id=&#34;创建文章&#34;&gt;创建文章
&lt;/h2&gt;&lt;h3 id=&#34;命令创建&#34;&gt;命令创建
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;命令 : hugo new post/foldername/*.md 
其模板来自 archetypes/default.md
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;手动创建&#34;&gt;手动创建
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;在post目录下，手动创建一个文件夹，在文件夹里创建md文件，并将需要的图片资源放入其中。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;文章推送&#34;&gt;文章推送
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;git add .&lt;/li&gt;
&lt;li&gt;git commit -m&amp;quot;update&amp;quot;&lt;/li&gt;
&lt;li&gt;git push&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;github-图床使用&#34;&gt;github 图床使用
&lt;/h2&gt;&lt;h3 id=&#34;创建github仓库并生成token&#34;&gt;创建Github仓库并生成Token
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;创建仓库
&lt;ul&gt;
&lt;li&gt;在 GitHub 上新建一个仓库（如 &lt;code&gt;my-image-repo&lt;/code&gt;），确保仓库是公开的（Public）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;生成 Token
&lt;ul&gt;
&lt;li&gt;登录 GitHub，进入 &lt;strong&gt;Settings&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Developer settings&lt;/strong&gt; -&amp;gt; &lt;strong&gt;Personal access tokens&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;点击 &lt;strong&gt;Generate new token&lt;/strong&gt;，勾选 &lt;code&gt;repo&lt;/code&gt; 权限。&lt;/li&gt;
&lt;li&gt;生成 Token 后，复制并保存（Token 只会显示一次）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;配置typora-picgo&#34;&gt;配置typora PicGo
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;打开 Typora 的配置文件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在 Typora 的图像设置中，点击 &lt;strong&gt;打开配置文件&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这会打开一个 &lt;code&gt;picgo.json&lt;/code&gt; 文件（如果没有，可以手动创建）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;编辑 &lt;code&gt;picgo.json&lt;/code&gt; 文件，添加 GitHub 图床的配置。以下是一个示例配置：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#34;picBed&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;current&amp;#34;: &amp;#34;github&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;github&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &amp;#34;repo&amp;#34;: &amp;#34;你的用户名/你的仓库名&amp;#34;, // 例如：your-username/my-image-repo
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &amp;#34;branch&amp;#34;: &amp;#34;main&amp;#34;, // 或 master
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &amp;#34;token&amp;#34;: &amp;#34;你的 GitHub Token&amp;#34;, // 生成的 Personal Access Token
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &amp;#34;path&amp;#34;: &amp;#34;assets/&amp;#34;, // 图片上传到仓库的 assets 目录
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &amp;#34;customUrl&amp;#34;: &amp;#34;&amp;#34; // 如果需要自定义域名，可以填写
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  },
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#34;picgoPlugins&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;github&amp;#34;: true // 确保启用 GitHub 插件
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;repo&lt;/strong&gt;: 你的 GitHub 仓库名，格式为 &lt;code&gt;用户名/仓库名&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;branch&lt;/strong&gt;: 仓库的分支，通常是 &lt;code&gt;main&lt;/code&gt; 或 &lt;code&gt;master&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;token&lt;/strong&gt;: 你的 GitHub Personal Access Token（生成方法见下文）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;path&lt;/strong&gt;: 图片上传的路径，例如 &lt;code&gt;assets/&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;customUrl&lt;/strong&gt;: 如果需要自定义域名（如 CDN），可以填写。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;保存 &lt;code&gt;picgo.json&lt;/code&gt; 文件。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>归档</title>
        <link>https://heimi2022.github.io/Hugo/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>关于</title>
        <link>https://heimi2022.github.io/Hugo/%E5%85%B3%E4%BA%8E/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/%E5%85%B3%E4%BA%8E/</guid>
        <description></description>
        </item>
        <item>
        <title>搜索</title>
        <link>https://heimi2022.github.io/Hugo/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/search/</guid>
        <description></description>
        </item>
        <item>
        <title>友情链接</title>
        <link>https://heimi2022.github.io/Hugo/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://heimi2022.github.io/Hugo/%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
